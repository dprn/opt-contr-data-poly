\chapter{Machine learning}

The fundamental problem of machine learning can be formalized as follows.

\pr[]{Machine learning problem}{
    Let $X,Y$ be two sets, and $A(X,Y)$ be a set of functions from $X$ to $Y$.
    Assume moreover that we are given a subset $\mathcal F\subset A(X,Y)$ of these functions parametrized over a set $\Theta$, that is
    \begin{equation}
        \mathcal F = \left\{ \phi_\theta \in A(X,Y)\mid \theta\in \Theta \right\}.
    \end{equation}
    Then, given a function $\psi\in A(X,Y)$ the machine learning (ML) problem consists in finding $\hat \theta\in \Theta$ such that
    \begin{equation}
        \psi \approx \phi_{\hat \theta}.
    \end{equation}
    This has to be done using the \emph{available} information about $\psi$.
}

\ex[pointwise-obs]{Linear approximation and pointwise observation}{
    Let $\psi:[0,2\pi]\to \bbR$ be a continuous function (in particular, $\psi\in L^2([0,2\pi]))$).
    
    Consider $\{\phi_i\}_{i=1}^{+\infty}\subset L^2([0,2\pi])\cap C([0,2\pi])$ to be an orthonormal system for $L^2([0,2\pi])$, and for any $N\in \bbN$ let $\Theta = \bbR^N$ and consider
    \begin{equation}
        \mathcal F = \left\{ \phi_\theta = \sum_{i=1}^N \theta_i \phi_i \mid \theta_i\in \bbR^N \right\}
    \end{equation}

    Assume to be given $M$ observations $\{(x_i,y_i)\}_{i=1}^M$ such that
    \begin{equation}
        y_i = \psi(x_i), \qquad i\in \llbracket 1,M\rrbracket.
    \end{equation}
    The task here is to find $\hat \theta\in\Theta$ such that 
    \begin{equation}
        \sum_{i=1}^M \left| \phi_{\hat\theta}(x_i)-y_i \right|^2
        =\min_{\theta} 
        \sum_{i=1}^M \left| \phi_{\theta}(x_i)-y_i \right|^2.
    \end{equation}
}

\ex[]{Finite elements}{
    Let $B_1(0)\subset \bbR^d$ and $\psi : B_1(0)\to \bbR$ be a real-valued function such that $\psi\in H_0^2(B_1(0))$. 
    Then, given independent functions $\{\phi_i\}_{i=1}^N$ we can consider $\Theta=\bbR^N$ and let
    \begin{equation}
        \mathcal F = \left\{ \phi_\theta = \sum_{i=1}^N \theta_i \phi_i \mid \theta_i\in \bbR^N \right\}
    \end{equation}

    A typical situation, and the basis for the \emph{finite element methods}, is to know $f = \Delta \psi$, in which case one looks for $\hat \theta\in \Theta$ such that 
    \begin{equation}
        \|\phi_{\hat\theta}-\psi\|_{L^2} = \min_\theta \|\phi_\theta-\psi\|_{L^2}.
    \end{equation}
}

In modern machine learning one is interested in ``going beyond'' these examples, by considering a nonlinear parametrization of the approximating family $\mcF$. That is, 
\begin{center}
    the map $\theta\mapsto \phi_\theta$ is nonlinear.
\end{center}

In the following we will focus on the problem of data-fitting exoked in Example~\ref{ex:pointwise-obs}, see also Example~\ref{ex:}. This is the following.

\dfn[data-fitting-pb]{Data-fitting problem}{
    Consider the following data:
    \begin{itemize}
        \item A function $\psi:X\to Y$ to approximate, of which only a set of samples $\mfX=\{( x_i,\psi( x_i))\mid i\in \llbracket 1,M\rrbracket\}$ is known;
        \item A set $\mcF = \{\psi_\theta:X\to Y\mid \theta\in \Theta\}$ of possible approximations;
        \item A loss function $\mcL : Y\times Y \to \bbR$.
    \end{itemize}
    Then, the data-fitting problem (or minimization of the empirical risk) is the following:
    \begin{equation}
        \text{Find } \hat\theta\in\Theta \text{ such that } \hat\theta = \arg\min_{\theta} \frac1N \sum_{(x_i,\hat y_i)\in\mfX} \mcL(\phi_\theta(x_i),\hat y_i).
    \end{equation}
}

\begin{figure}
    \centering
    \input{images/losses.tex}
    \caption{Typical loss functions for $Y=\bbR$ (right) and cross-entropy loss.}
    \label{fig:losses}
\end{figure}

When $Y$ has a vector space structure, it is natural to consider $\mcL(y,\hat y)=\ell(r)$ where $r=y-\hat y$ is called the residual.
In this case, typical choices for the loss function are appropriate norms on the space $Y$. 
% 
For example, if $Y\in \bbR^n$ one tyically considers $\mcL$ to be the $\ell_1$ or the $\ell_2$. Another common choice is the \emph{Huber error loss}, which ``mixes'' the two. It is defined for $\delta>0$ as
\begin{equation}
    \mfh_\delta(r) = 
    \begin{cases}
        \frac12 \|r\|_2^2, & \text{if }\|r\|_1\le \delta\\
        \delta\left( \|r\|_1 - \frac\delta2 \right)&\text{otherwise.}
    \end{cases}
\end{equation}
See Figure~\ref{fig:losses}, left.

A special mention need to be done in the case for \emph{classification problems}, i.e., problems where the function $\psi$ assigns a discrete label (e.g., cat or dog) to the inputs. In this the relevant values for $\psi$ are $\{0,1\}$, say, but one approximate it with functions taking value in $Y=\bbR$ and interprets values different from $0$ or $1$ as uncertain.
In this case, the most used loss function is the \emph{cross-entropy} function, which is defined by
\begin{equation}
    \mcL(y,\hat y) = -\left[ y \log \hat y + (1-y)\log(1-\hat y) \right].
\end{equation}
This function has a probabilistic interpretation and encodes, roughly speaking, the ``surprise'' of seeing $\hat y$ after the model predicted $y$.
See Figure~\ref{fig:losses}, right.









\chapter{Feed-forward neural networks}


In this chapter we introduce and discuss the simplest example of neural network\footnote{We mostly follow \cite{petersenMathematical2025}, to which we refer for further clarifications.}.
These are obtained by concatenating the simplest possible nonlinear operations:
\begin{equation}
    \phi_\theta(x) = \sigma(Wv + b), \qquad \text{where } \theta=(W,b) \in \bbR^{n\times m}\times \bbR^n.
\end{equation}
Here, the function $\sigma:\bbR\to \bbR$ is a nonlinear \emph{activation} function that is assumed to be applied element-wise to the vector $Wx+b$.
Typically activation functions are depicted in Figure~\ref{fig:activations}.

\begin{figure}[b]
    \centering
    \input{images/activations.tex}
    \caption{Activation functions}
    \label{fig:activations}
\end{figure}

\dfn[one-hidden-layer]{One-hidden layer feed-forward ANN}{
    Let $\sigma:\bbR\to \bbR$ be an activation function and let $d_1\in \bbN$.
    A scalar-valued one-hidden layer feed-forward ANN $\psi_\theta:\bbR^d\to \bbR$ is identified by the set of parameters
    \begin{equation}
        \Theta = \left\{ (W_2,b_2,W_1,b_1)\in \bbR^{1\times d_1}\times \bbR\times\bbR^{d_1\times d}\times \bbR^{d_1} \right\},
    \end{equation}
    and it reads
    \begin{equation}
        \psi_\theta(x) = W_2\, \sigma(W_1+b_1)+b_2 = \sum_{j=1}^{d_1} w_{2j} \sigma\left( \sum_{i=1}^{d} w_{ji}x_i + b_{1j} \right).
    \end{equation}

    Here, the \emph{architecture parameters} of the network are
    \begin{itemize}
        \item $d_1\in \bbN$ is the \emph{width} of the hidden layer;
        \item $W_i$ it the \emph{weight} matrix for the $i$-th layer;
        \item $b_1$ is the \emph{bias} of the \first layer.
    \end{itemize}
    }

In Figure~\ref{fig:one-hidden-layer} we present an example of a one-hidden layer feed-forward ANN

\def\layersep{4cm}
\begin{figure}[t]
    \centering
    \begin{adjustbox}{width=.7\textwidth}
        \input{images/ann-1-layer.tex}
    \end{adjustbox}
    \caption{\label{fig:one-hidden-layer}Graphical illustration of an single hidden layer ANN, yielding a function from $\bbR^3$ to $\bbR$. We only explicited the weights relative to the first variable.}
\end{figure}

The general expression for a feed-forward ANN can then be inferred, see Figure~\ref{fig:deepANN} for a graphical representation.

\dfn[feed-forward-ann]{Feed-forward fully-connected ANN}{
    Let $\sigma:\bbR\to \bbR$ be an activation function and let $d_1,\ldots, d_L\in \bbN$.
    An $N$-hidden layer feed-forward ANN $\phi\theta:\bbR^{d_0}\to \bbR^{d_{L+1}}$ is identified by the set of parameters
    \begin{equation}
        \Theta = \left\{ (W_{L+1}, b_{L+1},\ldots,W_1,b_1) \mid W_{\ell}\in\bbR^{d_{\ell}\times d_{\ell-1}}, \, b_\ell\in \bbR^{d_\ell} \right\}.
    \end{equation}
    The corresponding function is then
    \begin{equation}
        \label{eq:ANN}
        \phi_\theta(x) = \phi^{L+1} \circ \phi^L \circ\cdots\phi^1(x),
    \end{equation}
    where 
    \begin{gather}
        \label{eq:layer-ann}
        \phi^{\ell}: x^{(\ell-1)} \in \bbR^{d_{\ell-1}}\mapsto x^{(\ell)}:=\sigma(W_\ell x^{(\ell-1)}+b_\ell)\in \bbR^{d_\ell}, \qquad \forall \ell\in \llbracket 1,L\rrbracket\\
        \phi^{L+1}: x^{(L)} \in \bbR^{d_{L}}\mapsto x^{(L+1)}:=W_L x^{(L)}\in \bbR^{d_{L+1}}.
    \end{gather}

    Here, the \emph{architecture parameters} of the network are
    \begin{itemize}
        \item $d_\ell\in \bbN$, $\ell\in \llbracket1,L\rrbracket$, is the \emph{width} of the \nth{$\ell$} hidden layer;
        \item $W_\ell$ it the \emph{weight} matrix for the \nth{$\ell$} layer;
        \item $b_\ell$ is the \emph{bias} of the \nth{$\ell$} layer;
        \item $x^{\ell}$ collects the output of the \emph{neurons} in the \nth{$\ell$} layer;
    \end{itemize}



    Neural networks of depth one are called \emph{shallow}, if the depth is larger than one they are called \emph{deep}. 
}

\begin{figure}
    \centering
	% \hspace{-2cm}
    \begin{adjustbox}{width=.9\textwidth}
        \input{images/ann.tex}
    \end{adjustbox}
\caption{\label{fig:deepANN}Graphical illustration of a fully-connected feedforward ANN consisting of
$N+2\in\bbN$ affine transformations (i.e., consisting of $N+1$ layers: one input layer, $N$ hidden layers, and one output layer). Image from \cite{jentzenMathematical2023}.}
% with $l_-1\in\bbN$ neurons on the input layer (i.e., with $l_0$-dimensional input layer), with
% $l_0\in\bbN$ neurons on the \first hidden layer (i.e., with $l_1$-dimensional \first hidden layer),
% with $l_1\in\bbN$ neurons on the \second hidden layer (i.e., with $l_2$-dimensional \second hidden layer),
% $\dots$, with $l_{L-2}$ neurons on the \nth{$(L-1)$} hidden layer (i.e., with $(l_{L-1})$-dimensional \nth{$(L-1)$} hidden layer),
% and with $l_L$ neurons in the output layer (i.e., with $l_L$-dimensional output layer). Image from \cite{jentzenMathematical2023}.}
\end{figure}

\textbf{Remark: }
In this chapter we focus on this type of neural networks, although many adjustments are possible (as we will see in Chapter~\ref{chp:resnets}).
Some notable ones are:
\begin{itemize}
    \item The activation functions $\sigma$ can be different layer to layer, or even neuron to neuron.
    \item The output of layer $\ell$ could not only depend on layer $\ell-1$, but also on any combination of the preceding layers from $0$ to $\ell-1$ (i.e., $\phi^\ell$ can depend on $(x^{(0)},\ldots, x^{(\ell-1)})$). These are called \emph{skip connections} and the corresponding networks are \emph{residual}.
    \item Conversely, information could flow backwards, in the sense that layers $\ell-1$ to $L+1$ could serve as input for layer $\ell$. This creates loops in the flow of information, and one has to introduce a time index $t\in \bbN$, as the output of a node at time step $t$ might be different from the output at time step $t+1$. These networks are called \emph{recurrent}.
\end{itemize}


\section{Learning and backpropagation}

Once a loss function has been chosen, the data-fitting problem of Definition~\ref{def:data-fitting-pb} can be solved using the algorithms presented in Chapter~\ref{chp:numerical-algo}, along with their various extensions.
All of these algorithms, however, require the computation of the gradient of the loss function, which in turn involves differentiating the function $\phi_\theta$ from \eqref{eq:ANN}.  
This task is particularly challenging because $\phi_\theta$ is defined as a composition of many functions. Even in the simplified case where $\sigma$ is the identity (i.e., $\sigma(x) = x$), expanding \eqref{eq:ANN} explicitly leads to a sum with an order of $d_1 \times \dots \times d_N$ terms.  
In contemporary neural networks, the layer dimensions $d_1, \dots, d_N$ are so large that explicitly computing such a sum is completely infeasible, which is why efficient methods like backpropagation are essential.

\ass[]{}{
    Assume that the loss function $\mcL:\bbR^{d_{L+1}}\times \bbR^{d_{L+1}}\to \bbR$ is differentiable, and that a set $\mfX=\{(x_i,y_i) \in \bbR^{d_0}\times \bbR^{d_{L+1}}\mid i\in \llbracket 1,M\rrbracket\}$ is given.
}

Recall that our goal is to minimize the empirical risk 
\begin{equation}
    f(\theta) := \frac{1}{M}\sum_{i=1}^M \mcL(\phi_\theta(x_i),y_i),
\end{equation}
over all possible neural network parameters $\theta=(W_{L+1},b_{L+1},\ldots,W_1,b_1)$.
We thus need to find an efficient way to compute 
\begin{equation}
    \frac{\partial}{\partial b_\ell}\mcL(\phi_\theta(x),y)\in \bbR^{d_\ell}
    \qquad\text{and}\qquad
    \frac{\partial}{\partial W_\ell}\mcL(\phi_\theta(x),y)\in \bbR^{d_\ell\times d_{\ell-1}}.
\end{equation}
We have the following.

\thm[]{Backpropagation}{
    Let $x,y\in \bbR^{d_0}$ be fixed and define $\bar x^{(\ell)},\alpha^{(\ell)}\in \bbR^{d_\ell}$ by
    \begin{equation}
        \label{eq:backprop}
        \begin{cases}
        \bar x^{(1)} = W_1 x + b_1, \\
        \bar x^{(\ell)} = W_\ell \sigma(\bar x^{\ell-1})+b_\ell, \quad \ell\in \llbracket 2,L+1\rrbracket.
        \end{cases}
        \qquad
        \begin{cases}
            \alpha^{(L+1)} = \frac{\partial}{\partial \bar x^{L+1}}\mcL(\bar x^{L+1},y),\\
            \alpha^{(\ell)} = \sigma'(\bar x^{\ell})\odot (W_{\ell+1})^\top \alpha^{(\ell+1)}, \quad \ell\in \llbracket L,1\rrbracket.
        \end{cases}
    \end{equation}
    Here, we denoted by $\odot$ the elementwise product of vectors (i.e., $(v\odot w)_i=v_iw_i$).
    
    Then, it holds
    \begin{gather}
    \frac{\partial}{\partial b_\ell}\mcL(\phi_\theta(x),y) = \alpha^{(\ell)}, \qquad \ell\in \llbracket 1,L+1\rrbracket,\\
        \frac{\partial}{\partial W_1}\mcL(\phi_\theta(x),y) = \alpha^{(1)}x^\top, \qquad\text{and}\qquad 
        \frac{\partial}{\partial W_\ell}\mcL(\phi_\theta(x),y) = \alpha^{(\ell)}\sigma(\bar x^{(\ell-1)})^\top, \qquad \ell\in \llbracket 2,L+1\rrbracket.
    \end{gather}
}

\begin{remark}
    The $(\bar x^{\ell})$ defined in \eqref{eq:backprop} satisfy $x^{(\ell)}=\sigma(\bar x^{\ell})$ for $\ell\in \llbracket1,L\rrbracket$ and $x^{(L+1)}=\bar x^{(L+1)}$, where $x^{(\ell)}=\phi^\ell(x^{(\ell-1)})$ are the outputs of the neural network. 
    For this reason, $\bar x^{(\ell)}$ are called the \emph{preactivations}.
\end{remark}

\begin{remark}
    The above algorithm is called \emph{backpropagation} since it allows to compute the gradients of the loss function via the quantities $\alpha^{(\ell)}$ that are ``backpropagating'' through the network: from the last layer $L+1$ to the \first.
\end{remark}

We start by proving the following.

\lem[alpha-grad]{}{
    It holds that 
    \begin{equation}
        \alpha^{(\ell)} = \frac{\partial}{\partial \bar x^{(\ell)}}\mcL(\phi_\theta(x),y), \qquad \ell\in \llbracket1,L+1\rrbracket
    \end{equation}
}
\begin{proof}
    The statement holds for $L+1$ by definition. Assume by induction that the statement holds for $\ell+1\le L+1$ and compute by the chain rule 
    \begin{equation}
        \frac{\partial\mcL}{\partial \bar x^{(\ell)}} = 
        \left[\frac{\partial\bar x^{(\ell+1)}}{\partial \bar x^{(\ell)}} \right]^\top
        \frac{\partial\mcL}{\partial \bar x^{(\ell+1)}} = 
        \left[\frac{\partial\bar x^{(\ell+1)}}{\partial \bar x^{(\ell)}} \right]^\top
        \alpha^{(\ell+1)}.
    \end{equation} 
    The statement follows by \eqref{eq:backprop} and the direct computation
    \begin{equation}
        \left(\frac{\partial\bar x^{(\ell)}}{\partial \bar x^{(\ell+1)}}\right)_{ij} = (W_\ell)_{ij}\sigma'(\bar x^{(\ell-1)}).
    \end{equation}
\end{proof}

\begin{proof}
   We focus on proving the part concerning the derivative w.r.t.~$W_\ell$, the one w.r.t.~$b_\ell$ being similar.
   Note that $\bar x^{k}$ depends only on $(W_\ell,b_\ell)$ for $k \le \ell$. Hence, the chain rule and Lemma~\ref{th:alpha-grad} yield
   \begin{equation}
    \frac{\partial\mcL}{\partial W_\ell}=\frac{\partial\mcL}{\partial \bar x^{(\ell)}}\frac{\partial\bar x^{\ell}}{\partial W_\ell}
    =\alpha^{(\ell)}\frac{\partial\bar x^{\ell}}{\partial W_\ell}.
   \end{equation}
   The proof follows by definition of the preactivations $\bar x^{(\ell)}$. Indeed, we have
   \begin{eqnarray}
    \frac{\partial \bar x^{(1)}_k}{\partial (W_1)_{ij}} = \delta_{ki} x_j
    &\implies&
    \frac{\partial \bar x^{(1)}}{\partial W_1} = x^\top,\\
    \frac{\partial \bar x^{(\ell)}_k}{\partial (W_\ell)_{ij}} = \delta_{ki}\sigma(\bar x^{(\ell-1)}_j)
    &\implies&
    \frac{\partial \bar x^{(\ell)}}{\partial W_\ell} = \sigma(\bar x^{(\ell-1)})^\top
    , \qquad \ell\in \llbracket 2,L+1\rrbracket. 
   \end{eqnarray}
\end{proof}

\subsection{Issues with learning}
\label{sec:issues-learning}

We conclude this section by discussing two issues that might arise when training deep neural networks using backpropagation.
These are the \emph{vanishing gradient problem} and the \emph{dead neuron problem}.

\paragraph{Vanishing gradient problem}
The vanishing gradient problem arises when the gradients $\alpha^{(\ell)}$ defined in \eqref{eq:backprop} become very small as they are backpropagated through the layers of the network.
This can happen when the activation function $\sigma$ has a derivative $\sigma'$ that is small in magnitude for a large portion of its domain (e.g., the \emph{sigmoid function}).
As a result, the product of these small derivatives can lead to gradients that are effectively zero, making it difficult for the optimization algorithm to update the weights in the earlier layers of the network.
This can slow down or even halt the learning process, as the network struggles to adjust its parameters based on the loss function.

More precisely, if we assume that there exists $c\in (0,1)$ such that $|\sigma'(x)|\le c$ for all $x\in \bbR$, then from \eqref{eq:backprop} we have
\begin{equation}
    \|\alpha^{(\ell)}\| \le c^{L+1-\ell} \|W_{\ell+1}\| \cdots \|W_{L+1}\| \|\alpha^{(L+1)}\|.
\end{equation}
Thus, if $L$ is large and $c<1$, then $\alpha^{(\ell)}$ can become very small, leading to the vanishing gradient problem: the gradients used to update the weights in the earlier layers of the network become so small that they effectively stop learning.

\paragraph{Dead neuron problem}
The dead neuron problem occurs when certain neurons in the network become inactive and stop contributing to the learning process.
This can happen when the activation function $\sigma$ outputs zero for a wide range of inputs, causing the gradients to be zero during backpropagation.
As a result, the weights associated with these neurons do not get updated, leading to a situation where the neuron is effectively "dead" and does not participate in the network's computations.
This issue is particularly common with activation functions like the \emph{ReLU (Rectified Linear Unit)}, which outputs zero for all negative inputs.
If a neuron consistently receives inputs that lead to zero outputs, it may never recover and continue to contribute to the network's learning.  

More precisely, if there exists $a\in \bbR$ such that $\sigma'(x)=0$ for all $x\le a$, then any neuron receiving inputs that are always less than or equal to $a$ will have zero gradients during backpropagation.
Consequently, the weights associated with this neuron will not be updated, leading to a dead neuron that does not contribute to the network's output or learning process.

    
\section{Universal approximation theorem}

In this section we are interested in providing some theoretical results justifying the use of neural networks.
More precisely, we show that (under certain assumptions) we can always find a neural network approximating a given continuous function on compact sets.
To this aim we introduce the following definitions.

\dfn[]{Universal approximator}{
    A set of functions $\mathcal H$ form $\bbR^d$ to $\bbR$ is a \emph{universal approximator} (of $C^0(\bbR^d)$) if for any $\varepsilon>0$, $K\subset\bbR^d$ compact, and $f\in C^0(\bbR^d)$, there exists $g\in \mcH$ such that 
    \begin{equation}
        \|f-g\|_K := \sup_{x\in K}|f(x)-g(x)|\le \varepsilon.
    \end{equation}
    Equivalently, $C^0(\bbR^d)$ is contained in the closure of $\mcH$ with respect to the convergence on compact sets.
}

\dfn[]{Neural network spaces}{
    Let $d,m,L,n\in \bbN$ and $\sigma:\bbR\to \bbR$. The set of all functions realized by ANNs $\phi_\theta:\bbR^d\to\bbR^m$, with at most depth $L$,  width $n$, and activation function $\sigma$ is 
    \begin{equation}
        \mcN_d^m(\sigma;L,n).
    \end{equation}
    Furthermore,
    \begin{equation}
        \mcN_d^m(\sigma;L)=\bigcup_{n\in\bbN}\mcN_d^m(\sigma;L,n).
    \end{equation}
}


\subsection{Shallow neural networks}

In this section, we establish the following result, which stresses why the nonlinear nature of the activation function is essential.

\thm[shallow-uat]{Universal approximation for shallow neural networks}{
    Let $\sigma:\bbR\to \bbR$ be an bounded piecewise continuous function\footnote{Here, we implicitly require that the discontinuity point be locally finite.}. Then, $\mcN_d^1(\sigma;1)$ is a universal approximator of $C^0(\bbR^d)$ if and only if $\sigma$ is not a polynomial.
}

\begin{remark}
    This theorem can be extended to different functional spaces and different norms.
\end{remark}

The difficult part of this theorem (which holds for more general functions \cite{leshnoMultilayer1993}) is to establish the universal approximation property for non-polynomial activation fucntions. The reverse implication is left as an exercice.

The result follows from the following three facts:
\begin{enumerate}
    \item If $\mcN_1^1(\sigma;1)$ is a universal approximator of $C^0(\bbR)$, then $\mcN_d^1(\sigma;1)$ is a universal approximator of $C^0(\bbR^d)$;
    \item If $\sigma\in C^\infty(\bbR)$ is not a polynomial, then $\mcN_1^1(\sigma;1)$ is a universal approximator of $C^0(\bbR)$;
    \item If $\sigma$ is bounded and piecewise continuous, functions in $\mcN_1^1(\sigma;1)$ can approximate a smooth activation function $\tilde \sigma$ which is not a polynomial.
\end{enumerate}
In particular, since $\mcN_1^1(\sigma;1)$ is a universal approximator by the second point, it follows that the same is true for $\sigma$.


Let us start by showing the first point.
An essential tool to establish universal approximation results is the Stone-Weirstrass Theorem, that we hereby recall.
\thm[]{Stone-Weierstrass}{
    Let $K\subset \bbR^d$ be compact, and $\mcH\subset C^0(K,\bbR)$ be such that
    \begin{itemize}
        \item  For all $x\in K$ there exists $f\in \mcH$ such that $f(x)\neq 0$;
        \item  For all $x,y\in K$ there exists $f\in \mcH$ such that $f(x)\neq f(y)$;
        \item $\mcH$ is an algebra of functions, i.e., $\mcH$ is closed under addition, multiplication and scalar multiplication.
    \end{itemize}
    Then, $\mcH$ is dense in $C^0(K,\bbR)$.

    In particular, this holds when $\mcH$ is the space of real-valued polynomias and $K$ is an arbitrary compact set.
}

Then, we have the following.
\lem[point1]{Point 1 of the proof of Theorem~\ref{th:shallow-uat}}{
    Assume that $\mcH$ is a universal approximator of $C^0(\bbR)$. Let $d\in \bbN$, and define  
    \begin{equation}
        \mcX = \operatorname{span}\left\{ x\mapsto g(w\cdot x) \mid w\in \bbR^d,\, g\in \mcH \right\}.
    \end{equation}
    Then, $\mcX$ is a universal approximator of $C^0(\bbR^d)$.
}

\begin{proof}
    Let us consider the space of $k$-homogeneous polynomials 
    \begin{equation}
    \bbH_k = \{ P:\bbR^d\to \bbR\mid P(x) = x^\alpha,\, \alpha\in \bbN^d_0,\, |\alpha|=k\}.
    \end{equation}
    Then, by Stone-Weierstrass Theorem, it suffices to show that $\mcX$ is a universal approximator of $\bbH_k$ for all $k$.

    Assume that this is not the case, namely that the closure $\bar\mcX$ of $\mcX$ w.r.t.~compact convergence is a proper subset of $\bbH_k$ for some $k\in \bbN$. 
    Then, by the Hahn-Banach Theorem, there exists a linear functional $\mfq\in \bbH_k'$ such that $\mfq\neq 0$ and $\ker\mfq\supset\bar\mcX$.
    Let us show that this contradicts the universal approximation property of $\mcH$.

    We start by claiming that the set $\{D^\alpha \mid |\alpha|=k\}$ is a basis for the topological dual $\bbH_k'$. Here, $D^\alpha$ is the derivative with multi-index $\alpha$, and it is immediate to check that\footnote{Recall that $\alpha!=\prod_{i=1}^d \alpha_j!$.} $D^\alpha x^\beta = \delta_{\alpha,\beta}\alpha!$ for any $|\alpha|=|\beta|=k$. Since $p(x)=x^\beta$ for $|\beta|=k$ is a basis of $\bbH_k$, this proves the claim.

    Consider now the $k$-homogeneous polynomial $p_w(x)=(w\cdot x)^k$ with $w\in \bbR^d$, and observe that it holds
    \begin{equation}
        \label{eq:multinomial}
        p_w(x) = \left[ \sum_{i=1}^d w_ix_i \right]^k = \sum_{|\alpha|=k}\frac{k!}{\alpha!}w^\alpha x^\alpha.
    \end{equation}
    By the universal approximation property of $\mcH$ we have that $p_w\in \bar\mcX$ for any $w\in \bbR^d$.
    On the other hand, by the previous claim, we have that $\mfq = \sum_{|\alpha|=k} q^\alpha D^\alpha$ for some $q^\alpha\in \bbR$, and thus by \eqref{eq:multinomial} we get
    \begin{equation}
        0=\mfq(p_w) = k! \sum_{|\alpha|=k}q^\alpha w^\alpha, \qquad \forall w\in \bbR^d \implies \mfq =0.
    \end{equation}
    We have thus reached the desired contradiction.
\end{proof}

\lem[]{Point 2 of the proof of Theorem~\ref{th:shallow-uat}}{
    If $\sigma\in C^\infty(\bbR)$ is not a polynomial, then $\mcN_1^1(\sigma;1)$ is a universal approximator for $C^0(\bbR)$.
}

\begin{proof}
    Let $\mcX=\mcN_1^1(\sigma;1)$. We proceed as in the proof of Lemma~\ref{th:point1} and show that $\mcX$ is a universal approximator for the space of polynomials. This completes the proof by the Stone-Weierstrass Theorem.

    Let $b\in \bbR$, and consider the function 
    \begin{equation}
        f_b(x,w):=\sigma(wx+b), \qquad x,w\in \bbR.
    \end{equation}
    Denoting $\partial_w^k$ the \nth{$k$} derivative w.r.t.~the variable $w$, we have that 
    \begin{equation}
        \partial^k_w f_b(x,w)=x^k\sigma^{(k)}(wx+b)
        \implies
         \partial^k_w f_b(x,w)=x^k\sigma^{(k)}(b).
    \end{equation}
    Since $\sigma\in C^\infty(\bbR)$ is not a polynomial, for any $k$ there exsists $b_k\in \bbR$ such that $\sigma^{(k)}(b_k)\neq 0$. 
    Thus, to prove the statement it suffices to show that $\partial_w^k f_{b}(\cdot,0)\in \mcX$ for any $b$. 

    To this aim, consider the function $\phi_\theta\in \mcX$ given by  
    \begin{equation}
        \phi_{\theta}(x) = \frac{1}{h} \sigma((w+h)x + b) - \frac{1}{h}\sigma(wx+b).
    \end{equation}
    This is a one-hidde layer feed-forward ANN as per Definition~\ref{def:one-hidden-layer} for the choice $d=1$, $d_1 = 2$ and 
    \begin{equation}
        W_1 = \begin{pmatrix}
            w \\ w
        \end{pmatrix},
        \qquad b_1 = \begin{pmatrix}
            b \\ b
        \end{pmatrix},
        \qquad W_2=\begin{pmatrix}
            \frac{1}{h} & \frac{1}{h}
        \end{pmatrix},
        \qquad b_2 = 0.
    \end{equation}
    A simple Taylor development in $w$ shows that for any $h>0$ there exists $\xi\in [w,w+h]$ such that
    \begin{equation}
        \phi_\theta(x) 
        = \partial_w f_b(x,w)+ \frac{h}{2}\partial_w^2 f_b(x,\xi)
        = \partial_w f_b(x,w)+\frac{h}{2}x^2\sigma''(\xi x+b).
    \end{equation}
    But $\sigma''$ is continous, and thus for any compact set $K\subset \bbR$ we have 
    \begin{equation}
        \sup_{x\in K} \sup_{|h|<1}|x^2\sigma''(\xi x+b)| \le 
        \sup_{x\in K} \sup_{|\eta-w|<1}|x^2\sigma''(\eta x+b)| <+\infty. 
    \end{equation}
    In particular, letting $h\to 0$ we see that $\phi_\theta$ converges uniformly on $K$ to $\partial_w f_b(\cdot,w)$. Thus, $\partial_w f_b(\cdot,w)\in \mcX$.
    Applying the argument inductively, shows that $\partial_w^kf_b(\cdot,w)\in \mcX$, completing the proof.
\end{proof}

To conclude the proof, we need to recall the following concept.
\dfn[]{Convolution}{
    Let $f,g\in L^\infty(\bbR)$ and assume that $g$ has compact support. Then the \emph{convolution} of $f$ and $g$ is defined as
    \begin{equation}
        f\star g(x) := \int_\bbR f(x-y)g(y)\, dy.
    \end{equation}
}

\lem[]{Point 3 of the proof of Theorem~\ref{th:shallow-uat}}{
    Assume that $\sigma:\bbR\to \bbR$ is bounded and piecewise continous. Then, for any $\varphi\in C^\infty_c$ we have that $\sigma\star\varphi\in C^\infty_c(\bbR)$ is in the closure of $\mcN_1^1(\sigma;1)$ w.r.t.~uniform convergence on compact sets.

    Moreover, if $\sigma\star \varphi$ is a polynomial for any $\varphi\in C^\infty_c(\bbR)$, then $\sigma$ is a polynomial.
}

\begin{proof}
    The fact that $\sigma\star\varphi\in C^\infty_c(\bbR)$ is standard, and follows observing that $(\sigma\star \varphi)'=\sigma\star (\varphi')$.
    For the proof of the last part of the statement, we refer to \cite[Lemma~3.15]{petersenMathematical2025}.

    Let us build an explicit sequence $f_n\in \mcN_1^1(\sigma;1)$ such that $f_n\rightarrow \sigma\star\varphi$ uniformly on compact sets. 
    Let $a>0$ be such that $\operatorname{supp}\varphi\subset[-a,a]$.  
    Then, for any $n\in \bbN$ we let $y_j=-a+2a/n$ and set
    \begin{equation}
        f_n(x) := \frac{1}N \sum_{j=0}^{N-1} \sigma(x-y_j)\varphi(y_j).
    \end{equation}
    It follows easily that $f_n\in \mcN_1^1(\sigma;1)$. Let us show the required convengerce on an arbitrary compact set $[-b,b]$, $b>0$.
    
    Since $y_{j+1}-y_j\le 1/n$, for any $x\in [-b,b]$ we have 
    \begin{equation}
        \label{eq:stima1}
        |\sigma\star \varphi(x)-f_n(x)| \le
        \sum_{j=0}^{N-1} \left| \int_{y_j}^{y_{j+1}} \left( \sigma(x-y)\varphi(y)-\sigma(x-y_j)\varphi(y_j)\right)\, dy \right|.
    \end{equation}
    Let us bound each term in the sum above. 
    
    Observe that there exists $C>0$ such that 
    \begin{equation}
        \left| \sigma(x-y)\varphi(y)-\sigma(x-y_j)\varphi(y_j)\right|
        \le C, \qquad \forall x,y\in \bbR, \, j\in \llbracket0,n-1\rrbracket.
    \end{equation}
    Then, for any $j\in \llbracket0,N-1\rrbracket$ we can estimate 
    \begin{equation}
        \label{eq:stima2}
        \left| \int_{y_j}^{y_{j+1}} \left( \sigma(x-y)\varphi(y)-\sigma(x-y_j)\varphi(y_j)\right)\, dy \right|
        \le C (y_{j+1}-y_{j}) \le \frac{C}{n}.
    \end{equation} 
    Recall that, by assumption, there exists $\{z_1,\ldots z_M\}\subset [-a,a]$ such that $\sigma$ is continous on $[-a,a]\setminus\{z_1,\ldots,z_M\}$. 
    We will use the above estimate for the at most $M$ intervals containing one of the points $x-z_i$, $i\in \llbracket1,M\rrbracket$.
    Let $J_d$ be the set of such indexes and $J_c=\llbracket0,N-1\rrbracket\setminus J_d$.
    
    Let us now fix $\varepsilon>0$ and such that $\varepsilon\le \min \{ \frac{|(x-z_i)-y_j|}2 \mid i\in \llbracket1,M\rrbracket \text{ and } j\in \llbracket0,n\rrbracket \}$ and consider the compact set
    \begin{equation}
        K = [-a,a] \setminus \bigcup_{i=1}^m (z_i-\varepsilon,z_i+\varepsilon)\supset \bigcup_{j\in J_c} [y_j,y_{j+1}].
    \end{equation}
    Observe that $\sigma$ is uniformly continuous on $K$.
    Then, for any $j\in J_c$ and $y\in [y_j,y_{j+1}]$ we have that $x-y\in K$ and $x-y_j\in K$. 
    Thus,
    \begin{equation}
        \left| \sigma(x-y)\varphi(y)-\sigma(x-y_j)\varphi(y_j)\right|
        \le |\sigma(x-y)-\sigma(x-y_j)|\,|\varphi(y)|+|\sigma(x-y_j)|\,|\varphi(y)-\varphi(y_j)| 
        \le \|\varphi\|_\infty \max_{\substack{s_1,s_2\in K\\|s_1-s_2|\le \frac{2a}n}}|\sigma(s_1)-\sigma(s_2)|
        + \|sigma\|_\infty \max_{\substack{s_1,s_2\in [-a,a]\\|s_1-s_2|\le \frac{2a}n}}|\varphi(s_1)-\varphi(s_2)| := \eta_n.
    \end{equation}
    By uniform continuity of $\sigma$ on $K$ and of $\varphi$ on $[-a,a]$ we have $\eta_n(x)\rightarrow 0$ as $n\to 0$ uniformly for $x\in [-b,b]$.
    Using  \eqref{eq:stima1} and \eqref{eq:stima2}, this shows that
    \begin{equation}
        |\sigma\star \varphi(x)-f_n(x)| \le 
        \sum_{j\in J_d} \frac Cn + \sum_{j\in J_c} \frac{\eta_n(x)}{n} \le \frac{CM}n + \eta_n(x) \rightarrow 0,
    \end{equation}
    uniformly for $x\in [-b,b]$.
\end{proof}

Putting together the above results, we complete the proof of Theorem~\ref{th:shallow-uat}.

\subsection{Deep neural networks}

As a corollary to Theorem~\ref{th:shallow-uat} we have the following.

\cor[deep-uat]{Universal approximation for deep neural netowrks}{
    Let $\sigma:\bbR\to \bbR$ be an bounded piecewise continuous function. Then, for any $L\in \bbN$, $\mcN_d^1(\sigma;L)$ is a universal approximator of $C^0(\bbR^d)$ if and only if $\sigma$ is not a polynomial.
}

The proof reduces to show that it is possible to approximate the identity with a deep network with $L-1$ layers. This is contained in the following.

\prop[deep-identity]{}{
    Let $\sigma:\bbR\to \bbR$ be differentiable and non constant on an open set.
    The, for any $L\in \bbN$ and $\varepsilon>0$, there exists a neural network $\phi\in \mcN_d^1(\sigma;L,d)$ such that 
    \begin{equation}
        |\phi(x)-x|\le \varepsilon, \qquad  \forall x\in K.
    \end{equation}
}

\begin{proof}
   We prove the statement for $L=1$, the extension to $L>1$ being straightforward.
   Let $s^\star\in \bbR$ be such that $\sigma$ is differentiable in a neighborhood of $s^\star$ and $\varsigma:=\sigma'(s^\star)\neq 0$.
    Let $b=(s^\star,\ldots,s^\star)\in \bbR^d$ and, for $\lambda>0$, define 
    \begin{equation}
        \phi_\lambda(x) := \frac{\lambda}{\varsigma} \sigma\left(\frac{x}{\lambda} + b \right) - \frac{\lambda}{\varsigma}\sigma(b).
    \end{equation}
    It is clear that $\phi_\lambda\in \mcN_d^1(\sigma;1,d)$.
    Moreover, a Taylor development easily shows that
    \begin{equation}
        \phi_\lambda(x)-x = \lambda \frac{\sigma(x/\lambda +b)-\sigma(b)}{\varsigma} - x \rightarrow 0, \qquad \text{as }\lambda\to +\infty.
    \end{equation}
    This completes the proof.
\end{proof}


\noindent\textbf{\em Proof of Corollary~\ref{th:deep-uat}:}
    We already know that $\mcN_d^1(\sigma;1)$ is an universal approximator. That is, for any $f\in C(\bbR^d)$, for any $K$ compact and any $\varepsilon>0$ there exists a shallow netowrk $\phi_1\in \mcN_d^1(\sigma;1)$ such that 
    \begin{equation}
        \| f - \phi_1\|_K \le \frac\varepsilon2.
    \end{equation}
    Concatenanting this with the deep network approximating the identity given by Proposition~\ref{prop:deep-identity} completes the proof.
\qed

\subsection{Further results}

We conclude this chapter by mentioning the following remarkable result stating that, for an appropriate activation function, it is possible to approximate every function $f\in C(K)$, where $K\subset \bbR^d$, at every accuracy $\varepsilon>0$ with a neural network of size $O(d^2)$. Remarkably, this size is independent of $f$, of $K$, and even of $\varepsilon$.
We refer to \cite[Section~3.2]{petersenMathematical2025} for a proof, based on the Kolmogorov's superposition theorem.

\thm[]{Universal approximation via superexpressive activations}{
    There exists a continuous activation function $\sigma:\bbR\to \bbR$ such that for every compact $K\subset\bbR^d$, every $\varepsilon>0$, and every $f\in C(K)$, there exists\footnote{That is, $\operatorname{width}\phi = 2d^2+2$ and $\operatorname{depth}\phi=2$} $\phi\in \mcN_d^1(\sigma; 2,2d^2+d)$ such that
    \begin{equation}
        |f(x)-\phi(x)|\le \varepsilon, \qquad \forall x\in K.
    \end{equation}
}

\chapter{Residual neural networks, neural ODEs and control}
\label{chp:resnets}

In Section~\ref{sec:issues-learning} we discussed some issues that might arise when training deep neural networks using backpropagation.
To overcome these issues, a popular approach consists in considering \emph{residual neural networks} (ResNets) \cite{heDeep2016}.
These are defined as follows. 

\dfn[resnet]{Residual neural network}{
    Let $\sigma:\bbR\to \bbR$ be an activation function and let $d_1,\ldots, d_L\in \bbN$.
    An $N$-hidden layer ResNet $\Phi_\theta:\bbR^{d_0}\to \bbR^{d_{L+1}}$ is identified by the set of parameters
    \begin{equation}
        \Theta = \left\{ (W^{(1)}_{\ell}, b^{(1)}_{\ell},W^{(2)}_{\ell}, b^{(2)}_{\ell})_{\ell=1}^{L+1} \mid W_{\ell}^{(i)}\in\bbR^{d_{\ell}\times d_{\ell-1}}, \, b_\ell^{(i)}\in \bbR^{d_\ell}, i =1,2\right\}.
    \end{equation}
    The corresponding function is then
    \begin{equation}
        \label{eq:ANN}
        \Phi_\theta(x) = \Phi^{L+1} \circ \Phi^L \circ\cdots\Phi^1(x),
    \end{equation}
    where, each layer $\Phi^\ell:\bbR^{d_{\ell-1}}\to \bbR^{d_\ell}$ is given by
    \begin{equation}
        \Phi^{\ell}(x) : x^{(\ell-1)} \in \bbR^{d_{\ell-1}} \mapsto x^{(\ell-1)} + \phi^\ell(x^{(\ell-1)}) \in \bbR^{d_\ell},   \qquad \ell\in \llbracket1,L\rrbracket,
    \end{equation} 
    for some standard $2$-layer feed-forward ANN $\phi^\ell:\bbR^{d_{\ell-1}}\to \bbR^{d_\ell}$ defined by   
    \begin{equation}
        \phi^\ell(x) = W_\ell^{(2)} \sigma(W_\ell^{(1)}x+b_\ell^{(1)}) + b_\ell^{(2)}.
    \end{equation}
}

The advantage of ResNets is that the skip connections (i.e., the addition of the input $x^{(\ell-1)}$ to the output of the layer $\phi^\ell$) help in mitigating issues like vanishing gradients during training. Indeed, adapting the backpropagation algorithm to ResNets, one can see that the gradients $\alpha^{(\ell)}$ defined in \eqref{eq:backprop} satisfy
\begin{equation}
    \alpha^{(\ell)} = \alpha^{(\ell+1)} + \left[\frac{\partial \phi^{\ell+1}}{\partial x^{(\ell)}}\right]^\top \alpha^{(\ell+1)}, \qquad \ell\in \llbracket1,L\rrbracket,
\end{equation}
which prevents them from vanishing too quickly.

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{images/resnet.png}
    \caption{Schematic representation of a residual neural network layer. The input $x^{(\ell-1)}$ is transformed by a function $\mcF_\ell$ and then added to the original input to produce the output $x^{(\ell)}$.  Image from \cite{heDeep2016}}
\end{figure}

\section{Control formalism via neural ODEs}

The key observation in \cite{chenNeural2018,haberStable2018} is that ResNets such that $d_1=\ldots=d_L=d$ can be interpreted as a forward Euler discretization of an ordinary differential equation (ODE). 
In particular, letting $u=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$ denote 
\begin{equation}
    F(x,u) :=  W^{(2)} \sigma(W^{(1)} x + b^{(1)}) + b^{(2)}.
\end{equation}
Then, fixing $\eta>0$, the ResNet defined by the parameters $\theta = \{(W_\ell^{(1)},b_\ell^{(1)},\eta W_\ell^{(2)},\eta b_\ell^{(2)})\}_{\ell=1}^{L+1}$ can be rewritten as
\begin{equation}
    x^{(\ell)} = x^{(\ell-1)} + \eta F(x^{(\ell-1)},u_\ell), \qquad \ell\in \llbracket1,L\rrbracket,
\end{equation}
where $u_\ell = (W_\ell^{(1)},b_\ell^{(1)},W_\ell^{(2)},b_\ell^{(2)})$.
Rearranging the terms, we have
\begin{equation}
    \frac{x^{(\ell)} - x^{(\ell-1)}}{\eta} = F(x^{(\ell-1)},u_\ell), \qquad \ell\in \llbracket1,L\rrbracket.
\end{equation}
Thus, letting $\eta:=1/L$, as the number of layers goes to infinity ($L\to +\infty$) the ResNet can be seen as the Euler discretization of the ODE
\begin{equation}
    \frac{dx(t)}{dt} = F(x(t),u(t)), \qquad t\in [0,1],
\end{equation}
with initial condition $x(0)=x\in \bbR^{d_0}$ and control $u$ defined as $u(t) = u_\ell$ for $t\in [(\ell-1)\eta, \ell \eta)$.

This observatoin allows to interpret ResNets as \emph{neural ODEs}, i.e., ODEs whose dynamics is parametrized by a neural network.
Henceforth, for simplicity, we also assume $d_0=d_{L+1}=d$.
More precisely, we have the following.

\dfn[neural-ODE]{Neural ODEs control systems}{
    Let $\sigma:\bbR\to \bbR$ be an activation function, and let $F:\bbR^{d}\times U\to \bbR^{d}$ be defined as\footnote{Here, one could of course consider different and more general architectures for $F$.}
    \begin{equation}
        F(x,u) = W^{(2)} \sigma(W^{(1)} x + b^{(1)}) + b^{(2)},
    \end{equation}
    for $u=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})\in U$, where $U$ is a suitable subset of $\bbR^{d_1\times d}\times \bbR^{d_1}\times \bbR^{d\times d_1}\times \bbR^{d}$.
    
    A \emph{neural ODE} is the controlled ODE
    \begin{equation}
        \dot x(t) = F(x(t),u(t)), \qquad t\in [0,1],
    \end{equation}
    with initial condition $x(0)=x\in \bbR^{d}$ and control $u\in L^\infty([0,1];U)$.

    The \emph{flow} associated to the neural ODE is the map $\Phi_u:\bbR^d\to \bbR^d$ defined as
    \begin{equation}
        \Phi_u(x) := x_u(1),
    \end{equation}
    where $x_u$ is the solution of the neural ODE with control $u$ and initial condition $x$.
}

We summarize the previous observation in the following result.

\thm[neural-ODE]{Neural ODEs}{
    Then, for any control $u\in L^\infty([0,1];U)$ and $\varepsilon>0$ there exists a sufficiently small $\eta>0$ such that the solution $x_u$ of the neural ODE satisfies
    \begin{equation}
         |x_u(\ell/L) - x^{(\ell)}| \le \varepsilon,
         \qquad 
         \forall \ell\in \llbracket1,L\rrbracket,
    \end{equation}
    where $x^{(\ell)}$ are the layers outputs of the ResNet defined by
    \begin{equation}
        x^{(\ell)}
        = x^{(\ell-1)} + \eta F(x^{(\ell-1)},u((\ell-1)\eta)), \qquad \ell\in \llbracket1,L\rrbracket.
    \end{equation}
    }
The above result says that a ResNet with $L\gg1$ layers can be seen as the approximation of the flow of a neural ODE, that is, 
\begin{equation}
    \Phi_u \approx \Phi^{L} \circ \Phi^{L-1} \circ \cdots \circ \Phi^1.
\end{equation}

The advantage of the neural ODE formalism is that it allows to leverage the rich theory of optimal control to study and train deep neural networks.
In particular, we have that 
\begin{itemize}
    \item the \emph{expressivity} of the ResNet can be studied via controllability properties of the corresponding neural ODE,
    \item the \emph{training} of a ResNet with $L$ layers can be seen as the optimal control problem for the corresponding neural ODE with piecewise constant controls with $L$ pieces.
    % \item the \emph{backpropagation} algorithm can be interpreted as the Pontryagin's Maximum Principle. 
\end{itemize}

Since the control system given by the neural ODE is non-linear, we cannot apply directly the results of Chapter~\ref{chp:control}.
The results in the following sections discuss recent advances in this direction and are based on geometric control theory; we refer to \cite{agrachevControl2022,tabuadaUniversal2023} for more details.

\subsection{Training via optimal control}

Let us consider the training problem detailed in Definition~\ref{def:data-fitting-pb}, for a set of data $\mfX=\{(x_i,y_i)\}_{i=1}^N\subset \bbR^d\times \bbR^d$.
Using the neural ODE formalism, the training of a ResNet with $L$ layers is related to the following optimal control problem 
\begin{equation}
    \label{eq:neural-ODE-training}
    \text{Find } u^\star\in L^\infty([0,1];U) \text{ such that }
    u^\star = \arg\min_{u\in L^\infty([0,1];U)} \frac1N \sum_{i=1}^N \ell(\Phi_u(x_i),y_i),
\end{equation}
where $\Phi_u$ is the flow of the neural ODE defined in Definition~\ref{def:neural-ODE}.

Under mild assumptions on $\mcL$ (namely that $\mcL(x,x)=0$ for any $x\in \bbR^d$), we have that the solution to the above is obtained at zero cost if the following controllability property holds.

\dfn[]{Simultaneous controllability}{
    The neural ODE defined in Definition~\ref{def:neural-ODE} is said to be \emph{simultaneously controllable} on a set $\Omega\subset \bbR^d$ if for any $N\in \bbN$, any set of points $\{x_1,\ldots,x_N\}\subset \Omega$ and any set of target points $\{y_1,\ldots,y_N\}\subset \Omega$, there exists a control $u\in L^\infty([0,1];U)$ such that the corresponding flow $\Phi_u$ satisfies
    \begin{equation}
        \Phi_u(x_i) = y_i, \qquad \forall i\in \llbracket1,N\rrbracket.
    \end{equation}
}

Observe that this definition requires to be able to steer \emph{simultaneously} all points $x_i$ to the corresponding points $y_i$ using the same control $u$, and is thus stronger than the standard notion of controllability of Definition~\ref{def:controllability}.

\begin{figure}
    \centering
    \begin{tikzpicture}[>=latex, thick, 
    x={(0.8cm,-0.3cm)}, y={(0cm,1cm)}, z={(0.2cm,0.6cm)}]
        \input{images/ensemble-control.tex}
    \end{tikzpicture}
    \caption{Schematic representation of simultaneous controllability. The same control $u$ is used to steer all points $x_i$ to the corresponding points $y_i$.}
\end{figure}

A result in this direction has been recently obtained in \cite{tabuadaUniversal2023}, where the authors prove simultaneous controllability for neural ODEs with specific activation functions.

\thm[simultaneous-control]{Simultaneous controllability for generic inputs}{
    Assume that the activation function $\sigma:\bbR\to \bbR$ is such that 
    \begin{itemize}
        \item $\sigma$ is Lipschitz continuous and such that  $\sigma'(s)\ge 0$;
        \item there exists $j\in\bbN_0$, $a_1,a_2,a_3\in \bbR$, $a_2\neq 0$, such that $\eta(s):=\sigma^{(j)}(s)$ is injective and such that
        \begin{equation}
            \eta'(s) = a_1 + a_2 \eta(s) + a_3 \eta(s)^2, \qquad \forall s\in \bbR.
        \end{equation}
    \end{itemize}
    Then, the neural ODE defined in Definition~\ref{def:neural-ODE} is simultaneously controllable on the set 
    \begin{equation}
        \Omega = \{ x\in \bbR^d \mid x_i \neq x_j, \, \forall i\neq j\}.
    \end{equation}
}

The assumptions on the activation function are satisfied, for example, by the sigmoid function $\sigma(s) = 1/(1+e^{-s})$ and by the hyperbolic tangent $\sigma(s) = \tanh(s)$.

This approach has two problems, however:
\begin{itemize}
    \item The controllability result does not really provide a constructive (or easily implementable) way to compute the control $u$ steering the points $x_i$ to the points $y_i$;
    \item Striving for exactly zero training error might lead to overfitting issues, which is known to lead to poor generalization properties on unseen data.
\end{itemize}

To this end, an alternative approach studied in \cite{scagliottiEnsembles2022} consists in considering a relaxed version of the simultaneous controllability problem, where one looks for a control $u$ minimizing the cost functional
\begin{equation}
    J(u) = \frac1N \sum_{i=1}^N \| \Phi_u(x_i) - y_i|^2 + \frac{\beta}2  \|u(t)\|_2^2.
\end{equation}


\subsection{Expressivity via controllability}

% In this section we discuss recent results relating the expressivity of ResNets to controllability properties of the corresponding neural ODE. We refer to \cite{agrachevControl2022,tabuadaUniversal2023} for more details.

% \dfn[]{Flow of a neural ODE}{
%     Let $F:\bbR^d\times U\to \bbR^d$ be as in Definition~\ref{def:neural-ODE}, and let $u\in L^\infty([0,1];U)$.
%     The \emph{flow} associated to the neural ODE is the map $\Phi_u:\bbR^d\to \bbR^d$ defined as
%     \begin{equation}
%         \Phi_u(x) := x_u(1),
%     \end{equation}
%     where $x_u$ is the solution of the neural ODE with control $u$ and initial condition $x$.
% }

Observe, that $u\mapsto \Phi_u$ defines a family of diffeomorphisms of $\bbR^d$ parametrized by the controls $u$.
Since by Theorem~\ref{th:neural-ODE} ResNets approximate the flow of neural ODEs, it is natural to study the expressivity of ResNets via the properties of the family of diffeomorphisms $\{\Phi_u \mid u\in L^\infty([0,1];U)\}$.
More precisely, we have the following immediate consequence of Theorem~\ref{th:neural-ODE}.

\prop[]{}{
    Let $\resnet^d(\sigma;m)$ denote the set of all ResNets with activation function $\sigma:\bbR\to \bbR$, input/output dimension $d$, and width at most $m$.
    Then, the closure of $\resnet^d(\sigma;d)$ w.r.t.~uniform convergence on compact sets contains the set
    \begin{equation}
        \{\Phi_u \mid u\in L^\infty([0,1];U)\}.
    \end{equation}
    In particular, if for any compact $K\subset \bbR^d$ the family $\{\Phi_u \mid u\in L^\infty([0,1];U)\}$ is dense in $C^0(K,\bbR^d)$, then $\resnet^d(\sigma;d)$ is a universal approximator of $C^0(\bbR^d,\bbR^d)$.
}

Observe that the density of the family $\{\Phi_u \mid u\in L^\infty([0,1];U)\}$ in $C^0(K,\bbR^d)$ is a much stronger assumption that the notion of controllability defined in Definition~\ref{def:controllability} and the one of simultaneous controllability defined above. 
Indeed, here we need to be able to find a single control that steers any point $x\in K$ to any target point $y:=f(x)$, for a given continuous function $f:K\to \bbR^d$.

Moreover, for topological reasons, it is not possible to approximate arbitrary continuous functions with diffeomorphisms.
Indeed, we have the following.

\prop[]{}{
    For any control $u$, the corresponding flow $\Phi_u$ is homotopic to the identity.
    That is, there exists continuous map $h:[0,1]\times \bbR^d\to \bbR^d$ such that
    \begin{equation}
        \label{eq:homotopy}
        h(0,x) = x, \qquad h(1,x) = \Phi_u(x), \qquad \forall x\in \bbR^d.
    \end{equation}
}

\begin{proof}
    The homotopy is given explicitly by
    \begin{equation}
        h(t,x) = x_u(t),
    \end{equation}
    where $x_u$ is the solution of the neural ODE with control $u$ and initial condition $x$.
\end{proof}

As a consequence of the above, the most we can expect is to approximate continuous functions $f:K\to \bbR^d$ that are homotopic to the identity. The result we present below requires slightly stronger assumptions.

\dfn[]{Monotone analytic homotopy}{
    A function $f:\bbR^d\to \bbR^d$ is monotone homotopic to the identity if there exists an analytic homotopy $h:[0,1]\times \bbR^d\to \bbR^d$ (i.e., $h(0,x)=x$ and $h(1,x)=f(x)$) such that, for any $t\in [0,1]$, the map $h(t,\cdot)$ is monotone, i.e.,
    \begin{equation}
        x_i \le y_i \qquad \forall i\in \llbracket1,d\rrbracket \implies h(t,x)_i \le h(t,y)_i \qquad \forall i\in \llbracket1,d\rrbracket.
    \end{equation} 
}

We then have the following result in \cite{tabuadaUniversal2023}.

\thm[]{Universal approximation of functions homotopic to the identity}{
    Assume that $\sigma$ satisfies the assumptions of Theorem~\ref{th:simultaneous-control}.
    Then, for any compact set $K\subset \bbR^d$ and any continuous function $f:K\to \bbR^d$ that is monotone analytic homotopic to the identity, for any $\varepsilon>0$ there exists a control $u\in L^\infty([0,1];U)$ such that the corresponding flow $\Phi_u$ satisfies
    \begin{equation}
        \| f - \Phi_u\|_K \le \varepsilon.
    \end{equation}

    In particular, $\resnet^d(\sigma;d)$ is a universal approximator of the set of functions that are monotone analytic homotopic to the identity on compact sets.
}

The above theorem can be extended to general continuous functions assuming that the initial and final layers have different dimensions, i.e., $d_0, d_{L+1}\neq d$.

\thm[]{}{
    Assume that $\sigma$ satisfies the assumptions of Theorem~\ref{th:simultaneous-control} and let $d'$ be such that $d=2d'+1$.
    Then, for any compact set $K\subset \bbR^d$ and any continuous function $f:K\to \bbR^{d'}$, for any $\varepsilon>0$ there exists a control $u\in L^\infty([0,1];U)$, an injection $\alpha:\bbR^{d'}\to \bbR^{2d'+1}$, and a projection $\beta:\bbR^{2d'+1}\to \bbR^{d'}$, such that 
    \begin{equation}
        \| f - \beta\circ \Phi_u\alpha\|_K \le \varepsilon.
    \end{equation}

    In particular, letting $\resnet^{d'}(\sigma;2d'+1)$ denote the set of all ResNets with activation function $\sigma:\bbR\to \bbR$, input/output dimension $d'$, and width at most $2d'+1$, we have that $\resnet^{d'}(\sigma;2d'+1)$ is a universal approximator of $C^0(\bbR^{d'},\bbR^{d'})$.
}


