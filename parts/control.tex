\chapter{Controllability}

For this chapter we follow \cite{trelatControl2024}.
% Unless specified otherwise, missing proofs can be found in 

\section{Control systems}

\dfn[]{Control system}{
    A control system in $\bbR^n$ is the differential equation 
    \begin{equation}
        \label{eq:cs-edo}
        \dot x = f(t,x,u),
    \end{equation}
    where
    \begin{itemize}
        \item The \emph{state} is a the function $x: \bbR \to \bbR^n$;
        \item $f:\bbR\times \bbR^n \times \bbR^m \to \bbR^n$ is  of class $C^1$ w.r.t.~$(x,u)\in \bbR^n\times \bbR^m$, and locally integrable w.r.t.~$t\in \bbR$;
        \item The \emph{control} $u:\bbR \times U$ is a measurable and essentially bounded function of time, taking values in $U\subset \bbR^m$.
    \end{itemize}

    The control system is 
    \begin{itemize}
        \item \emph{Linear} is $f(t,x,u) = A(t)x + B(t)u+r(t)$ for $A:\bbR\to \bbR^{n\times n}$, $B:\bbR\to \bbR^{n\times m}$, $r:\bbR\to \bbR^n$. In this case, we assume these functions to be of class $L^\infty$ on every compact interval.
        \item \emph{Autonomous} if $f(t,x,u)=f(x,u)$ is independent of time. Otherwise, the system is \emph{instationary} or \emph{time-varying}.
    \end{itemize}
}

Once a control $u$ and an initial condition $x_0\in\bbR^n$ are fixed, the existence and uniqueness of solutions to the non-autonomous equation \eqref{eq:cs-edo} is guaranteed by the following.

\thm[]{CarathÃ©odory Existence Theorem}{
    Consider the Cauchy problem 
    \begin{equation}
        \label{eq:cauchy-prob}
        \begin{cases}
            \dot x = f(t,x),\\
            x(0)=x_0\in \bbR^n.
        \end{cases}
    \end{equation}
    Assume that $f:\bbR\times \bbR^n\to \bbR^n$ satisfies the following conditions
    \begin{itemize}
        \item $f(t,\cdot)$ is Lipschitz continuous for any $t\in \bbR$ with Lispchitz constant $L(t)$ that is locally integrable;
        \item $f(\cdot,x)$ is measurable for any $x\in \bbR^n$;
        \item there exists $r,M>0$ such that $\|f(t,x)\|_2\le M$ for any $(t,x)\in (-r,r) \times B(0,r)$.
    \end{itemize}
    Then, there exists a unique solution to \eqref{eq:cauchy-prob}, maximally defined on some open interval $I\subset\bbR$ such that $0\in \bbR$.
}

When considering the control system on an interval $[0,T]$, we need its solutions to not blow up before the time $T$. 

\dfn[]{Admissible controls}{
    Let $x_0\in \bbR^n$ and $T>0$. A control $u\in L^\infty([0,T],U)$ is \emph{admissible} on $[0,T]$ at $x_0$ if the associated trajectory $x_u$ of \eqref{eq:cs-edo} such that $x_u(0)=x_0$ is well-defined on $[0,T]$.
    We let 
    \begin{equation}
        \mcU_{x_0,T} = \{ u\in L^\infty([0,T],U) \mid u \text{ is admissible} \}.
    \end{equation}
}

\dfn[]{Controllability}{
    The \emph{end-point mapping} $\End_{x_0,T}$ is defined by
    \begin{equation}
        \End_{x_0,T} :\mcU_{x_0,T}\to \bbR^n ,\qquad 
        \End_{x_0,T}(u) = x_u(T).
    \end{equation}
    The \emph{reachable (or accessible) set} from $x_0$ in time $T>0$ is
    \begin{eqnarray}
        \reach_{x_0,T} =\End_{x_0,T}(\mcU_{x_0,T}) .
    \end{eqnarray}

    The system \eqref{eq:cs-edo} is
    \begin{itemize}
        \item \emph{Globally controllable} from $x_0$ in time $T>0$ if $\End_{x_0,T}$ is surjective, that is,
              \begin{equation}
                  \reach_{x_0,T}= \bbR^n.
              \end{equation}
        \item \emph{Locally controllable} from $x_0$ in time $T>0$ around $x_1\in \reach_{x_0,T}$ if $x_1$ is in the interior of $\reach_{x_0,T}$. That is, if $\End_{x_0,T}$ is locally surjective near $x_1$.
    \end{itemize}
}

\ex[trivial-control]{Trivial control system}{
    Consider the control system, for $x\in \bbR^d$,
    \begin{equation}
        \dot x= u, \qquad u\in L^\infty([0,T],\bbR^d).
    \end{equation}
    Then,
    \begin{equation}
        \End_{x_0,T}(u) = x_0 + \int_0^T u(s)\, ds.
    \end{equation}
    In particular, the system is globally controllable from any $x_0\in \bbR^d$ and in any time $T>0$.
    Indeed, for any $x_1\in \bbR^d$, the constant control 
    \begin{equation}
        u(t) = \frac{x_1 - x_0}{T}, \qquad t\in [0,T],
    \end{equation}
    steers the system from $x_0$ to $x_1$ in time $T$.
}

\ex[]{Not globally controllable system}{
    Consider the control system, for $x\in \bbR$,
    \begin{equation}
        \label{eq:not-globally-contr}
        \dot x = u, \qquad u\in L^\infty([0,T],[0,+\infty)).
    \end{equation}
    Since $\dot x \ge 0$, the system cannot be globally controllable from $x_0\in \bbR$ in any time $T>0$.
    Indeed, proceeding as in Example~\ref{ex:trivial-control} one shows that 
    \begin{equation}
        \reach_{x_0,T}=[x_0,+\infty), \qquad \forall x_0\in\bbR,\,  T>0.
    \end{equation}
    In particular, the system is locally controllable around any $x_1>x_0$.
}

% \ex[]{Not globally controllable system}{
%     Consider the control system, for $x\in \bbR$,
%     \begin{equation}
%         \label{eq:not-globally-contr}
%         \dot x = |x| + u, \qquad u\in L^\infty([0,T],[-1,1]),
%     \end{equation}
%     with initial condition $x_0=0$.
%     Let us show that 
%     \begin{equation}
%         \reach_{0,T} =(-1,+\infty) \qquad  \forall T>0,
%     \end{equation}
%     In particular, the system is not globally controllable from $0$ in any time $T>0$. 
    
%     We start by claiming that $x(t)> -1$ for any $t\ge 0$. 
%     To see this, assume by contradiction that
%     \begin{equation}
%         t_1 := \inf\{ t\ge 0 \mid x(t)<-1\} < +\infty.
%     \end{equation}
%     In particular, by continuity of $x$, there exists $\varepsilon>0$ such that $x(t)<0$ for $t\in [t_1,t_1+\varepsilon]$.
%     Letting 
%     \begin{equation}
%         t_2 := \sup\{t\in [0,t_1]\mid x(s)<0, \, \forall s\in [t,t_1]\},
%     \end{equation}
%     we then have that $x(t)\in[-1,0]$ for $t \in [t_2,t_1+\varepsilon]$, and $x(t_2)=0$. This implies $\dot x \ge -x + 1$ on $[t_2,t_1+\varepsilon]$, and thus
%     \begin{equation}
%         x(t) \ge -1 + e^{-(t-t_2)} > -1 \qquad \forall s\in [t_2,t_1+\varepsilon].
%     \end{equation}
%     This contradicts the definition of $t_1$, and thus proves that $\reach_{0,T} \subset (-1,+\infty)$.

%     To show the converse inclusion, let us exhibit a control steering the system from $x_0=0$ to $x_1$ in time $T>0$.
% }

\section{Controllability of linear autonomous systems}

In this section we consider the linear autonomous control system
\begin{equation}
    \label{eq:cs-linear}
    \dot x = Ax + Bu.
\end{equation}
It is possible to show that there is no blow-up in finite time for linear systems, and thus $\mcU_{x_0,T} = L^\infty([0,T], U)$.

An essential tool for the study of these systems is the variation of constants formula (or Duhamel formula) for its solutions:
\begin{equation}
    \label{eq:duhamel-formula}
    x_u(t) = e^{tA}x_0 + \int_0^t e^{(t-s)A}Bu(s)\, ds, 
    \qquad \forall t\in [0,T], \,  u\in L^\infty([0,T], U).
\end{equation}

We also need to recall the following celebrated result.

\thm[cayley-hamilton]{Cayley-Hamilton Theorem}{
    Let $A\in \bbR^{n\times n}$ be a matrix with characteristic polynomial 
    \begin{equation}
        \chi_A(z) = \det(z\idty - A) = z^n + a_1 z^{n-1}+\ldots +a_{n-1}z + a_n.
    \end{equation}
    Then, letting $p_A(A)$ be the number obtained by replacing the unknown $z$ with the matrix $A$ in this expression, we have $p_A(A)=0$. 
    That it,
    \begin{equation}
        A^n + a_1 A^{n-1} + \ldots + a_{n-1} A + a_n =0.
    \end{equation}
}

In the case of linear systems, it turns out that controllability can be verified via a purely algebraic condition.

\dfn[]{Kalman rank condition}{
    We say that the pair $(A,B)\in \bbR^{n\times n}\times \bbR^{n\times m}$ satisfies the \emph{Kalman rank condition} if the Kalman matrix 
    \begin{eqnarray}
        K=[ B, AB, \ldots, A^{n-1}B] \in \bbR^{  n \times nm},
    \end{eqnarray}
    is of maximal rank $n$.
}

\thm[kalman-thm]{Kalman Theorem}{
    Assume that $U=\bbR^n$. Then, \eqref{eq:cs-linear} is controllable from $x_0\in\bbR^n$ and in time $T>0$ if and only if $(A,B)$ satisfies the Kalman rank condition.

    In particular, if a linear system is controllable from $x_0$ in time $T>0$, then it is controllable from any initial point and in any time.
}

\begin{proof}
    By the variation of constants formula \eqref{eq:duhamel-formula}, we see that controllability is equivalent to the surjectivity of the linear operator 
    \begin{equation}
        \label{eq:Lu}
        L : L^\infty([0,T],\bbR^m) \to \bbR^n, 
        \qquad 
        Lu =\int_0^T e^{-As}Bu(s)\, ds .
    \end{equation}
    Here, we used that the exponential matrix $e^{TA}$ is always invertible.

    Let us prove that the fact that $L$ invertible implies $\rank K=n$. We proceed by contradiction and assume that $\rank K<n$. That is, there exists $p\in \bbR^n$, $p\neq 0$, such that 
    \begin{equation}
        \label{eq:AiB}
        p^\top K = 0 
        \iff
        p^\top A^i B = 0, \qquad \forall i\in \llbracket 1,n\rrbracket.
    \end{equation}
    Recall that by Cayley-Hamilton Theorem, we can write $A^j$ as a linear combination of $\idty, A,\ldots, A^{n-1}$. That is, for any $j\in\bbN$, there exists $a_{0},\ldots, a_{n-1}$ such that
    \begin{equation}
        A^j = \sum_{i=0}^{n-1}a_i A^i.
    \end{equation}
    This implies that \eqref{eq:AiB} actually holds for any $i\in \bbN$, which yields
    \begin{equation}
        p^\top e^{-As}B =  \sum_{j=0}^{+\infty} p^\top \frac{(-As)^j}{j!}B = 0.
    \end{equation}
    In particular, this shows that $p^\top Lu=0$ for any $u\in L^\infty([0,T],\bbR^m)$ proving that $L$ is not surjective.

    To prove the opposite implication, assume that there exists $p\in \bbR^n$, $p\neq0$, such that 
    \begin{equation}
        \label{eq:pLu}
        p^\top Lu=0\qquad \forall u\in L^\infty([0,T],\bbR^m). 
    \end{equation}
    Consider, for $i\in \llbracket 1,n\rrbracket$ and $\tau\in [0,T]$, the control 
    \begin{equation}
        u(\tau) = 
        \begin{cases}
            e_i, & \text{ if } t\in[0,\tau],\\
            0 & \text{ otherwise.}
        \end{cases}
    \end{equation}
    Here, $e_i$ is the $i$-th element of the canonical basis of $\bbR^n$. Thus, we have
    \begin{equation}
        Lu = \int_0^\tau e^{-As}Bu\,ds = \left[\frac{\idty - e^{-\tau A}}{A}\right] B u, \qquad
        \text{where}\qquad 
        \frac{\idty - e^{-\tau A}}{A} = \sum_{j=1}^{+\infty} \frac{(-1)^{j-1}\tau^j}{j!}A^{j-1}.
    \end{equation}
    Assumption \eqref{eq:pLu} then yields 
    \begin{equation}
       0 = p^\top\left[\frac{\idty - e^{-\tau A}}{A}\right]Bu = \sum_{j=1}^{+\infty} \frac{(-1)^{j-1}\tau^j}{j!}p^{\top}A^{j-1}Bu , 
       \qquad \forall \tau\in [0,T].
    \end{equation}
    By analyticity\footnote{Equivalently, one can observe that $$0=\frac{d^k}{d\tau^k}\left[\sum_{j=1}^{+\infty} \frac{(-1)^{j-1}\tau^j}{j!}p^{\top}A^{j-1}Bu\right]_{\tau =0} = p^\top A^{k-1}Bu, \qquad \forall k\in\bbN.$$} w.r.t.~$\tau$ of the right-hand side, this implies that $p^\top A^{j-1}Bu=0$, that is $\rank K<n$.
\end{proof}

\cor[local-contr-with-control-constraint]{Controllability with control constraints}{
    Assume that $0\in \Int(U)$, and that the Kalman condition holds true. Then, the control system is locally controllable around $e^{TA}x_0$ for any $x_0\in \bbR^n$ and any $T>0$. Namely,
    \begin{equation}
        e^{TA}x_0\in \Int\reach(x_0,T) .
    \end{equation}
}

\begin{proof}
    By the variation of constant formula \eqref{eq:duhamel-formula}, we have (for the unconstrained system)
    \begin{equation}
        \End_{x_0,T}(u) = e^{TA}x_0 + Lu,
    \end{equation}
    where $L$ is the operator defined in \eqref{eq:Lu}. 
    Observe that $L$ is a continuous linear map, since 
    \begin{equation}
        \|Lu\|_2 \le T \max_{s\in [0,T]} \|e^{-As}B\| \, \|u\|_\infty.
    \end{equation}
    In particular, $L$ is an open mapping and hence for any neighborhood $V\subset U$ of the origin, we have that $\End_{x_0,T}(V)$ is a neighborhood of $\End_{x_0,T}(0)= e^{TA}x_0\in\reach(x_0,T)$.
\end{proof}


\thm[]{Hautus Test}{
    The following assertions are equivalent 
    \begin{enumerate}
        \item The pair $(A,B)$ satisfies the Kalman rank condition.
        \item $\rank [\lambda\idty - A, B] = n$ for any $\lambda\in \bbC$.
        \item $\rank [\lambda\idty - A, B] = n$ for any $\lambda\in \Spec A$.
        \item For any eigenvector $z$ of $A^\top$, we have $B^\top z \neq 0$.
        \item There exists $c>0$ such that 
        \begin{equation}
            \label{eq:lambda-c}
            \|(\lambda\idty - A^\top) z \|_2^2 + \|B^\top z\|_2^2 \ge c\|z\|_2^2, 
            \qquad
            \forall z\in \bbR^n,\, 
            \forall \lambda\in \bbC.
        \end{equation}
    \end{enumerate}
}

\begin{proof}
    We start by showing the equivalence of assertions 2 to 5.

    \emph{$2\iff 3$} Since $\Spec A \subset \bbC$, we have that assertion 2 implies assertion 3.
    On the other hand, if assertion 3 holds we obtain assertion 2 by recalling that $\lambda\idty - A$ is invertible for any $\lambda \in \bbC\setminus \Spec A$.


    \emph{$3\iff 4$}
    If assertion 4 does not hold for an eigenvector $z$ associated to $\lambda\in \Spec A$, we clearly have $z^\top (\lambda\idty - A)  =z^\top B=0$, which contradicts assertion 3. A similar reasoning shows the opposite implication.

    \emph{$2\iff 5$} If assertion 2 does not hold, we contradict assertion 5 as above. To prove the other implication, let
    \begin{equation}
        M_\lambda = (\bar \lambda\idty - A)(\lambda \idty - A^\top ) + BB^\top.
    \end{equation}
    The matrix $M_\lambda$ is symmetric and it holds
    \begin{equation}
        \label{eq:lambda-c}
        \|(\lambda\idty - A^\top) z \|_2^2 + \|B^\top z\|_2^2 \ge z^\top M_\lambda z
        \qquad
        \forall z\in \bbR^n.
    \end{equation}
    Hence, letting $\mu(\lambda)$ be the smallest eigenvalue of $M_\lambda$, we have assertion 5 with $c = \inf_{\lambda \in \bbC} \mu(\lambda)$. We have that $c>0$ since $\lambda\mapsto \mu(\lambda)$ is continuous and $\mu(\lambda) \rightarrow + \infty$ as $|\lambda|\to +\infty$.

    \emph{$1\iff 4$}
    We are left to showing that assertion 1 is equivalent to the other equivalent assertions. 
    It is immediate to observe that if assertion 4 does not hold, the same is true for assertion 1. 
    To show the opposite implication,  set
    \begin{equation}
        N = \{ z\in\bbR^n\mid z^\top A^k B = 0\, \forall k\in \bbN\}.
    \end{equation}
    In particular, $N = \{0\}$ if and only if $(A,B)$ satisfies the Kalman rank condition. Assume this is not the case. Then, since $N$ is non-trivial $A^\top$ invariant (i.e., $A^\top N \subset N$) subspace, we have that $A^\top$ has at least one non-zero eigenvalue $z\in N\setminus\{0\}$. But then, $B^\top z=0$ by definition of $N$, contradicting assertion 4.
\end{proof}

\subsection{Similar systems and normal forms}

In this section we look at what happens if we perform a change of basis $x_2 = Px_1$ for some $P\in \operatorname{GL}_2(\bbR)$.

\dfn{Similar systems}{
    The linear control systems 
    \begin{equation}
        \dot x_1 = A_1x_1 + B_1 u_1
        \qquad\text{and}\qquad
        \dot x_2 = A_2x_2 + B_2 u_2,
    \end{equation}
    are \emph{similar} if there exists $P\in \operatorname{GL}_2(\bbR)$ such that $A_2= PA_1P^{-1}$ and $B_2 = PB_1$. 
    In this case we say that the pairs $(A_1,B_1)$ and $(A_2,B_2)$ are similar.
}

Observe that the Kalman property is intrinsic, i.e., is invariant under the similarity transformation $P$.
Indeed, letting $K_1$ and $K_2$ be the Kalman matrices associated to two similar systems, we have $K_2=PK_1$.

An important application of similar systems is the existence of various normal forms, i.e., a change of coordinates (and sometimes a change of inputs) that transforms a nonlinear or linear control system into a simpler, standardized structure, where its controllability, observability, or stabilization properties become explicit.

The following result goes in that direction, and can be seen as an extension of Kalaman Theorem (Theorem~\ref{th:kalman-thm}) to non-controllable systems.

\prop[]{}{
    Consider a linear system \eqref{eq:cs-linear} whose Kalman matrix $K$ satisfies $\rank K = r$. Then, letting $y=(y_1,y_2)^\top\in \bbR^{r\times (n-r)}$, the system is similar to 
    \begin{eqnarray}
        \dot y_1 &=& A_1' y_1 + B_1 u + A_3' y_2 \\
        \dot y_2 &=& A'_2 y_2.
    \end{eqnarray}
    In particular, this splits the original system in a controllable part (the variable $y_1$) and an uncontrollable one (the variable $y_2$).
}

\begin{proof}
    Let us assume that $\rank K < n$, otherwise there is nothing to prove.
    Consider the subspace $F=\operatorname{Ran} K$, and observe that it holds 
    \begin{equation}
        F =\operatorname{Ran} B + \operatorname{Ran} AB + \ldots +\operatorname{Ran} A^{n-1}B.
    \end{equation}
    Then, $\dim F=r$ and, using the Cayley-Hamilton Theorem is straightforward to verify that $F$ is invariant under $A$ (i.e., $AF\subset F$).
    Hence, $\bbR^n = F\oplus G$ for some subspace $G$ such that $\dim G=n-r$.
    Pick   a basis $(f_1,\ldots, f_r)$ of $F$, and a basis $(f_{r+1},\ldots, f_n)$ of $G$, and let $P$ be the matrix encoding the change of basis from $(f_1,\ldots, f_n)$ to the canonical basis of $\bbR^n$.

    Using the invariance of $F$ w.r.t.~$A$, we obtain that 
    \begin{equation}
        A' = PAP^{-1} =
        \begin{pmatrix}
            A_1' & A_3' \\ 
            0 & A_2'
        \end{pmatrix},
    \end{equation}
    where $A_1'\in \bbR^{r\times r}$. Moreover, since $\operatorname{Ran} B \subset F$, we have that 
    \begin{equation}
        B' = P B = 
        \begin{pmatrix}
            B_1'\\0
        \end{pmatrix}.
    \end{equation}
\end{proof}

\thm[brunovski-single-input]{Brunovski normal form, single-input case}{
    Consider the linear system \eqref{eq:cs-linear} with scalar input (i.e., $m=1$ so that $B\in \bbR^{n\times 1}$), and assume that $(A,B)$ satisfies the Kalman rank condition.
    Then, letting the characteristic polynomial of $A$ be 
    \begin{equation}
        \chi_A(z) = \det(z\idty - A) = z^n + a_1 z^{n-1} + \ldots + a_{n-1}z + a_n,
    \end{equation}
    the control system is similar to the following chained form 
    \begin{equation}
        \begin{cases}
            \dot x_1 = x_2 \\
            \quad \vdots \\
            \dot x_{n-1} = x_n \\
            \dot x_n = -a_n x_1 - a_{n-1} x_2 -\cdots -a_1 x_n + u.
        \end{cases}
    \end{equation}
}

\begin{proof}
    It suffices to show that the pair $(A,B)$ is similar to $(\tilde A, \tilde B)$ given by 
    \begin{equation}
        \tilde A = 
        \begin{pmatrix}
            0 & 1 &  \cdots & 0 \\
            \vdots & \ddots & \ddots & \vdots \\
            0 & \cdots &  0 & 1 \\
            -a_n & -a_{n-1} &  \cdots & -a_1 
        \end{pmatrix}
        \qquad\text{and}\qquad
        \tilde B = 
        \begin{pmatrix}
            0\\ \vdots \\ 0 \\ 1
        \end{pmatrix}.
    \end{equation} 
    
    Let us define the vectors 
    \begin{equation}
        v_n = B, \, v_{n-1} = A B,\, \ldots ,\, v_1 = A^{n-1}B.
    \end{equation}
    These form a basis, since the Kalman matrix $K = [v_n,\ldots, v_1]$ is full rank. 
    By definition of $v_n$ it is immediate to observe that $B$ transforms to $\tilde B$ under the change of basis defined by $\{v_1,\ldots,v_n\}$.

    Let us check that this is true also for $\tilde A$ with respect to $A$.
    By definition, it trivially holds that 
    \begin{equation}
        A v_{j} = A \left(A^{n-j}B\right) = A^{n-(j-1)}B =  v_{j-1}, \qquad \forall j\in \llbracket 2,n\rrbracket.
    \end{equation}
    In other words, in the basis $\{v_1,\ldots, v_n\}$ the matrix $A$ acts as $\tilde A$ on the last $n-1$ coordinates.
    To compute $A v_1$, we apply Cayley-Hamilton Theorem (Theorem~\ref{th:cayley-hamilton}) to obtain
    \begin{equation}
        A v_1 = A^n B = (-a_{1}A^{n-1}- \ldots - a_{n-1}A-a_n)B = -a_1 v_1 - \ldots -a_n v_n.
    \end{equation}
    This shows that indeed $\tilde A$ corresponds to the matrix $A$ under the change of basis $\{v_1,\ldots,v_n\}$. 
\end{proof}

In the general case $m > 1$, the system decomposes into $m$ \emph{controllability chains} (also called \emph{Jordan chains of the couple} $(A,B)$).
This, however, requires to perform also a linear change of input.
More precisely, we have the following.

\thm[]{Brunovski normal form, general case}{
    Consider the linear system \eqref{eq:cs-linear} and assume that $(A,B)$ satisfies the Kalman rank condition.
    Then there exist invertible matrices
    $P \in \mathbb{R}^{n \times n}$ (change of state coordinates)
    and $R \in \mathbb{R}^{m \times m}$ (change of input coordinates)
    such that, under the transformations
    \[
        x = P \tilde x,
        \qquad
        u = R \tilde u,
    \]
    the system becomes
    \[
        \dot{\tilde x} = A_c \tilde x + B_c \tilde u,
        \qquad\text{with}\qquad
        A_c = T^{-1} A T,
        \qquad
        B_c = T^{-1} B R.
    \]
    Here $(A_c,B_c)$ has the block-diagonal \emph{BrunovskÃ½ form}
    \[
        A_c =
        \begin{bmatrix}
            A_1    & 0      & \cdots & 0      \\
            0      & A_2    & \cdots & 0      \\
            \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & \cdots & A_m
        \end{bmatrix},
        \qquad
        B_c =
        \begin{bmatrix}
            B_1    & 0      & \cdots & 0      \\
            0      & B_2    & \cdots & 0      \\
            \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & \cdots & B_m
        \end{bmatrix},
    \]
    where each block $(A_i,B_i)$ is a single-input BrunovskÃ½ block of size $r_i$, as in Theorem~\ref{th:brunovski-single-input}.
    The integers $r_1,\dots,r_m$ are the \emph{controllability indices} of $(A,B)$ and satisfy
    \[
        r_1 + \cdots + r_m = n.
    \]
}

\section{Controllability of time-varying linear systems}

We now turn to time-varying control systems 
\begin{equation}
    \label{eq:cs-linear-time-var}
    \dot x = A(t)x + B(t)u,
\end{equation}
where $A:\bbR \to \bbR^{n\times n}$ and $B:\bbR\to \bbR^{n\times m}$.

\dfn[]{State-transition matrix}{
    The \emph{state-transition matrix} $R:\bbR\times\bbR \to \bbR^{n\times n}$ of sytem $\dot x = A(t)x$ is the unique solution of 
    \begin{equation}
        \frac{\partial}{\partial t} R(t,s) = A(t) R(t,s), \qquad R(s,s) = \idty.
    \end{equation}
}

We have the following standard result.

\prop[state-transition-matrix]{}{
    Let $R$ be the state-transition matrix of $\dot x = A(t)x$. We have
    \begin{itemize}
        \item In the autonomous case (i.e., $A(t)\equiv A$), we have $R(t,s) = e^{(t-s)A}$.
        \item Semigroup property: It holds 
        \begin{equation}
            R(t,s)R(s,\tau) = R(t,\tau) \qquad \forall t,s,\tau\in\bbR.
        \end{equation}
        In particular, $R(t,s)^{-1}=R(s,t)$.
        \item Solutions to \eqref{eq:cs-linear-time-var}: For any $x_0\in \bbR^n$, $T>0$, and $u\in \mathcal U_{x_0,T}$, we have 
        \begin{equation}
            x(t) = R(t,0)x_0 + \int_0^t R(t,s)Bu(s)\, ds.
        \end{equation}
    \end{itemize}
}

Due to the time-varying nature of the system, a reasonable generalization of Kalman rank condition would be that the Kalman matrix $K(t)$ of $(A(t),B(t))$ be full rank at each time $t>0$. 
This is however too strong, as the following shows.

\ex[]{}{
    Consider the time-varying linear system with $n=2$ and $m=1$:
    \begin{equation}
        \dot x = B(t)u(t), \qquad
        B(t) =
        \begin{cases}
             (1,0)^\top 
             & \text{ if } t\in [0,1], \\
             (0,1)^\top
             & \text{ if } t>1.
        \end{cases}
    \end{equation}
    Since $A(t)=0$ for all times, the instananeous Kalman matrix $K(t)$ is not full-rank. However, it is straightforward to explicitly show that $\reach(x_0,T)=\bbR^2$ for all $T>1$.
}

Indeed, in the time-varying case, the instantaneous lack of controllability for certain directions at a time $t_0$ is not an issue if at later times these directions are controllable. To formalize this idea, we introduce the following.

\dfn[]{Controllability Gramian}{
    The \emph{Gramian matrix} of system \eqref{eq:cs-linear-time-var} at time $T>0$ is the matrix
    \begin{equation}
        G_T := \int_0^T R(T,t)B(t)B(t)^\top R(T,t)^\top \, dt \in \bbR^{n\times n}.
    \end{equation}
}

Before proving the controllability theorem, let us make the following observation.

\prop[gram-matrix-observability]{Observability inequality}{
    Let $T>0$. The Gramian matrix is a symmetric non-negative matrix, whose invertibility is equivalent to the following \emph{observability inequality}: There exists $C_T>0$ such that
    \begin{equation}
        \label{eq:observability}
        \int_0^T \|B(t)^\top R(T,t)^\top \psi\|_2^2 \, dt \ge C_T \|\psi\|_2^2
        \qquad \forall \psi\in\bbR^n.
    \end{equation}
}

\begin{proof}
    The symmetry of $G_T$ is immediate from the definition. Moreover,
    \begin{equation}
        \psi^\top G_T \psi = \int_0^T \psi^\top R(T,t)B(t) B(t)^\top R(T,t)^\top \psi\, dt = \int_0^T \|B(t)^\top R(T,t)^\top \psi\|_2^2 \, dt \ge 0.
    \end{equation}
    This proves both the non-negativity and the equivalence between the invertibility of $G_T$ and \eqref{eq:observability}.
\end{proof}

\begin{remark}
    Inequality \eqref{eq:observability} is called an observability inequality for the following reason. Consider the \emph{adjoint system} to \eqref{eq:cs-linear-time-var}, which is
    \begin{equation}
        \dot z = -A(t)^\top  z, \qquad z(T) = \psi,
    \end{equation}
    and assume that the output $y(t) = B(t)^\top z(t)$ is measured.
    In particular, the energy of this output over $[0,T]$ is the quantity
    \begin{equation}
        E(T) = \int_0^T \|y(t)\|_2^2\, dt.
    \end{equation}
    But, since $y(t) = B(t)^\top R(T,t)^\top \psi$, this coincide with the left-hand side of \eqref{eq:observability}, which can then be recast as
    \begin{equation}
        \int_0^T \|y(t)\|_2^2 \, dt \ge C_T \|z(T)\|_2^2.
    \end{equation}
    Namely, the output $y(t)$ controls the size of the final state $z(T)$.
    In particular, if $y(t)\neq 0$ for all $t\in [0,T]$ we are sure that $z(T)=0$.
    More generally, one can show that this inequality allows to reconstruct the state $z(T)$ from the measurements $y:[0,T]\to \bbR$.
% 
    This property is called \emph{observability of the adjoint system}. 
\end{remark}

The following theorem shows that the observability property introduced above is actually equivalent to the controllability of the original system.

\thm[]{}{
    Assume that $U=\bbR^m$. Then, the control system \eqref{eq:cs-linear-time-var} is controllable from $x_0\in \bbR^n$ in time $T>0$ if and only if the Gramian matrix $G_T$ is invertible.
    In particular, if a linear time-varying system is controllable from $x_0$ in times $T>0$, then it is controllable for any time $T'>T$ and from any initial point.
}

\begin{proof}
    By Proposition~\ref{prop:state-transition-matrix}, given a control $u$ we have that
    \begin{equation}
        \label{eq:end-point-formula}
        \End_{x_0,T}(u) = x_u(T) = x^\star + Lu,
        \qquad \text{where}\qquad
        x^\star = R(T,0)x_0,
        \quad\text{and}\quad
        Lu = \int_0^T R(T,t)B(t)u(t)\, dt
    \end{equation}


    Assume that $G_T$ be invertible and let us prove that $\End_{x_0,T}$ is surjective (i.e., that the system is controllable).
    Fix $x_1\in \bbR^n$ and let us construct a control $u$  of the form $u(t) = B(t)^\top R(t,T)^\top \psi$ for some $\psi\in \bbR^n$ such that $\End_{x_0,T}(u)=x_1$.
    Since this choice of $u$ implies that $Lu = G_T\psi$, we have
    \begin{equation}
        \End_{x_0,T}(u) = x^\star + G_T \psi.
    \end{equation}
    By invertibility of $G_T$ it then suffices to choose $\psi = G^{-1}_T (x_1-x^\star)$.

    Conversely, let us assume that $G_T$ is not invertible. By Proposition~\ref{prop:gram-matrix-observability} we then have that there exists $\psi\in \bbR^n$, $\psi\neq 0$, such that
    \begin{equation}
        \int_0^T \|B(t)^\top R(T,t)^\top \psi\|_2^2 \, dt = 0
        \implies
        B(t)^\top R(T,t)^\top \psi = 0 \qquad \text{for a.e.\ } t\in [0,T].
    \end{equation}
    It follows that
    \begin{equation}
        \psi^\top Lu = 0,
        \qquad \forall u\in L^\infty([0,T],\bbR^m),
    \end{equation}
    where $L$ is defined in \eqref{eq:end-point-formula}.
    Hence,
    \begin{equation}
        \psi^\top \End_{x_0,T}(u) = \psi^\top (x^\star + Lu) = \psi^\top x^\star, \qquad \forall u \in L^\infty([0,T],\bbR^m).
    \end{equation}
    In particular, the range of $\End_{x_0,T}$ is contained in a proper affine subspace of $\bbR^n$:
    \begin{equation}
        \operatorname{ran}\End_{x_0,T} \subset \{ z \in \bbR^n\mid \psi^\top (z-x^\star) = 0\}.
    \end{equation}
    This contradicts the surjectivity of $\End_{x_0,T}$.
\end{proof}

The same argument used to derive Corollary~\ref{th:local-contr-with-control-constraint} can be used to derive the following.

\cor[]{Controllability with control constraints}{
    Assume that $0\in \operatorname{int}(U)$ and that the Gramian matrix $G_T$ is invertible for some $T>0$. 
    Then, the control system is locally controllable around $x_0\in \bbR^n$ in time $T>0$.
}


We conclude this section by stating a proper generalization of the Kalman rank condition to the time-varying case.

\thm[Time-varying Kalman theorem]{}{
    Consider \eqref{eq:cs-linear-time-var} with $U=\bbR^m$ and such that $t\mapsto A(t)$ and $t\mapsto B(t)$ are of class $C^\infty$.
    Define the sequence of matrices 
    \begin{equation}
        B_0(t) = B(t),
        \qquad 
        B_{j+1} = A(t)B_j(t) - \frac{d}{dt}B_j(t), \qquad j\in \bbN.
    \end{equation}
    Then, the system is controllable if there exists $t_0\in [0,T]$ such that 
    \begin{equation}
        \bigcup_{j\in \bbN} \operatorname{ran} B_j(t_0)= \bbR^n.
    \end{equation}
    If, moreover, $t\mapsto A(t)$ and $t\mapsto B(t)$ are analytic, then the above property is equivalent to controllability and independent of $t_0\in [0,T]$.
}


\chapter{Optimal control theory for linear systems}

Given a control system 
\begin{equation}
    \dot x = f(t,x,u), \qquad x(0)=x_0,
\end{equation}
that is controllable at $x_1$ in a certain time $T>0$, it is natural to wonder which control $u\in \mathcal U_{x_0,T}$ is the ``best'' to reach $x_1$.
To answer this question, we need to introduce a cost functional $J:\mathcal U_{x_0,T} \to \bbR$, and then look for a control $u^\star$ minimizing $J$ among all controls that steer $x_0$ to $x_1$ in time $T$.
This is the goal of optimal control theory.

\dfn[]{Optimal control problem}{
Given $x_0\in\bbR^d$, $T>0$, and $x_1\in \reach_{x_0,T}$, the \emph{optimal control problem} associated with a cost functional $J:\mathcal U_{x_0,T}\to \bbR$ is
\begin{equation}
    \label{eq:ocp}
    \begin{cases}
        \dot x = f(t,x,u) \\
        x(0)=x_0 \\
        x(T) = x_1 \\
        J(u) \rightarrow \min.
    \end{cases}
\end{equation}
}
Many variants of this problem exist, e.g., where the initial or final positions are not fixed but belong to a certain subset of the state space, or where the final time is free.

\dfn[]{Admissible controls for \eqref{eq:ocp}}{
The set of \emph{admissible controls} to \eqref{eq:ocp} is 
\begin{equation}
    \Omega_{x_0,x_1,T} = \{ u\in \mathcal U_{x_0,T}\mid x_u(T) = x_1 \}.
\end{equation}
}
Then, \eqref{eq:ocp} can be recast as an (infinite-dimensional) optimization problem under constraints. Namely,
\begin{equation}
    \min_{u\in \Omega_{x_0,x_1,T}} J(u).
\end{equation}

\section{Linear-quadratic optimal control problems}

In this chapter, we follow \cite{jurdjevicGeometric1997}, and we only consider linear autonomous control systems of the form \eqref{eq:cs-linear}.
For this kind of systems, we consider the following quadratic cost functional
\begin{equation}
    J(u) = \int_0^T c(x_u(t),u(t))\, dt,
\end{equation}
where $c:\bbR^n\times \bbR^m \to \bbR$ is a quadratic form. That is,
\begin{equation}
    c(x,u) = \frac12 u^\top Pu + u^\top Qx + \frac12 x^\top R x,
    \qquad P\in \sym(\bbR^{m}), \, Q \in \bbR^{m\times d},\, R\in \sym(\bbR^d).
\end{equation}

The first question to be answered is the existence of solutions to the corresponding linear-quadratic (LQ) optimal control problem:
\begin{equation}
   \label{eq:ocp-linear}
   \begin{cases}
    \dot x = A x + Bu \\
    x(0)=x_0 \\
    x(T) = x_1 \\
    J(u) = \int_0^T c(x_u(t),u(t))\, dt \rightarrow \min.
   \end{cases} 
\end{equation}

\thm[ocp-existence-linear]{Existence of solution to \eqref{eq:ocp-linear}}{
    An optimal solution for \eqref{eq:ocp-linear} exists if and only if 
    \begin{equation}
        J(u) \ge 0 \qquad \forall u \in \Omega_{0,0,T}.
    \end{equation}
    If, moreover, there exists $\alpha>0$ such that $J(u)>\alpha\|u\|_2^2$ for all $u\in \Omega_{0,0,T}$ then the optimal solution is unique.
}

\begin{proof}
    The idea is to use controls in $\tilde u \in \Omega_{0,0,T}$ as variations to test the optimality of $u\in \Omega_{x_0,x_1,T}$.
    Indeed, by linearity of the control system, the control $u_\lambda = u + \lambda \tilde u$ belongs to $\Omega_{x_0,x_1,T}$ for any $\lambda\in \bbR$, i.e., it steers the system from $x_0$ to $x_1$ in time $T$.
    Indeed, by the variation of constants formula \eqref{eq:duhamel-formula} we have
    \begin{equation}
        \End_{x_0,T}(u_\lambda) = e^{TA}x_0 + \int_0^T e^{(T-s)A}B(u + \lambda \tilde u(s))\, ds = x_1 + \lambda \underbrace{\int_0^T e^{(T-s)A}B\tilde u(s)\, ds}_{\End_{0,T}(\tilde u)=0} = x_1.
    \end{equation}
    In particular, letting $x$ (resp.~$\tilde x$, and $x_\lambda$) be the trajectory corresponding to $u$ (resp.~$\tilde u$, and $u_\lambda$), it then follows that $x_\lambda = x + \lambda \tilde x$.
    Then, by the expression of $c(x,u)$, we have
    \begin{equation}
        \label{eq:aaaa}
        c(x_\lambda,u_\lambda)
        = \lambda^2 c(\tilde x, \tilde u) + \lambda \left[\frac12\tilde u^\top P u + \tilde u^\top Q x + u^\top Q \tilde x + \tilde x^\top Rx \right] + c(x, u),
        \qquad \forall t\in [0,T]
    \end{equation}
    
    Assume that $u$ is optimal. In particular, $J(u_\lambda)\ge J(u)$ for all $\lambda\in \bbR$ and $\tilde u\in \Omega(0,0,T)$. Hence, integrating \eqref{eq:aaaa} over $[0,T]$, we then have 
    \begin{equation}
       \lambda^2 \underbrace{\int_0^T c(x_0, \tilde u)\, ds}_{=J(\tilde u)} + \lambda \int_0^T \left[\frac12\tilde u^\top P u^\star + \tilde u^\top Q x^\star + (u^\star)^\top Q x_0 + x_0^\top Rx^\star \right] \, ds \ge 0, 
       \qquad \forall \lambda\in \bbR, \, \tilde u\in \Omega(0,0,T).
    \end{equation}
    This immediately yields $J(\tilde u)\ge 0$ for all $\tilde u\in \Omega(0,0,T)$.

    Let us now prove the converse implication.
    Assume that $J(\tilde u)\ge 0$ for all $\tilde u\in \Omega_{0,0,T}$.
    Up to fixing any $u_0\in \Omega_{x_0,x_1,T}$, we have that for any $u\in \Omega_{x_0,x_1,T}$ it holds $u - u_0 \in \Omega_{0,0,T}$, i.e., $u=u_0+\tilde u$ for some $\tilde u\in \Omega_{0,0,T}$. Hence, minimizing $J$ over $\Omega_{x_0,x_1,T}$ is equivalent to minimizing the functional $J(u_0+\tilde u)$ over $\tilde u\in \Omega_{0,0,T}$.
    From \eqref{eq:aaaa} with $\lambda=1$, this is equivalent to minimizing the functional $\phi:\Omega_{0,0,T}\to \bbR$ defined as
    \begin{equation}
        \phi(\tilde u) = J(\tilde u) + L \tilde u, 
        \qquad \text{where } 
        L \tilde u :=\int_0^T \left[\frac12\tilde u^\top P u_0 + \tilde u^\top Q x + u_0^\top Q \tilde x + \tilde x^\top Rx \right] \, dt, 
    \end{equation}
    Here, $J$ is a quadratic form and $L$ is a linear functional. 
    Even though the problem is posed on $\Omega_{0,0,T}\subset L^\infty([0,T],\bbR^m)$, we can equivalently consider it on the Hilbert space $X\subset L^2([0,T],\bbR^m)$ of $L^2$ controls steering the system from $0$ to itself in time $T$, since $L^\infty$ is dense in $L^2$ and both $J$ and $L$ extends continuously to the latter.
    Let $N = \ker(J) \subset L^2([0,T],\bbR^m)$ and observe that $\ker(L)\supset N$. Indeed, if this was not the case $J(u_0+\lambda \tilde u)$ would be unbounded from below for $\tilde u\in N$ such that $L\tilde u <0$ and $\lambda\to +\infty$, contradicting the non-negativity of $J$ on $\Omega_{0,0,T}$.

    Let now $N^\perp$ be the orthogonal complement of $N$ in $X$.
    Since $J$ is non-negative by assumption, it is strictly positive on $N^\perp$. 
    Thus, $\phi$ is continous, coercive and strictly convex on $N^\perp$. 
    In particular, there exists a unique $\tilde u^\star \in N^\perp$ such that 
    \begin{equation}
        \phi(\tilde u^\star) = \min_{\tilde u\in N^\perp} \phi(\tilde u).
    \end{equation}
    Since $\phi$ is zero on $N$, it follows that $\tilde u^\star$ is a minimizer of $\phi$ on the whole $X$.
    To conclude the proof of the converse implication it then suffices to observe that $\tilde u^\star\in \Omega_{0,0,T}$. We omit a precise argument for this fact, but this can be done exploiting the necessary conditions for optimality of the Pontryiagin Maximum Principle, to be presented in the next section.

    Finally, if $J(u)\ge \alpha\|u\|_2^2$ for all $u\in \Omega_{0,0,T}$, then $J$ is strictly convex on $\Omega_{0,0,T}$, thus on $X$, and thus the minimizer is unique.
\end{proof}

We henceforth consider the following assumptions.

\ass[]{}{
    \begin{itemize}
        \item The pair $(A,B)$ satisfies the Kalman rank condition.
        \item There exists $T_{\max}>0$ such that the functional $J$ is non-negative on $\Omega_{0,0,T_{\max}}$.
        \item The system is \emph{dissipative}, i.e., for any $T\in (0,T_{\max}]$ there exists a control $u\in \Omega_{0,0,T}$ such that $J(u)>0$.
    \end{itemize}
}

The first assumption ensures that the system is controllable, and thus that the optimal control problem \eqref{eq:ocp-linear} can have a solution. 
By Theorem~\ref{th:ocp-existence-linear}, the second assumption guarantees the existence of a solution to \eqref{eq:ocp-linear} for any $T<T_{\max}$ and any $x_0,x_1\in \bbR^d$.
The third assumption is a technical condition, and it guarantees the uniqueness of the solution.

An important consequence of the third assumption is the following.

\prop[dissipativity]{}{
    Let $u\in \Omega_{x_0,x_1,T}$. Then, for any $\varepsilon>0$ there exists a control $u_\varepsilon\in \Omega_{x_0,x_1,T}$ such that 
    \begin{equation}
        J(u_\varepsilon)= J(u)+\varepsilon.
    \end{equation}
}

\begin{proof}
    Let $\bar u \in \Omega_{0,0,T}$ be such that $J(\bar u)>0$, which exists by the dissipativity assumption.
    Then, for any $\lambda\in\bbR$ the control $u_\lambda = u+\lambda\bar u$ belongs to $\Omega_{x_0,x_1,T}$.
    Then, $\lambda\mapsto J(u_\lambda)$ is continuous and strictly increasing, with $J(u_0)=J(u)$ and $\lim_{\lambda\to +\infty} J(u_\lambda) = +\infty$.
    The conclusion follows by the intermediate value theorem.
\end{proof}

Observe that the second assumption is always satisfied if $P$ is non-negative definite (i.e., $c(x,u)$ is strictly convex in $u$ for any fixed $x$).
It turns out that this is actually necessary.

\prop[P-semi-def]{}{
    We have that $P\in \sym_{\ge0}(\bbR^m)$.
}

\begin{proof}
    Assume by contradiction that there exists a splitting $E_1\oplus E_2 = \bbR^m$ such that $P$ is negative definite on $E_2$.
    Then, there exists $K>0$ such that $u^\top P u \le -K \|u\|_2^2$ for all $u\in E_2$.
    This implies that 
    \begin{equation}
        c(x,u) \le -\frac{K}2 \|u\|_2^2 + \|u\|_2\|Q\|\|x\|_2 + \frac12 \|R\|\|x\|_2^2 \qquad \forall (x,u)\in \bbR^d\times E_2.
    \end{equation}
    % As a consequence, there exists $T_0,c>0$ such that for any $T<T_0$ and any control $u\in L^\infty([0,T],E_2)$, with $u\in \Omega_{0,0,T}$ we have $J(u)\le -c\|u\|_{L^2}^2<0$.
    By controllability, for any $T>0$ there exists a control $u\in \Omega_{0,0,T}$ with values in $E_2$ and arbitrarily large $L^2$ norm. 
    But then, the above inequality implies that $J(u)$ can be made arbitrarily negative, contradicting the non-negativity of $J$ on $\Omega_{0,0,T}$.
\end{proof}

\subsection{Pontryagin Maximum Principle for LQ problems}

Let us consider the extended system on $(x,\bar x)\in \bbR^d\times \bbR$, given by 
\begin{equation}
    \begin{cases}
        \dot x = Ax + B u,\\
        \dot{\bar x} = c(x,u).
    \end{cases}
    \qquad 
    (x(0),\bar x(0)) = (x_0,0).
\end{equation}
Denote the corresponding reachable set at time $T>0$ by $\reach^{\text{ext}}_{x_0,T}$.
We then have the following, to be compared with the geometric interpretation for optimality, presented in Section~\ref{sec:geometric-interpretation}.

\thm[]{Optimality and extended system}{
    Assume that the control $u^\star\in \Omega_{x_0,x_1,T}$ is an optimal solution to \eqref{eq:ocp-linear}. 
    Then,
    \begin{equation}
        \label{eq:extended-optimality}
        (x_1, J(u^\star)) \in \partial \reach^{\text{ext}}_{x_0,T}.
    \end{equation}
    Conversely, every point $(x_1,\bar J)\in \partial \reach^{\text{ext}}_{x_0,T}$ corresponds to an optimal solution $u^\star\in \Omega_{x_0,x_1,T}$ with $J(u^\star) = \bar J$.
}

\begin{proof}
    The key observation is that any point $(x_,\bar J)\in \reach^{\text{ext}}_{x_0,T}$ is of the form $(x_u(T), J(u))$ for some $u\in \mathcal U_{x_0,T}$.

    Then, if $u^\star$ is optimal, the extended terminal point $(x_1,J(u^\star))$ must be the lowest point of the line $\{x_1\}\times \bbR$ to belong to $\reach^{\text{ext}}_{x_0,T}$. In particular, this implies \eqref{eq:extended-optimality}.
    
    Conversely, assume that $(x_1,\bar J(\bar u))\in \partial \reach^{\text{ext}}_{x_0,T}$ for some control $\bar u\in \Omega_{x_0,x_1,T}$, but that $v$ is not optimal for \eqref{eq:ocp-linear}.
    Then, there exists a control $v\in \Omega_{x_0,x_1,T}$ such that $J(v)=J(\bar u)-\varepsilon$ for a certain $\varepsilon>0$.
% 
    Given $\delta>0$, let $V_\delta = \{ u \in L^\infty([0,T],\bbR^m)\mid \|u-v\|_\infty<\delta\}$. 
    % Then, the map 
    % \begin{equation}
    %     F:u \in V_\delta \mapsto (x_u(T),J(u)) \in \bbR^{n+1},
    % \end{equation} 
    % is a continous map. 
    Since the system is controllable (see Corollary~\ref{th:local-contr-with-control-constraint}), $\End_{x_0,T}$ is an open mapping, and thus $\End_{x_0,T}(V_\delta)$ covers a neighborhood of $x_1$ in $\bbR^d$.
    This implies that there exists $\delta>0$ sufficiently small such that 
    \begin{equation}
        J(u) \le J(v)+\frac\varepsilon2 = J(\bar u)-\frac{\varepsilon}2, \qquad \forall u\in V_\delta.
    \end{equation}
    In particular, by Proposition~\ref{prop:dissipativity}, for any $y\in \End_{x_0,T}(V_\delta)$ and any $\alpha\in [J(\bar u)-\frac{\varepsilon}2,J(\bar u)+\frac{\varepsilon}2]$ there exists $u_\alpha\in \Omega_{x_0,x_1,T}$ such that $J(u_\eta) = J(\bar u)-\frac{\varepsilon}2 + \eta$. Namely,
    \begin{equation}
       W:=\left[J(\bar u)-\frac{\varepsilon}2,J(\bar u)+\frac{\varepsilon}2\right] \times  \End_{x_0,T}(V_\delta) \subset \reach^{\text{ext}}_{x_0,T}.
    \end{equation}
    Since $W$ is a neighborhood of $(x_1,J(\bar u))$, this contradicts the fact that $(x_1,J(\bar u))\in \partial \reach^{\text{ext}}_{x_0,T}$.
\end{proof}

\ex[]{Convex LQ systems}{
    The LQ optimal control problem \eqref{eq:ocp-linear} is \emph{convex} if $c:\bbR^d\times\bbR^m\to \bbR$ is a convex function. This is the case, e.g., if $P\in \sym_{>0}(\bbR^m)$, $R\in \sym_{\ge0}(\bbR^d)$, and $Q=0$.

    An LQ optimal control problem always satisfies $c(x,u)\ge 0$. Moreover, in this case the reachable set $\reach^{\text{ext}}_{x_0,T}$ is convex.
}


\thm[pmp]{Pontryagin Maximum Principle for LQ problems}{
    Let $u^\star\in \Omega_{x_0,x_1,T}$ be an optimal solution to \eqref{eq:ocp-linear}, and let $x^\star$ be the corresponding trajectory.
    Then, there exists a nontrivial Lipschitz function $p:[0,T]\to \bbR^d$ such that 
    \begin{equation}
        \begin{cases}
            \dot x^\star = Ax^\star + Bu^\star, \\
            \dot p = -A^\top p + Rx^\star + Q^\top u^\star,
        \end{cases}
        \qquad
        \text{and}\qquad
        u^\star(t) = \arg\max_{v\in \bbR^m} H(x^\star(t),p(t),v),
    \end{equation}
    where the Hamiltonian $H:\bbR^d\times \bbR^d\times \bbR^m\to \bbR$ is given by
    \begin{equation}
        H(x,p,u) =p^\top (Ax + Bu) -c(x,u).
    \end{equation}
}

\begin{proof}
    We have that $(x_1,J(u^\star))\in \partial \reach^{\text{ext}}_{x_0,T}$. 
    Let us consider variations of the form $u_\lambda = u^\star + \lambda \tilde u$, where $\tilde u\in L^\infty([0,T],\bbR^m)$.
    Then, by the same argument used in the proof of Theorem~\ref{th:ocp-existence-linear}, we have that $x_\lambda = x^\star + \lambda \tilde x$, where $\tilde x$ is the trajectory corresponding to $\tilde u$ with initial condition $\tilde x(0)=0$. Observe that $x_\lambda(T)\neq x_1$ in general and that, as in \eqref{eq:aaaa}, 
    \begin{equation}
        \label{eq:aaaa2}
        J(u_\lambda) = J(u^\star) + \lambda \int_0^T \left[ \frac{\partial}{\partial u} c(x^\star, u^\star) \,\tilde u + \frac{\partial}{\partial x}c(x^\star, u^\star) \, \tilde x  \right] \, dt + \lambda^2 J(\tilde u).
    \end{equation}

    Consider the map $F:L^\infty([0,T],\bbR^m) \to \bbR^{d+1}$ given by
    \begin{equation}
        F(u) = (x_u(T), J(u)).
    \end{equation}
    Then, $F(u_\lambda) = (x_\lambda(T), J(u_\lambda))$ belongs to $\reach^{\text{ext}}_{x_0,T}$ for all $\lambda\in \bbR$. 
    We compute its directional derivative $L(\tilde u):=DF_{u^\star}(\tilde u) 
        = \lim_{\lambda\to 0} \frac{F(u_\lambda)-F(u^\star)}{\lambda}$ at $u^\star$ in the direction $\tilde u$, obtaining
    \begin{equation}
        L(\tilde u)
        = \left( \tilde x(T), \int_0^T \left[ \frac{\partial}{\partial u} c(x^\star, u^\star) \, \tilde u + \frac{\partial}{\partial x}c(x^\star, u^\star) \, \tilde x  \right] \, dt   \right).
    \end{equation}

    The key observation is then the following: there exists a linear form $p_T:\bbR^d\to \bbR$ such that 
    \begin{equation}
        \label{eq:L-der}
        L(\tilde u) = (\tilde x(T),p_T(\tilde x(T))), \qquad \forall \tilde u\in L^\infty([0,T],\bbR^m).
    \end{equation}
    Indeed, $L$ is a linear map from the infinite-dimensional space $L^\infty([0,T],\bbR^m)$ to the finite-dimensional space $\bbR^{d+1}$, and thus $\operatorname{ran} L$ is a linear subspace of $\bbR^{d+1}$ of dimension at most $d+1$. But, on one hand, controllability implies that $\operatorname{ran} L$ contains the subspace $\bbR^d\times \{0\}$, which has dimension $d$. On the other hand, if $\operatorname{ran} L = \bbR^{d+1}$ then for any $\alpha>0$ there would exists $v$ such that $L(v) = (0,-\alpha)$. 
    In particular, the corresponding trajectory $y$ satisfies $y(T)=0$ and hence $x_\lambda (T) = x^\star(T) +\lambda y(T) = x_1$. 
    As a consequence, \eqref{eq:aaaa2} yields 
    \begin{equation}
    J(u_\lambda) = J(u^\star)-\lambda \alpha+ \lambda^2 J(v), \qquad \forall \lambda\in\bbR 
    \implies J(u_\lambda) < J(u^\star), \qquad \lambda \ll 1.
    \end{equation} 
    This contradicts the optimality of $u^\star$. Hence, $\operatorname{ran} L$ has dimension exactly $d$, and thus it is the graph of a linear form $p_T:\bbR^d\to \bbR$.

    To complete the proof, we need to show that $p_T$ is of the form $p_T(\tilde x(T)) = p(T)^\top \tilde x(T)$ for some absolutely continuous function $p:[0,T]\to \bbR^d$ satisfying the adjoint equation $\dot p = -A^\top p + Rx^\star$.
    \begin{equation}
        \label{eq:adjoint-eq-proof}
        \dot p =-A^\top p + Rx^\star + Q^\top u^\star =  -A^\top p + \frac{\partial}{\partial x}c(x^\star, u^\star).
    \end{equation}
    Let us consider a solution to the above with terminal condition $p(T)=p_T$.
    Then, recalling that $\tilde x = A\tilde x + B \tilde u$ and that $\tilde x(0)=0$, by \eqref{eq:L-der} we obtain
    \begin{equation}
       \int_0^T \left[ \frac{\partial}{\partial u} c(x^\star, u^\star) \, \tilde u + \frac{\partial}{\partial x}c(x^\star, u^\star) \, \tilde x  \right] \, dt 
       = p(T)^\top (x(T)) 
       = \int_0^T \frac{d}{dt} \left[p^\top x\right]\, dt 
       = \int_0^T \left[ \dot p^\top x + p^\top (Ax + Bu) \right]\, dt.
    \end{equation}
    Replacing \eqref{eq:adjoint-eq-proof} in the above, we obtain 
    \begin{equation}
        \int_0^T \left[\frac{\partial}{\partial u} c(x^\star, u^\star) - B^\top p \right]\, \tilde u = 0, \qquad \forall \tilde u\in L^\infty([0,T],\bbR^m). 
    \end{equation} 
    That is, 
    \begin{equation}
       \frac{\partial}{\partial u} c(x^\star, u^\star) - B^\top p = 0 \qquad \text{for a.e.\ } t\in [0,T],  
    \end{equation}
    which is equivalent to the maximization condition $u^\star(t) = \arg\max_{v\in \bbR^m} H(x^\star(t),p(t),v)$, since
    \begin{equation}
        \frac{\partial}{\partial u} H(x,p,u) = \frac{\partial}{\partial u} c(x,u) - B^\top p,
    \end{equation}
    and $H$ is concave in $u$ for any fixed $(x,p)$ due to the fact that $P\in \sym_{\ge0}(\bbR^m)$ by Proposition~\ref{prop:P-semi-def}.
\end{proof}

% \subsection{Lagrangian interpretation of PMP for LQ problems}

% Let $\Lip([0,T], \bbR^d)$ be the space of Lipschitz functions $x:[0,T]\to \bbR^d$ endowed with the norm $\|x\|_{\Lip}=\|x\|_\infty + L_x$, where $L_x$ denotes the Lipschitz constant of $x$. 
% Let us consider the extended cost functional 
% \begin{equation}
%     J: \Lip([0,T],\bbR^d) \times L^\infty([0,T],\bbR^m) \to \bbR, \qquad
%     J(x,u) = \int_0^T c(x(t),u(t))\, dt.
% \end{equation}
% Then, the optimal control problem \eqref{eq:ocp-linear} can be rephrased as the following infinite-dimensional optimization problem with dynamical constraints:
% \begin{equation}
%     \min_{(x,u) \in \Lip([0,T],\bbR^d) \times L^\infty([0,T],\bbR^m)} J(x,u) \qquad\text{subject to}\qquad \dot x = Ax + Bu,\, x(0)=x_0,\, x(T)=x_1.
% \end{equation}
% Assume for simplicity that $Q=0$, so that $c(x,u)=\frac12\left(u^\top P u + x^\top Rx\right)$.
% Following the ideas presented in Chapter~\ref{chp:optimization-problems}, we then introduce the Lagrangian $\mathcal L:\Lip([0,T],\bbR^d) \times L^\infty([0,T],\bbR^m) \times \Lip([0,T],\bbR^d) \to \bbR$ given by
% \begin{equation}
%     \begin{split}
%         \mathcal L (x,u, p)
%          & =  \frac12\int_0^T \left(u^\top P u + x^\top Rx\right)\,dt + \int_0^T p^\top \left(\dot x - Ax-Bu\right)\, dt                                \\
%          & =  \frac12\int_0^T \left(u^\top P u + x^\top Rx\right)\,dt - \int_0^T p^\top \left(Ax+Bu-(x_1-x_0)\right)\, dt -\int_0^T \dot p^\top x\, dt. \\
%     \end{split}
%     \end{equation}
% Here $p\in \Lip([0,T],\bbR^d)$ is the Lagrange multiplier associated with the dynamical constraint $\dot x = Ax + Bu$, and in the last equality we have integrated by parts the term $\int_0^T p^\top \dot x\, dt$ and used the boundary conditions $x(0)=x_0$ and $x(T)=x_1$.

% An optimal solution $(x^\star, u^\star)$ should then satisfy the stationarity condition 
% \begin{equation}
%     \frac{\partial}{\partial (x,u)} \mathcal L(x^\star, u^\star, p) = 0, \qquad \text{for some } p \in\Lip([0,T],\bbR^d).
% \end{equation}
% But this yields:
% \begin{itemize}
%     \item Variation w.r.t.~$u$:
%     \begin{equation}
%         0= \frac{\partial}{\partial u} \mathcal L(x^\star, u^\star, p) = Pu + B^\top p.
%     \end{equation}
%     This yields the control law $u^\star = - P^{-1} B^\top p$.
%     \item Variation w.r.t.~$x$: 
%     % in order to do this, we need to integrate by parts the term $\int_0^T p^\top \dot x\, dt$:
%     % \begin{equation}
%     %     \int_0^T p^\top \dot x\, dt = \left[ p^\top x \right]_0^T - \int_0^T \dot p^\top x\, dt = p^\top (x_1 - x_0) - \int_0^T \dot p^\top x\, dt.
%     % \end{equation}
%     % Then, we obtain 
%     \begin{equation}
%         0 = \frac{\partial}{\partial x} \mathcal L(x^\star, u^\star, p) = R x - \dot p - A^\top p.
%     \end{equation}
%     This yields the adjoint equation $\dot p = -A^\top  p + Rx$.

% \end{itemize}

\subsection{Lagrangian interpretation of PMP for LQ problems}

For simplicity, in this section we assume $Q=0$, so that the running cost takes the form
\[
c(x,u) = \frac{1}{2} \left( x^\top R x + u^\top P u \right).
\]
Let $\Lip([0,T], \mathbb{R}^d)$ denote the space of Lipschitz functions $x:[0,T]\to \mathbb{R}^d$, endowed with the norm 
\[
\|x\|_{\Lip} := \|x\|_\infty + L_x,
\] 
where $L_x$ is the Lipschitz constant of $x$. Consider the extended cost functional
\begin{equation}
    J: \Lip([0,T],\mathbb{R}^d) \times L^\infty([0,T],\mathbb{R}^m) \to \mathbb{R}, 
    \qquad
    J(x,u) = \int_0^T c(x(t),u(t))\, dt.
\end{equation}
Then, the linearâquadratic optimal control problem \eqref{eq:ocp-linear} can be rewritten as an infinite-dimensional optimization problem with dynamical constraints:
\begin{equation}
    \min_{(x,u) \in \Lip([0,T],\mathbb{R}^d) \times L^\infty([0,T],\mathbb{R}^m)} 
    J(x,u)
    \quad \text{subject to} \quad
    \dot x = A x + B u, \quad x(0)=x_0, \quad x(T)=x_1.
\end{equation}

Following the ideas of Chapter~\ref{chp:optimization-problems}, we introduce the Lagrangian
\begin{equation}
    \mathcal{L}(x,u,p) := \frac12 \int_0^T \big( x^\top R x + u^\top P u \big)\, dt
    + \int_0^T p^\top (\dot x - A x - B u)\, dt,
\end{equation}
where $p \in \Lip([0,T], \mathbb{R}^d)$ is the Lagrange multiplier associated with the dynamical constraint $\dot x = Ax + Bu$. 

Integrating the term $\int_0^T p^\top \dot x\, dt$ by parts and using the boundary conditions $x(0)=x_0$, $x(T)=x_1$ gives
\begin{equation}
    \mathcal{L}(x,u,p) 
    = \frac12 \int_0^T \big( x^\top R x + u^\top P u \big)\, dt
    - \int_0^T p^\top (A x + B u)\, dt
    + p(T)^\top x_1 - p(0)^\top x_0 - \int_0^T \dot p^\top x \, dt.
\end{equation}

An optimal solution $(x^\star, u^\star)$ should satisfy the stationarity condition
\begin{equation}
    \frac{\partial}{\partial (x,u)} \mathcal{L}(x^\star, u^\star, p) = 0
    \qquad \text{for some } p \in \Lip([0,T], \mathbb{R}^d).
\end{equation}
By considering appropriate variations of $\mathcal{L}$ with respect to $x$ and $u$, concentrated on small time intervals, it is possible to show that the above condition is equivalent to the corresponding pointwise conditions (i.e., without integration in time).

This yields the following conditions:

\begin{itemize}
    \item {Variation with respect to $u$:} 
    \begin{equation}
        0 %= \frac{\partial \mathcal{L}}{\partial u}(x^\star, u^\star, p)
          = P u^\star + B^\top p,
    \end{equation}
    which gives the optimal feedback law
    \[
        u^\star = - P^{-1} B^\top p.
    \]
    
    \item {Variation with respect to $x$:} 
    \begin{equation}
        0 %= \frac{\partial \mathcal{L}}{\partial x}(x^\star, u^\star, p)
          = R x^\star - \dot p - A^\top p,
    \end{equation}
    which yields the adjoint (costate) equation
    \[
        \dot p = - A^\top p + R x^\star.
    \]
\end{itemize}

Thus, the Pontryagin Maximum Principle (PMP) for the linearâquadratic problem can be interpreted as a first-order stationarity condition for the Lagrangian $\mathcal{L}$, with the adjoint $p$ serving as the Lagrange multiplier for the dynamics.

\section{Linear-quadratic problems with unconstrained terminal state}

The PMP given in Theorem~\ref{th:pmp} for problem \eqref{eq:ocp-linear} presents an important drawback: even though the control $u^\star$ is completely determined by the pair $(x^\star,p^\star)$, the ODE determining the latter cannot be easily solved due to the fact that the boundary conditions are on $x^\star(0)=x_0$ and $x^\star (T)=x_1$. 
Namely, $p^\star$ is determined implicitly and is typically found by ``shooting'' methods (i.e., one chooses initial conditions $x^\star(0)=x_0$ and $p^\star(0)=\lambda$ and then looks for $\lambda$ such that $x^\star(T)=x_1$).

In this section, to circumvent this problem, we consider the following relaxed version of \eqref{eq:ocp-linear}:
\begin{equation}
   \label{eq:ocp-linear-no-terminal}
   \begin{cases}
    \dot x = A x + Bu \\
    x(0)=x_0 \\
    J(u) = \int_0^T c(x_u(t),u(t))\, dt \rightarrow \min.
   \end{cases} 
\end{equation}
Since no terminal condition is required, to avoid trivial optimal solutions $u\equiv 0$ one needs to adjust the cost $J$ by adding a ``final cost'' as
\begin{equation}
    \label{eq:J-no-terminal}
    J(u) = \int_0^T c(x,u)\, dt + x(T)^\top W x(T).
\end{equation}
Here, $W\in \sym_{\ge 0}(\bbR^d)$ is a matrix. 
(Compare with the penalty methods discussed in Section~\ref{sed:saddle-point}.)

For simplicity we consider the following.

\ass[]{}{
    \begin{itemize}
        \item The pair $(A,B)$ satisfies the Kalman rank condition.
        \item The running cost is given by 
        \begin{equation}
            c(x,u)= \frac12 u^\top P u + \frac12 x^\top R x, 
        \end{equation}
        with $P\in \sym_{>0}(\bbR^d)$ and $R\in \sym_{\ge 0}(\bbR^d)$.
    \end{itemize}
}

The results previously discussed still hold in this case (observe that $J(u)>0$ for any non-zero control). 
We resume them in the following and refer to \cite{trelatControle2005} for the proofs.

\thm[pmp-no-terminal]{Pontryiagin Maximum Principle for LQ systems with terminal cost}{
There exists a unique optimal solution $u^\star$ to \eqref{eq:ocp-linear-no-terminal}, with corresponding trajectory $x^\star$. Moreover, there exists a nontrivial Lipschitz function $p^\star:[0,T]\to \bbR^d$ such that
\begin{equation}
    \begin{cases}
        \dot x^\star = Ax^\star + Bu^\star,  \qquad x^\star(0)=x_0\\
        \dot p = -A^\top p + Rx^\star , \qquad p^\star(T)=-Wx(T),
    \end{cases}
\end{equation}
and yielding 
\begin{equation}
    u^\star = P^{-1}B^\top p, \qquad \text{on }[0,T].
\end{equation}
}

We also need the following concept.

\dfn[]{Value function}{
    The \emph{value function} associated with the optimal control problem \eqref{eq:ocp-linear} with final time $T>0$ is the map $V_T:\bbR^d \to \bbR$ defined as
    \begin{equation}
        V_T(x_0) = \inf \left\{ J(u) \mid x_u(0)=x_0\right\}.
    \end{equation}
}

\prop[value-fct]{}{
    For any $T>0$ there exists $c>0$ such that 
    \begin{equation}
        V_s(x_0)\le c\|x_0\|_2^2, \qquad \forall x_0\in \bbR^d, \, s\in [0,T].
    \end{equation}
}

\begin{proof}
    Fix $x_0\in \bbR^d$ and let us consider the control $u\equiv 0$ on $[0,s]$. Then, the corresponding trajectory satisfies $x(t) =e^{tA}x_0$. 
    Observing that $\|x(t)^\top Rx(t)\|_2^2 \le \|R\|\|x(t)\|_2^2$ and that $\|x(t)\|\le \|e^{tA}\|\|x_0\|_2^2$, we get
    \begin{equation}
        |J(0)| =\left| \frac12 \int_0^s x(t)^\top R x(t)\, dt + x(s)^\top W x(s)\right| 
        \le \left[ \frac12\int_0^s \|e^{tA}\|^2 \|R\|\, dt + \|W\|\|e^{sA}\|  \right]\|x_0\|^2_2.
    \end{equation}
    The statement follows since $V_s(0)\le J(0)$.
\end{proof}

\subsection{The Riccati equation and the optimal feedback law}

Although the PMP provides necessary conditions for optimality, it does not directly yield an explicit expression for the optimal control $u^\star$ in terms of the state $x^\star$.
However, in the case of linearâquadratic problems, it is actually possible to express the adjoint state $p^\star$ as a function of the state $x^\star$ via the solution of a matrix differential equation known as the Riccati equation. 
In particular, this allows us to derive an explicit feedback law for the optimal control, i.e., an expression of the form $u^\star(t) = K(t) x^\star(t)$ for some time-varying gain matrix $K(t)$.



We have the following.

\thm[riccati]{Riccati Equation}{
    The unique optimal solution $u^\star$ to \eqref{eq:ocp-linear} is given by %
    \begin{equation}
        \label{eq:optimal-control-riccati}
        u^\star(t) = -P^{-1} B^\top E(t) x^\star(t).
    \end{equation}
    Here, $x^\star$ is the corresponding optimal trajectory, and $E:[0,T]\to \sym(\bbR^d)$ is the unique solution to the matrix Riccati differential equation
    \begin{equation}
        \label{eq:riccati}
        \begin{cases}
            \dot E  =R- A^\top E - E A - E B P^{-1} B^\top E, \\
            E(T) = -W,
        \end{cases}
    \end{equation}
    In particular, the value function is given by 
    \begin{equation}
        \label{eq:value}
        V_T(x_0) = x_0^\top E(0) x_0.
    \end{equation}
}

\begin{proof}
    % By Theorem~\ref{th:ocp-existence-linear}, the positive definiteness of $P$ implies the existence and uniqueness of the optimal control $u^\star\in \Omega_{x_0,x_1,T}$.
    Let $x^\star$ be the optimal trajectory corresponding to the unique optimal control $u^\star$, and let $p^\star$ be the associated adjoint state, satisfying the PMP conditions of Theorem~\ref{th:th:pmp-no-terminal}.
    In particular, we have
    \begin{equation}
        \label{eq:u-star-pmp}
        u^\star(t) = -P^{-1} B^\top p^\star(t).
    \end{equation}
    This implies that the optimal pair $(x^\star,p^\star)$ satisfies the closed-loop system
    \begin{equation}
        \begin{cases}
            \dot x^\star = A x^\star - B P^{-1} B^\top p^\star, \\
            \dot p^\star = -A^\top p^\star + R x^\star,
        \end{cases}
        \qquad
        \text{with } x^\star(0)=x_0, \quad x^\star(T)=x_1, \quad p^\star(T)=-Wx(T).
    \end{equation}
    
    % Looking for a solution of the form $p= E(t) x^\star(t)$ for some matrix function $E:[0,T]\to \bbR^{d\times d}$, one easily checks that $E$ must satisfy the Riccati equation \eqref{eq:riccati}.
    Let $E$ be a solution to \eqref{eq:riccati} (which we will show to exist on $[0,T]$ in Lemma~\ref{lem:existence-riccati}), and observe that $E(t)\in \sym(\bbR^d)$ for all $t\in [0,T]$. 
    Indeed, $E(0)\in \sym(\bbR^d)$, and the right-hand side of \eqref{eq:riccati} is symmetric whenever $E(t)$ is symmetric.
    
    Let now $p=E x$, where $x$ is the solution to \eqref{eq:cs-linear}, i.e., $\dot x = Ax+Bu$, w.r.t.~the feedback control $u=P^{-1}B^\top E x$.
    Then, by \eqref{eq:u-star-pmp}, it follows that 
    \begin{equation}
        \dot p = \dot x^\top E + x^\top \dot E 
        = -A^\top p + R x^\star.
    \end{equation}
    That is, $(u, x, p)$ satisfies the conditions of the PMP. By unicity of the optimal control we then get $u=u^\star$, which in turn implies that $x=x^\star$, and  finally that $p=p^\star$.

    We are left to show \eqref{eq:value}. 
    We have,
    \begin{equation}
        \frac{d}{dt} x^\top E x 
        =\frac{d}{dt} p^\top x 
        = {\dot p}^\top x + p^\top \dot x
        = x^\top R x + p^\top B u
        = x^\top R x + u^\top P u = c(x,u).
    \end{equation}
    Here, we used that a simple computation relying on the feedback expression of $u$ and the fact that $p=Ex$, yields $p^\top B u = u^\top Pu$.
    Finally, recalling that $E(T)=-W$,
    \begin{equation}
        V_T(x) =  \int_0^T c(x^\star,u^\star)\, dt + x(T)^\top W x(T) = \int_0^T \frac{d}{dt} x^\top E x\, dt+ x(T)^\top W x(T) = x(0)^\top E(0) x(0).
    \end{equation}
\end{proof}

\lem[existence-riccati]{Existence of solutions to the Riccati equation}{
    The solution to \eqref{eq:riccati} is defined on $[0,T]$.
}

\begin{proof}
    By contradiction, let us assume that there exists $t_*\in (0,T)$ such that $\|E(t)\|\rightarrow +\infty$ as $t\downarrow t_*$. In particular, this implies that for any $M>0$ there exists $x_0\in \bbR^d$ such that $\|x_0\|_2=1$ and 
    \begin{equation}
        \forall M>0 \qquad \exists x_0\in \bbR^d  \text{ such that } \frac{|x_0^\top E(t_0)x_0|}{\|x_0\|^2}\ge M.
    \end{equation}

    We now claim that for any $t_0\in [0,T)$ and $x_0\in \bbR^d$, there exists $c>0$ such that 
    \begin{equation}
        |x_0^\top E(t_0)x_0|\le c \|x_0\|_2^2.
    \end{equation}
    This contradicts \eqref{eq:contr}, and thus concludes the proof.
    
    Let $t_0\in [0,T)$ and $x_0\in \bbR^d$ and consider $x^\star$ to be the unique optimal trajectory for \eqref{eq:ocp-linear} on the interval $[t_0,T]$ such that $x(t_0)=x_0$ (this exists since $P\in \sym_{>0}(\bbR^d)$). Then, the same computations carried out at the end of the proof of Theorem~\ref{th:riccati}, show that it must hold 
    \begin{equation}
        V_{T-t_0}(x_0)=-x_0^\top E(t_0)x_0.
    \end{equation}
    The statement then follows from Proposition~\ref{prop:value-fct}
\end{proof}

\subsection{The tracking problem}

Consider the problem of finding a trajectory of a linear control system $\dot x = Ax + Bu$ in such a way that it ``best'' follows a given trajectory $\gamma:[0,T]\to \bbR^d$. 
In general the trajectory $\gamma$ is not a solution of the control system, and thus we set up an optimal control problem in order to minimize the error 
\begin{equation}
    z(t) := x(t)-\gamma(t).
\end{equation}
We will also try to minimize the $L^2$ norm of the  control $u$.

% We proceed by linearizing the control system around our trajectory:
% \begin{equation}
%     \dot x = f(t, \gamma + e, u)-\dot \gamma = A e + B u + r.
% \end{equation}
% with 
% \begin{equation}
%     A(t) = \frac{\partial f}{\partial x}(t,\gamma(t),0), 
%     \qquad
%     B(t) = \frac{\partial f}{\partial u}(t,\gamma(t),0), 
%     \qquad 
%     r(t) = f(t,\gamma(t),0) - \dot\gamma(t) + o(e(t),u(t)).
% \end{equation}
% Hence, we consider a linear

Observe that $z$ is a solution of the following control system
\begin{equation}
    \label{eq:z-system}
    \dot z = A z + Bu + r, \qquad z(0) = z_0,
\end{equation}
where $z_0 = x_0-\gamma(0)$ and $r(t) = A\gamma(T)-\dot\gamma(t)$. It is then reasonable to look for controls $u$ minimising a cost in the form \eqref{eq:J-no-terminal}. 
% 


We then have the following.

\prop[]{Tracking problem}{
    Let $\gamma:[0,T]\to \bbR^d$ and consider the tracking problem for the linear control system
    \begin{equation}
        \dot x = A x + Bu, \qquad x(0)=x_0,
    \end{equation}
    with cost 
    \begin{equation}
        J(u) =  (x(T)-\gamma(T))^\top W (x(T)-\gamma(T)) +  \int_0^T \left[(x-\gamma)^\top R(x-\gamma) + u^\top P u\right]\, dt.
    \end{equation}
    Then, there exists a unique optimal control $u$ which is given in feedback form by
    \begin{equation}
        u(t) = P^{-1} B^\top E(t)(x(t)-\gamma(t)) + P^{-1}B^\top h(t),
    \end{equation}
    where $E:[0,T]\to \sym(\bbR^{ d})$ and $h:[0,T]\to \bbR^d$ are the solutions of
    \begin{eqnarray}
        \label{eq:a1}
        \dot E = R - A^\top E-EA-EBP^{-1}B^\top E, \qquad &E(T)=-W,\\
        \label{eq:a2}
        \dot h = -A^\top h - E(A\gamma - \dot \gamma) -EBP^{-1}B^\top h,\qquad &h(T)=0.
    \end{eqnarray}
}

\begin{proof}
    We consider the system \eqref{eq:z-system}.
    In order to get rid of the perturbation term $r$, we augment the dimension of the system, considering the system
    \begin{equation}
        \dot z_1 = A_1z_1+B_1 u,
        \qquad
        z_1(0)=\begin{pmatrix}
            z(0) \\1
        \end{pmatrix},
        \qquad
        A_1 =
        \begin{pmatrix}
            A & r \\ 0 & 0
        \end{pmatrix},
        \qquad
        B_1 =
        \begin{pmatrix}
            B \\ 0
        \end{pmatrix}
    \end{equation}
    where $z_1 = (z,1)^\top \in \bbR^{d+1}$ and $R_1, W_1$ are chosen so that the cost $J(u)$ reads
    \begin{equation}
        J(u) = \frac{1}2\int_0^T \left[ u^\top P u + z_1^\top R_1 z_ 1 \right] \, dt + z_1(T)^\top W_1 z_1(T).
    \end{equation}

    Then, by the preceding section, we have that the optimal control is unique and reads
    \begin{equation}
        u(t) = P^{-1}B_1^\top E_1(t)z_1(t),
    \end{equation}
    where $E_1:[0,T]\to \sym(\bbR^{d+1})$ satisfies the corresponding Riccati equation. 
    Let
    \begin{equation}
       E_1 = 
       \begin{pmatrix}
        E & h \\ h^\top & \alpha 
       \end{pmatrix} ,
    \end{equation}
    for $E\in \sym(\bbR^d)$, $h\in \bbR^d$ and $\alpha\in \bbR$. Then, the Riccati equation yields \eqref{eq:a1}, \eqref{eq:a2}, and
    \begin{equation}
        \dot \alpha = -2r^\top h - h^\top BP^{-1}B^\top h, \qquad \alpha(T) = 0.
    \end{equation}
    The statement follows by the formula \eqref{eq:optimal-control-riccati} in Theorem~\ref{th:riccati} for the optimal control.
\end{proof}