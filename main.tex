\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{biblio.bib}

\begin{document}
	
	\thispagestyle{empty}
	\mytitleb{Optimisation, Control and Data}{Dario Prandi}{dario.prandi@centralesupelec.fr}{2025}
	\newpage% or \cleardoublepage
	\tableofcontents
	
	\part{Optimisation}
	\chapter{Convex Analysis}

	This chapter closely follows chapter 5 of the lecture notes \cite{fornasierFoundations}.
	For most of the proof in this chapter we refer to \cite{rockafellarConvex2015}.

	\section{Convex sets}

	\dfn[convex-set]{}{
	 A set $K\subset \bbR^d$ is \emph{convex} if 
				\begin{equation}
					t x +(1-t)y \in K \qquad \forall x,y \in K, \, t\in [0,1]
				\end{equation}
	}

	We have the following fact.

	\prop[convex-comb]{}{
		The set $K\subset\bbR^d$ is convex if and only if for any $n\in\bbN$, and $t_1,\ldots, t_n\ge 0$ such that $\sum_{i=1}^n t_i=1$, it holds 
		\begin{equation}
			\label{eq:convex-comb}
			x_1,\ldots,x_n\in K \implies \sum_{i=1}^n t_ix_i \in K.
		\end{equation}
	}

	 \begin{proof}
		Property \eqref{eq:convex-comb} with $n=2$ is exactly the definition of $K$ is convex. The statement then follows by induction on $n$. 
	 \end{proof}

	\dfn{}{
		The convex hull $\operatorname{conv}(\Omega)$ of $\Omega\subset \bbR^d$ is the smallest convex set $K$ containing $\Omega$.
	}

	By Proposition~\ref{prop:convex-comb}, it is immediate to observe that 
	\begin{equation}
		\operatorname{conv}(\Omega) = \left\{ \sum_{i=1}^n t_i x_i \mid t_i\ge 0, \sum_{i=1}^n t_i = 1,\, x_i \in \Omega \right\}.
	\end{equation}

	\ex{Convex sets}{
		\begin{itemize}
			\item Unit ball w.r.t.~any norm.
			\item Vector subspaces.
			\item Hyperplanes, i.e., for any $v\in \bbR^d$ and $\lambda\in \bbR$,
			\begin{equation}
				\label{eq:hyperplane}
				H_{v,\lambda} := \{x\in \bbR^d \mid \langle v, x \rangle \ge \lambda\}.
			\end{equation}
		\end{itemize}
	}

	An important result (that we will not prove) on convex sets is the following.

	\thm{Separation theorem}{
		Let $K_1,K_2\subset \bbR^d$ be two convex sets such with disjoint interior. Then there exists $v\in \bbR^d$ and $\lambda \in \bbR$ such that 
		\begin{equation}
			K_1\subset H_{v,\lambda}, \qquad K_2 \subset \bbR^d \setminus H_{v,\lambda}. 
		\end{equation}
		Here, $H_{v,\lambda}$ is defined in \eqref{eq:hyperplane}.
	}

	As a direct consequence, we have the following (see Figure~\ref{fig:supporting-hyperplane}).

	\begin{figure}
		\centering
		\includegraphics[width=.3\textwidth]{images/supporting-hyperplane.png}
		\caption{Supporting hyperplane for a set $S$.}
		\label{fig:supporting-hyperplane}
	\end{figure}

	\cor[supporting-hyperplane]{Supporting hyperplane theorem}{
		Let $K\subset \bbR^d$ be a convex set and $x\in\partial K$. Then, there exists a supporting hyperplane of $K$ containing $x_0$. 
		That is, there exists $v\in\bbR^d$ and $\lambda\in\bbR$ such that $K\subset H_{v,\lambda}$ and $x_0\in \partial H_{v,\lambda}$.
	}

	When $K$ is a convex polygon, it is natural to expect it to be determined by its vertices. In order to formalize this intuition we need the following.

	\dfn{}{
		Let $K\subset\bbR^d$ be a convex set. A point $x\in K$ is an extremum of $K$ if for any $y,z\in K$ and $t\in (0,1)$ we have that
		\begin{equation}
			x = t y +(1-t) z \implies x=y=z.
		\end{equation}
		The set of exterma of $K$ is denoted by $\operatorname{extr}(K)$.
	}

	In particular, for a convex polygon $\operatorname{extr}(K)$ is the set of its vertices.

	\prop{}{
		Let $K\subset\bbR^d$ be a convex set that is compact. Then,
		\begin{equation}
			\operatorname{conv}(K)= \operatorname{conv}\left(\operatorname{extr}(K)\right).
		\end{equation}
	}

	\section{Cones}

	\dfn{}{
		A set $K\subset \bbR^d$ is a cone if 
		\begin{equation}
			tx \in K \qquad \forall x\in K, \, t\ge 0.
		\end{equation}
	}

	Observe that every cone contains the origin.

	\ex{Cones}{
		\begin{itemize}
			\item The second order cone $$C = \{ x=(x',x_n)\in \bbR^{d}\times \bbR \mid \|x'\|_2 \le x_n\}.$$
			\item Positive orthant $\bbR_+^d = \{ x\in\bbR^d \mid x_i\ge 0, \quad \forall i\in \llbracket 1,d\rrbracket \}$.
			\item The set of positive semidefinite matrices $\operatorname{Sym}_+(\bbR^d)$.
		\end{itemize}
	}

	\dfn{}{
		The conic hull $\operatorname{cone}(\Omega)$ of a set $\Omega\subset \bbR^d$ is the smallest cone containing $\Omega$. Namely,
		\begin{equation}
			\operatorname{cone}(\Omega) = \left\{ \sum_{i=1}^n t_i x_i \mid t_i\ge 0\text{ and } x_i\in \Omega  \text{ for any } i \in \llbracket 1,n\rrbracket   \right\}.
		\end{equation}
	}

	\dfn{}{
		The polar cone $K^*$ of a cone $K\subset\bbR^d$ is the set 
		\begin{equation}
			K^* := \left\{ y\in\bbR^d \mid \rangle x,y \langle \ge 0 \quad \forall x\in K \right\}.
		\end{equation}
	}

	\begin{figure}[t]
		\centering
		\includegraphics[width=.3\textwidth]{images/polar-cone-1}
		\hspace{.1\textwidth}
		\includegraphics[width=.3\textwidth]{images/polar-cone-2}
		\caption{Two examples of polar cone.}
	\end{figure}

	We 	have the following properties for the polar cone.

	\prop{}{
		The polar cone $K^*$ is a closed, convex cone. If, moreover, the cone $K$ is closed, then $K^**=K$.
	}

	\section{Convex functions}

	We will work with extended functions $F:\bbR^d\to \bbR\cup\{+\infty\}$. The domain of an extended function is 
	\begin{equation}
		\operatorname{dom}(F) = \{x\in \bbR^d\mid F(x)<+\infty\}.
	\end{equation}
	An extended function such that $\operatorname{dom}(F)\neq \varnothing$ is called \emph{proper}.
	
	Given a standard function $F:\Omega\to \bbR$, we can identify it with the extended function $\bar F:\bbR^d\to \bbR\cup\{+\infty\}$ defined by
	\begin{equation}
		\bar F(x) = 
		\begin{cases}
			F(x) & \text{if }x\in \Omega,\\
			+\infty & \text{if }x\in \bbR^d\setminus \Omega.
		\end{cases}
	\end{equation}

	\dfn{Convex functions}{
		Let $F:\bbR^d\to\bbR\cup\{+\infty\}$ be an extended function. Then, 
		\begin{itemize}
			\item $F$ is convex if 
			\begin{equation}
				F(tx + (1-t)y) \le t F(x) + (1-t)F(y) \qquad \forall x,y\in \bbR^d,\, t\in [0,1].
			\end{equation}
			\item $F$ is strictly convex if 
			\begin{equation}
				F(tx + (1-t)y) < t F(x) + (1-t)F(y) \qquad \forall x,y\in \bbR^d,\, x\neq y, \, t\in [0,1].
			\end{equation}
			\item $F$ is strongly convex if there exists $\gamma>0$ such that
			\begin{equation}
				F(tx + (1-t)y) \le t F(x) + (1-t)F(y) - \frac{\gamma}{2}t(t-1)\|x-y\|_2^2 \qquad \forall x,y\in \bbR^d,\, t\in [0,1].
			\end{equation}
		\end{itemize}

		We say that $F$ is \emph{concave} if $-F$ is convex.
	}

	Observe that it holds 
	\begin{equation}
		\text{convex} \impliedby \text{strongly convex} \impliedby \text{strictly convex}  
	\end{equation}

	We say that a standard function $F:K\to \bbR$ is convex, strictly convex, strongly convex, or concave, if the same is true for its extension $\bar F$. Observe that this requires $K$ to be convex.

	\ex{}{
		\begin{itemize}
			\item 	The prototypical convex function, used in the definition of strongly convex, is the quadratic function 
		\begin{equation}
			F(x) = \frac{\|x\|_2^2}{2} = \frac{1}{2}\sum_{i=1}^d |x_i|^2.
		\end{equation}
			\item More generally, every norm is convex.
			\item The norm $\ell_p$ is strictly convex if and only if $p\in (1,+\infty)$.
			\item $F(x) = x^\top A x$ is convex if $A$ is positive semidefinite (i.e.,  $A\in \operatorname{Sym}_{\ge 0}(\bbR^d)$), and strongly convex if $A$ is positive definite.
		\end{itemize}
	}

	\prop[epigraph]{}{
		A function $F:K\to\bbR$ is convex if and only if its epigraph $\operatorname{epi}(F)\subset \bbR^{d+1}$ is convex. Here, we let 
		\begin{equation}
			\operatorname{epi}(F) = \{ (x,r) \mid r\ge F(x) \}.
		\end{equation}
	}

	\begin{figure}
		\begin{minipage}{.48\textwidth}
			\centering
			\includegraphics[width=.6\textwidth]{images/epigraph.png}
			\caption{Epigraph of a function.}
		\end{minipage}
		\begin{minipage}{.48\textwidth}
			\centering
			\includegraphics[width=.6\textwidth]{images/convexity-diff.png}
			\caption{Graphical representation of Proposition~\ref{prop:convexity-diff}.}
		\end{minipage}
	\end{figure}

	\begin{proof}
		Assume $F$ is convex and let $(x,r),(y,s)\in\operatorname{epi}(F)$. 
		In particular, $r\ge F(x)$ and $s\ge F(y)$.
		Let $t\in [0,1]$ and observe that
		\begin{equation}
			t r +(1-t)s \ge t F(x)+(1-t)F(y) \ge  F(tx+(1-t)y).
		\end{equation}
		Hence, $t(x,r) + (1-t)(y,s)\in\operatorname{epi}(F)$. A similar reasoning proves the opposite implication.
	\end{proof}	

	\prop[convexity-diff]{Differential characterisations of convexity}{
		Let $F:\bbR^d \to \bbR$ be an everywhere differentiable function. Then,
		\begin{itemize}
			\item $F$ is convex if and only if 
			\begin{equation}
				F(y) \ge F(x) + \langle \nabla F(x), y-x \rangle, \qquad \forall x,y\in \bbR^d.
			\end{equation}
			\item $F$ is strongly convex with parameter $\gamma>0$ if and only if 
			\begin{equation}
				F(y) \ge F(x) + \langle \nabla F(x), y-x \rangle + \frac{\gamma}{2}\|x-y\|^2_2, \qquad \forall x,y\in \bbR^d.
			\end{equation}
			\item If $F$ is everywhere twice differentiable, then it is convex if and only if 
			\begin{equation}
				\operatorname{Hess} F(x) \ge 0 \qquad \forall x\in\bbR^d.
			\end{equation}
			Here, we denoted by $\operatorname{Hess}F(x)$ the Hessian of $F$ at $x$.
		\end{itemize}
	}

	\prop[convex-cont]{}{
		Let $F:K\to \bbR$ be convex. Then, $F$ is continuous on the interior of $K$.
	}

	\begin{proof}
		Let $x_0\in \operatorname{int}(K)$ and consider $r>0$ such that $B(x_0,r)\subset K$. Without loss of generality, we assume $x_0=0$ (otherwise, replace the function $F$ by its translation $G(x) = F(x)-F(x_0)$).

		Convexity will allow to bound the difference $F(y)-F(0)$ with the values of $F$ on the sphere $\partial B(0,r)$.
		However, without continuity, the function $F$ need not be bounded on the compact set $\partial B(0,r)$, and hence we need some additional care.

		Pick $d+1$ linearly independent points $v_0,\ldots v_{d+1}\in \partial B(0,r)$, and consider the corresponding symplex
		\begin{equation}
			\Delta = \operatorname{conv}\left(\{v_0,\ldots,v_{d+1}\}  \right) 
			= \left\{  \sum_{i=1}^{d+1} t_i v_i \mid t_i\ge 0,\, \sum_i t_i=1 \right\}\subset B(0,r).
		\end{equation}
		Then, letting $M = \max_{i\in \llbracket1,d+1 \rrbracket} F(v_i)$, the fact that $F$ is convex yields that for any $x=\sum_{i=1}^{d+1} t_i v_i\in \Delta$ it holds
		\begin{equation}
			\label{eq:bdd}
			F(x) \le \sum_{i=1}^{d+1} t_i F(v_i) \le M.
		\end{equation}
		In particular, we can fix a radius $r'<r$ such that  $B(0,r')\subset \Delta$ where $F$ is bounded.

		We now proceed to bound the difference $F(x)-F(0)$.
		Let $x\in U\subset B(0,r')$ and set $t = \|x\|/r'$. In particular, $t\in [0,1]$ and the ray $\{sx\mid s\ge 0$ meets the sphere $\partial B(x_0,r')$ at the point 
		\begin{equation}
			y= \frac{r'}{\|x\|}(x).
		\end{equation}
		In particular, $x=(1-t)0 + ty$. By convexity and \eqref{eq:bdd}, we have 
		\begin{equation}
			\label{eq:upper}
			F(x) \le (1-t) F(0) + t F(y) \le. (1-t)F(0)+ t M 
			\implies 
			F(x)-F(0) \le t(M-F(0)).
		\end{equation}

		To derive a bound from below, we proceed similarly, considering 
		\begin{equation}
			z = \frac{r'}{\|x\|-r'}x.
		\end{equation}
		Indeed, we then have $0 = (1-t)x + t x$, where $t=\|x\|/r'$ as above. Then, convexity and the fact that $z\in B(0,r')$ yield
		\begin{equation}
			\label{eq:lower}
			F(0)\le (1-t)F(x) +tM \implies F(x)-F(0)\ge -\frac{t}{1-t}(M-F(0)).
		\end{equation}

		Combining \eqref{eq:upper} and \eqref{eq:lower}, we obtain 
		\begin{equation}
			- \frac{t}{1-t}(M-F(0)) \le F(x)-F(0) \le t(M-F(0)), \qquad t=\frac{\|x\|}{r'}
		\end{equation}
		Since $x$ was arbitrary in $B(0,r')$ we can take the limit as $x\to 0$, which implies $t\to 0$ and thus that 
		\begin{equation}
			\lim_{x\to 0}|F(x)-F(0)|=0,
		\end{equation}
		concluding the proof.
	\end{proof}

	The following result is at the core of the relation between optimisation and convexity.

	\thm{}{
		Let $F:\bbR^d\to \bbR\cup\{+\infty\}$ be a convex extended function. Then,
		\begin{itemize}
			\item Any local minimum of $F$ is global.
			\item The set of minima of $F$ is convex.
			\item If $F$ is strictly convex and admits a minimum, this minimum is unique.
			\item If $F$ is real-valued and strongly convex, then it has a unique minimum.
		\end{itemize}
	}

	\begin{proof}
		Assume that $x^\star$ is a local minimum, i.e., there exists $r>0$ such that $F(x^\star)\le F(x)$ for any $x\in B(0,r)$. 
		Let $y\in \bbR^d$ and consider a point on the ray starting at $x^\star$ and passing through $y$:
		\begin{equation}
		z = x^\star + s (y-x^\star) =   (1-s)x^\star + s y\qquad s\ge.
		\end{equation}
		Taking $s < \min\{1, r'/\|x^\star - y\|\}$ we have that $z\in B(0,r)$. Hence, by local minimality of $x^\star$ and convexity of $F$ we have
		\begin{equation}
			F(x^\star) \le F(z) \le (1-s)F(x^\star) + s F(y) \implies F(x^\star) \le F(y).
		\end{equation}
		This concludes the proof of the first point.

		Assume now that $x_1,x_2$ are minima for $F$. This clearly implies that $F(x_1)=F(x_2) =: m$, and thus, by convexity of $F$, for any $t\in [0,1]$ we have
		\begin{equation}
			m \le F(t x_1 + (1-t)x_2) \le t F(x_1) +(1-t) F(x_2) = m \implies F(t x_1 + (1-t)x_2).
		\end{equation}
		This implies that $t x_1 + (1-t)x_2$ is a minimum for any $t\in [0,1]$, thus proving the second point.
		
		The same argument as above in the case of a strictly convex function yields to
		\begin{equation}
			m \le F(tx_1+(1-t)x_2) < m \qquad \text{ if } x_1\neq x_2.
		\end{equation}
		This implies immediately that the minimum is unique.
	
		Assume, finally, that $F$ is strongly convex. Since it is strictly convex, we just need to prove the existence of a minimum. 
		By Proposition~\ref{prop:convex-cont} we have that $F$ is continuos, and thus it suffices to prove its coercivity: $F(x)\to +\infty$ if $\|x\|\to +\infty$. 
		We provide a proof of this fact in the case where $F$ is differentiable. In this case, by Proposition~\ref{prop:convexity-diff} we have that 
		\begin{equation}
			F(y) \ge F(0)+\langle \nabla F(0),y\rangle +\frac\gamma2\|y\|_2^2 \qquad \forall y\in \bbR^d.
		\end{equation}
		Since $\langle \nabla F(0),y\rangle \le \|y\|_2$, the quadratic term on the right-hand side of the above equation, implies that the limit as $\|y\|_2\to +\infty$ is $+\infty$.
	\end{proof}

	\begin{remark}
		Strict convexity is not enough to ensure the existence of a minimum. Consider, for example, $F(x) = e^x$.
	\end{remark}

\section{Convex conjugate and sub-differential}

\dfn{}{
	The convex conjugate (of Fenchel dual)  of an extended function $F:\bbR^d\to \bbR\cup\{+\infty\}$ is the function $F^*:\bbR^d\to \bbR\cup\{+\infty\}$ defined by
	\begin{equation}
		F^*(y) = \sup_{x\in\bbR^d}\left[\langle x,y\rangle-F(x)\right].
	\end{equation}
}

	Recall the following.
	
	\dfn{}{A function $F:\bbR^d\to \bbR$ is \emph{lower semicontinuous} (l.s.c.) if 
	\begin{equation}
		\liminf_{x\to x_0} F(x) \ge F(x_0), \qquad \forall x_0\in\bbR^d.
	\end{equation}
	Equivalently, $F$ is l.s.c.~if its epigraph is closed.
	}
	
	\ex{}{
		\begin{itemize}
			\item Every continuous function is lower semicontinuous.
			\item For any set $\Omega\subset \bbR^d$, the $0-\infty$ characteristic function 
			\begin{equation}
				\label{eq:0-infty-char}
				\chi_K=\begin{cases}
					0 & \text{if } x\in\Omega,\\
					+\infty & \text{otherwise},
				\end{cases}
			\end{equation}
			is lower semicontinuous, but not continuous.
		\end{itemize}
	}


\prop[convex-conj-prop]{}{
	Let $F:\bbR^d\to \bbR\cup\{+\infty\}$. Then,
	\begin{enumerate}
		\item The convex conjugate $F^*$ is a lower semicontinuous convex function.
		\item We have the Fenchel (or Young, or Fenchel-Young) inequality
		\begin{equation}
			\langle x,y\rangle \le F(x)+F^*(y)
		\end{equation}
	\end{enumerate}
}

\begin{proof}
	For any $y_1,y_2\in\bbR^d$ and $t\in[0,1]$ we have
	\begin{equation}
		\langle x, ty_1+(1-t)y_2 \rangle-F(x) = 
		t \left(\langle x, y_1\rangle-F(x)\right) + (1-t) \left(\langle x, y_2\rangle-F(x)\right).
	\end{equation}
	Taking the supremum for $x\in\bbR^d$ of the above, and recalling that $\sup(g(x)+h(x))\le \sup g(x)+\sup h(x)$ proves convexity of $F^*$. 

	Lower semicontinuity of $F^*$ follows since it is the supremum for $x\in\bbR^d$ of $g_x(y):=\langle x,y\rangle -F(x)$, which is affine and in particular lower semicontinuous. Indeed, the supremum of a family of l.s.c.~functions is l.s.c..

	The second point (Fenchel inequality) is a direct consequence of the definition of $F^*$.
\end{proof}

\thm{Fenchel-Moreau Theorem}{
	The biconjugate $F^**$ is the largest convex lower semicontinuous function satisfying $F^**(x)\le F(x)$ for any $x\in \bbR^d$. In particular, $F^**=F$ if $F$ is convex and proper.
}

\begin{proof}
	% We start by proving that for any extended function $F$ it holds
	% \begin{equation}
	% 	F^**(x)\le F(x), \qquad \forall x\in\bbR^d.
	% \end{equation}	
	% Indeed, w
	We have that $-F^*(y)=\inf_{x\in\bbR^d} \left(F(x)-\langle x,y\rangle\right)$, which implies that for any $y,z\in\bbR^d$ it holds 
	\begin{equation}
		\langle z,y\rangle - F^*(y) \le \langle z-x,y\rangle +F(x), \qquad \forall x\in\bbR^d.
	\end{equation}
	In particular, considering $z=x$ we have 
	\begin{equation}
		F^**(x) = \sup_{y\in\bbR^d} \left(\langle x,y\rangle - F^*(y)\right) \le F(x),
	\end{equation}
	proving the first part of the statement.
	
	Since $F^{**}$ is convex and l.s.c.~by Proposition~\ref{prop:convex-conj-prop}, in order to complete the proof it suffices to show that if $F$ is convex, then 
	\begin{equation}
		F^**(x)\ge F(x),\qquad \forall x\in\bbR^d.
	\end{equation}
	....
\end{proof}


\ex{}{
	\begin{itemize}
		\item Let $F(x)=\frac12 \|x\|_2^2$. Then, $F^*(y)=\frac12\|y\|_2^2=F(y)$. This is the only function with this property.
		\item Let $F=\chi_K$ be the $0-\infty$ characteristic function of a convex set $K\subset\bbR^d$ defined in \eqref{eq:0-infty-char}. Then, 
		\begin{equation}
			F^*(y)=\sup_{x\in K} \langle x,y\rangle.
		\end{equation}
	\end{itemize}
}

\dfn{}{
	The \emph{subdifferential} of a convex extended function $F:\bbR^d\to \bbR\cup\{+\infty\}$ at $x\in\bbR^d$ is the set 
	\begin{equation}
		\partial F(x) = \left\{ v\in\bbR^d \mid F(y) \ge F(x)+\langle v,y-x \rangle, \qquad \forall y\in\bbR^d \right\}.
	\end{equation}
	A vector $v\in \partial F(x)$ is called a \emph{subgradient} for $F$ at $x$.
}

We have the following.

\prop{}{
	Let $F:\bbR^d\to \bbR\cup \{+\infty\}$ be a convex function. Then,
	\begin{itemize}
		\item For any $x\in \bbR^d$ the subdifferential $\partial F(x)$ is non-empty.
		\item If $F$ is differentiable at $x\in\bbR^d$, then $\partial F(x)=\{\nabla F(x)\}$.
	\end{itemize}
}

\begin{proof}
	The first part of the theorem is a consequence of the Supporting Hyperplane Theorem (see Corollary~\ref{th:supporting-hyperplane}) and Proposition~\ref{prop:epigraph}. Indeed, the latter implies that the epigraph $\operatorname{epi}(F)$ is convex and hence, by the former, any of its boundary point admits a supporting hyperplane. Using the fact that $\partial\operatorname{epi}(F)=\{(x,F(x))\mid x\in\bbR^d\}$ allows to conclude.

	Concerning the proof of the second statement, the fact that $\nabla F(x)\in \partial F(x)$ follows from the characterisation of convexity for differentiable functions given in Proposition~\ref{prop:convexity-diff}.
	To prove the opposite implication, let $v\in \partial F(x)$ and observe that by definition of subgradient the directional derivative $\partial_h F(x)$ of $f$ in the direction $h\in\bbR^d$ at $x$ satisfies
	\begin{equation}
		\partial_h F(x) = \lim_{t\to 0} \frac{F(x+th)-F(x)}{t} \ge \langle v, h\rangle.
	\end{equation}
	Since we know that $\partial_hF (x)=\langle \nabla F(x),h\rangle$, we have that 
	\begin{equation}
		\langle \nabla F(x) - v,h\rangle \ge 0, \qquad \forall h\in\bbR^d.
	\end{equation}
	But this implies that $\nabla F(x)=v$, concluding the proof.
\end{proof}

\begin{figure}
	\centering
	\includegraphics[width=.4\textwidth]{images/abs-subgrad.png}
	\caption{Visualization of the subgradients of $F(x)=|x|$ at $x=0$. Image from \href{https://tlienart.github.io/posts/2018/09/23-convex-optimisation-1/}{this website}.}
	\label{fig:subgrad-abs}
\end{figure}

\ex{}{
	Consider $F(x)=|x|$. Then,
	\begin{equation}
		\partial F(x) =
		\begin{cases}
			\{ \operatorname{sgn}(x) \} & \text{if }x\neq 0,\\
			[-1,1] & \text{if } x=0.
		\end{cases}
	\end{equation}
	Here, $\operatorname{sgn}(x) = x/|x|$ is the sign function.
	See Figure~\ref{fig:subgrad-abs}.
}

\thm{}{
	Let $F:\bbR^d\to \bbR\cup\{+\infty\}$ be a convex function. Then, $x\in\bbR^d$ is a minimum for $F$ if and only if $0\in \partial F(x)$
}

\begin{proof}
	The fact that $x$ is a minimum means that $F(x)\le F(y)$ for any $y\in\bbR^d$, which is the definition of $0\in\partial F(x)$.
\end{proof}

\prop{}{
	Let $F:\bbR^d\to \bbR\cup\{+\infty\}$ be a convex function and $x,y\in \bbR^d$. Then, the following are equivalent:
	\begin{enumerate}
		\item[i.] $y\in \partial F(x)$.
		\item[ii.] $F(x)+F^*(y) = \langle  x,y\rangle$.
	\end{enumerate}
	If, additionally, $F$ is l.s.c.~, then the above are also equivalent to 
	\begin{enumerate}
		\item[iii.] $x\in \partial F^*(y)$.
	\end{enumerate}
}

	% \dfn{Definition Topic}{Definition Statement}
	% \thm{Theorem Name}{Theorem Statement}
	% \cor[cori]{Corollary Name}{Corollary Statement}
	% \lem{Lemma Name}{Lemma Statement}
	% \clm{Claim Name}{Claim Statement}
	% \ex{Example Name}{Example explained}
% \opn{Open Question Name}{Question Statement}
% \qs{Question Name}{Question Statement}
% \nt{Special Note}
% \wc{Wrong Concept topic}{Explanation}
% \pf{Proof}{Proof}
	
\chapter{Second Chapter}
\section{Section 1}

\printbibliography
\end{document}