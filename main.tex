\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{biblio.bib}

\DeclareMathOperator{\feas}{\Phi}

\begin{document}
	
	\thispagestyle{empty}
	\mytitlec{Optimisation, Control,}{and Data}{Dario Prandi}{dario.prandi@centralesupelec.fr}{2025}
	\newpage% or \cleardoublepage
	\tableofcontents
	
	\part{Optimisation}
	\chapter{Convex Analysis}

	This chapter closely follows chapter 5 of the lecture notes \cite{fornasierFoundations}.
	For most of the proof in this chapter we refer to \cite{boydConvex2023} or \cite{rockafellarConvex2015}.

	\section{Convex sets}

	\dfn[convex-set]{}{
	 A set $K\subset \bbR^d$ is \emph{convex} if 
				\begin{equation}
					t x +(1-t)y \in K \qquad \forall x,y \in K, \, t\in [0,1]
				\end{equation}
	}

	We have the following fact.

	\prop[convex-comb]{}{
		The set $K\subset\bbR^d$ is convex if and only if for any $n\in\bbN$, and $t_1,\ldots, t_n\ge 0$ such that $\sum_{i=1}^n t_i=1$, it holds 
		\begin{equation}
			\label{eq:convex-comb}
			x_1,\ldots,x_n\in K \implies \sum_{i=1}^n t_ix_i \in K.
		\end{equation}
	}

	 \begin{proof}
		Property \eqref{eq:convex-comb} with $n=2$ is exactly the definition of $K$ is convex. The statement then follows by induction on $n$. 
	 \end{proof}

	\dfn{}{
		The convex hull $\operatorname{conv}(\Omega)$ of $\Omega\subset \bbR^d$ is the smallest convex set $K$ containing $\Omega$.
	}

	By Proposition~\ref{prop:convex-comb}, it is immediate to observe that 
	\begin{equation}
		\operatorname{conv}(\Omega) = \left\{ \sum_{i=1}^n t_i x_i \mid t_i\ge 0, \sum_{i=1}^n t_i = 1,\, x_i \in \Omega \right\}.
	\end{equation}

	\ex{Convex sets}{
		\begin{itemize}
			\item Unit ball w.r.t.~any norm.
			\item Vector subspaces.
			\item Hyperplanes, i.e., for any $v\in \bbR^d$ and $\lambda\in \bbR$,
			\begin{equation}
				\label{eq:hyperplane}
				H_{v,\lambda} := \{x\in \bbR^d \mid \langle v, x \rangle \ge \lambda\}.
			\end{equation}
		\end{itemize}
	}

	An important result (that we will not prove) on convex sets is the following.

	\thm{Separation theorem}{
		Let $K_1,K_2\subset \bbR^d$ be two convex sets such with disjoint interior. Then there exists $v\in \bbR^d$ and $\lambda \in \bbR$ such that 
		\begin{equation}
			K_1\subset H_{v,\lambda}, \qquad K_2 \subset \bbR^d \setminus H_{v,\lambda}. 
		\end{equation}
		Here, $H_{v,\lambda}$ is defined in \eqref{eq:hyperplane}.
	}

	As a direct consequence, we have the following (see Figure~\ref{fig:supporting-hyperplane}).

	\begin{figure}
		\centering
		\includegraphics[width=.3\textwidth]{images/supporting-hyperplane.png}
		\caption{Supporting hyperplane for a set $S$.}
		\label{fig:supporting-hyperplane}
	\end{figure}

	\cor[supporting-hyperplane]{Supporting hyperplane theorem}{
		Let $K\subset \bbR^d$ be a convex set and $x\in\partial K$. Then, there exists a supporting hyperplane of $K$ containing $x_0$. 
		That is, there exists $v\in\bbR^d$ and $\lambda\in\bbR$ such that $K\subset H_{v,\lambda}$ and $x_0\in \partial H_{v,\lambda}$.
	}

	When $K$ is a convex polygon, it is natural to expect it to be determined by its vertices. In order to formalize this intuition we need the following.

	\dfn{}{
		Let $K\subset\bbR^d$ be a convex set. A point $x\in K$ is an extremum of $K$ if for any $y,z\in K$ and $t\in (0,1)$ we have that
		\begin{equation}
			x = t y +(1-t) z \implies x=y=z.
		\end{equation}
		The set of exterma of $K$ is denoted by $\operatorname{extr}(K)$.
	}

	In particular, for a convex polygon $\operatorname{extr}(K)$ is the set of its vertices.

	\prop{}{
		Let $K\subset\bbR^d$ be a convex set that is compact. Then,
		\begin{equation}
			\operatorname{conv}(K)= \operatorname{conv}\left(\operatorname{extr}(K)\right).
		\end{equation}
	}

	\section{Cones}

	\dfn{}{
		A set $K\subset \bbR^d$ is a cone if 
		\begin{equation}
			tx \in K \qquad \forall x\in K, \, t\ge 0.
		\end{equation}
	}

	Observe that every cone contains the origin.

	\ex{Cones}{
		\begin{itemize}
			\item The second order cone $$C = \{ x=(x',x_n)\in \bbR^{d}\times \bbR \mid \|x'\|_2 \le x_n\}.$$
			\item Positive orthant $\bbR_+^d = \{ x\in\bbR^d \mid x_i\ge 0, \quad \forall i\in \llbracket 1,d\rrbracket \}$.
			\item The set of positive semidefinite matrices $\operatorname{Sym}_+(\bbR^d)$.
		\end{itemize}
	}

	\dfn{}{
		The conic hull $\operatorname{cone}(\Omega)$ of a set $\Omega\subset \bbR^d$ is the smallest cone containing $\Omega$. Namely,
		\begin{equation}
			\operatorname{cone}(\Omega) = \left\{ \sum_{i=1}^n t_i x_i \mid t_i\ge 0\text{ and } x_i\in \Omega  \text{ for any } i \in \llbracket 1,n\rrbracket   \right\}.
		\end{equation}
	}

	\dfn{}{
		The polar cone $K^*$ of a cone $K\subset\bbR^d$ is the set 
		\begin{equation}
			K^* := \left\{ y\in\bbR^d \mid \rangle x,y \langle \ge 0 \quad \forall x\in K \right\}.
		\end{equation}
	}

	\begin{figure}[t]
		\centering
		\includegraphics[width=.3\textwidth]{images/polar-cone-1}
		\hspace{.1\textwidth}
		\includegraphics[width=.3\textwidth]{images/polar-cone-2}
		\caption{Two examples of polar cone.}
	\end{figure}

	We 	have the following properties for the polar cone.

	\prop{}{
		The polar cone $K^*$ is a closed, convex cone. If, moreover, the cone $K$ is closed, then $K^{**}=K$.
	}

	\section{Convex functions}

	We will work with extended functions $F:\bbR^d\to \bbR\cup\{+\infty\}$. The domain of an extended function is 
	\begin{equation}
		\operatorname{dom}(F) = \{x\in \bbR^d\mid F(x)<+\infty\}.
	\end{equation}
	An extended function such that $\operatorname{dom}(F)\neq \varnothing$ is called \emph{proper}.
	
	Given a standard function $F:\Omega\to \bbR$, we can identify it with the extended function $\bar F:\bbR^d\to \bbR\cup\{+\infty\}$ defined by
	\begin{equation}
		\bar F(x) = 
		\begin{cases}
			F(x) & \text{if }x\in \Omega,\\
			+\infty & \text{if }x\in \bbR^d\setminus \Omega.
		\end{cases}
	\end{equation}

	\dfn{Convex functions}{
		Let $F:\bbR^d\to\bbR\cup\{+\infty\}$ be an extended function. Then, 
		\begin{itemize}
			\item $F$ is convex if 
			\begin{equation}
				F(tx + (1-t)y) \le t F(x) + (1-t)F(y) \qquad \forall x,y\in \bbR^d,\, t\in [0,1].
			\end{equation}
			\item $F$ is strictly convex if 
			\begin{equation}
				F(tx + (1-t)y) < t F(x) + (1-t)F(y) \qquad \forall x,y\in \bbR^d,\, x\neq y, \, t\in [0,1].
			\end{equation}
			\item $F$ is strongly convex if there exists $\gamma>0$ such that
			\begin{equation}
				F(tx + (1-t)y) \le t F(x) + (1-t)F(y) - \frac{\gamma}{2}t(t-1)\|x-y\|_2^2 \qquad \forall x,y\in \bbR^d,\, t\in [0,1].
			\end{equation}
		\end{itemize}

		We say that $F$ is \emph{concave} if $-F$ is convex.
	}

	Observe that it holds 
	\begin{equation}
		\text{convex} \impliedby \text{strongly convex} \impliedby \text{strictly convex}  
	\end{equation}

	We say that a standard function $F:K\to \bbR$ is convex, strictly convex, strongly convex, or concave, if the same is true for its extension $\bar F$. Observe that this requires $K$ to be convex.

	\ex{}{
		\begin{itemize}
			\item 	The prototypical convex function, used in the definition of strongly convex, is the quadratic function 
		\begin{equation}
			F(x) = \frac{\|x\|_2^2}{2} = \frac{1}{2}\sum_{i=1}^d |x_i|^2.
		\end{equation}
			\item More generally, every norm is convex.
			\item The norm $\ell_p$ is strictly convex if and only if $p\in (1,+\infty)$.
			\item $F(x) = x^\top A x$ is convex if $A$ is positive semidefinite (i.e.,  $A\in \operatorname{Sym}_{\ge 0}(\bbR^d)$), and strongly convex if $A$ is positive definite.
		\end{itemize}
	}

	\prop[epigraph]{}{
		A function $F:K\to\bbR$ is convex if and only if its epigraph $\operatorname{epi}(F)\subset \bbR^{d+1}$ is convex. Here, we let 
		\begin{equation}
			\operatorname{epi}(F) = \{ (x,r) \mid r\ge F(x) \}.
		\end{equation}
	}

	\begin{figure}
		\begin{minipage}{.48\textwidth}
			\centering
			\includegraphics[width=.6\textwidth]{images/epigraph.png}
			\caption{Epigraph of a function.}
		\end{minipage}
		\begin{minipage}{.48\textwidth}
			\centering
			\includegraphics[width=.6\textwidth]{images/convexity-diff.png}
			\caption{Graphical representation of Proposition~\ref{prop:convexity-diff}.}
		\end{minipage}
	\end{figure}

	\begin{proof}
		Assume $F$ is convex and let $(x,r),(y,s)\in\operatorname{epi}(F)$. 
		In particular, $r\ge F(x)$ and $s\ge F(y)$.
		Let $t\in [0,1]$ and observe that
		\begin{equation}
			t r +(1-t)s \ge t F(x)+(1-t)F(y) \ge  F(tx+(1-t)y).
		\end{equation}
		Hence, $t(x,r) + (1-t)(y,s)\in\operatorname{epi}(F)$. A similar reasoning proves the opposite implication.
	\end{proof}	

	\prop[convexity-diff]{Differential characterisations of convexity}{
		Let $F:\bbR^d \to \bbR$ be an everywhere differentiable function. Then,
		\begin{itemize}
			\item $F$ is convex if and only if 
			\begin{equation}
				F(y) \ge F(x) + \langle \nabla F(x), y-x \rangle, \qquad \forall x,y\in \bbR^d.
			\end{equation}
			\item $F$ is strongly convex with parameter $\gamma>0$ if and only if 
			\begin{equation}
				F(y) \ge F(x) + \langle \nabla F(x), y-x \rangle + \frac{\gamma}{2}\|x-y\|^2_2, \qquad \forall x,y\in \bbR^d.
			\end{equation}
			\item If $F$ is everywhere twice differentiable, then it is convex if and only if 
			\begin{equation}
				\operatorname{Hess} F(x) \ge 0 \qquad \forall x\in\bbR^d.
			\end{equation}
			Here, we denoted by $\operatorname{Hess}F(x)$ the Hessian of $F$ at $x$.
		\end{itemize}
	}

	\prop[convex-cont]{}{
		Let $F:K\to \bbR$ be convex. Then, $F$ is continuous on the interior of $K$.
	}

	\begin{proof}
		Let $x_0\in \operatorname{int}(K)$ and consider $r>0$ such that $B(x_0,r)\subset K$. Without loss of generality, we assume $x_0=0$ (otherwise, replace the function $F$ by its translation $G(x) = F(x)-F(x_0)$).

		Convexity will allow to bound the difference $F(y)-F(0)$ with the values of $F$ on the sphere $\partial B(0,r)$.
		However, without continuity, the function $F$ need not be bounded on the compact set $\partial B(0,r)$, and hence we need some additional care.

		Pick $d+1$ linearly independent points $v_0,\ldots v_{d+1}\in \partial B(0,r)$, and consider the corresponding symplex
		\begin{equation}
			\Delta = \operatorname{conv}\left(\{v_0,\ldots,v_{d+1}\}  \right) 
			= \left\{  \sum_{i=1}^{d+1} t_i v_i \mid t_i\ge 0,\, \sum_i t_i=1 \right\}\subset B(0,r).
		\end{equation}
		Then, letting $M = \max_{i\in \llbracket1,d+1 \rrbracket} F(v_i)$, the fact that $F$ is convex yields that for any $x=\sum_{i=1}^{d+1} t_i v_i\in \Delta$ it holds
		\begin{equation}
			\label{eq:bdd}
			F(x) \le \sum_{i=1}^{d+1} t_i F(v_i) \le M.
		\end{equation}
		In particular, we can fix a radius $r'<r$ such that  $B(0,r')\subset \Delta$ where $F$ is bounded.

		We now proceed to bound the difference $F(x)-F(0)$.
		Let $x\in U\subset B(0,r')$ and set $t = \|x\|/r'$. In particular, $t\in [0,1]$ and the ray $\{sx\mid s\ge 0$ meets the sphere $\partial B(x_0,r')$ at the point 
		\begin{equation}
			y= \frac{r'}{\|x\|}(x).
		\end{equation}
		In particular, $x=(1-t)0 + ty$. By convexity and \eqref{eq:bdd}, we have 
		\begin{equation}
			\label{eq:upper}
			F(x) \le (1-t) F(0) + t F(y) \le. (1-t)F(0)+ t M 
			\implies 
			F(x)-F(0) \le t(M-F(0)).
		\end{equation}

		To derive a bound from below, we proceed similarly, considering 
		\begin{equation}
			z = \frac{r'}{\|x\|-r'}x.
		\end{equation}
		Indeed, we then have $0 = (1-t)x + t x$, where $t=\|x\|/r'$ as above. Then, convexity and the fact that $z\in B(0,r')$ yield
		\begin{equation}
			\label{eq:lower}
			F(0)\le (1-t)F(x) +tM \implies F(x)-F(0)\ge -\frac{t}{1-t}(M-F(0)).
		\end{equation}

		Combining \eqref{eq:upper} and \eqref{eq:lower}, we obtain 
		\begin{equation}
			- \frac{t}{1-t}(M-F(0)) \le F(x)-F(0) \le t(M-F(0)), \qquad t=\frac{\|x\|}{r'}
		\end{equation}
		Since $x$ was arbitrary in $B(0,r')$ we can take the limit as $x\to 0$, which implies $t\to 0$ and thus that 
		\begin{equation}
			\lim_{x\to 0}|F(x)-F(0)|=0,
		\end{equation}
		concluding the proof.
	\end{proof}

	The following result is at the core of the relation between optimisation and convexity.

	\thm{}{
		Let $F:\bbR^d\to \bbR\cup\{+\infty\}$ be a convex extended function. Then,
		\begin{itemize}
			\item Any local minimum of $F$ is global.
			\item The set of minima of $F$ is convex.
			\item If $F$ is strictly convex and admits a minimum, this minimum is unique.
			\item If $F$ is real-valued and strongly convex, then it has a unique minimum.
		\end{itemize}
	}

	\begin{proof}
		Assume that $x^\star$ is a local minimum, i.e., there exists $r>0$ such that $F(x^\star)\le F(x)$ for any $x\in B(0,r)$. 
		Let $y\in \bbR^d$ and consider a point on the ray starting at $x^\star$ and passing through $y$:
		\begin{equation}
		z = x^\star + s (y-x^\star) =   (1-s)x^\star + s y\qquad s\ge.
		\end{equation}
		Taking $s < \min\{1, r'/\|x^\star - y\|\}$ we have that $z\in B(0,r)$. Hence, by local minimality of $x^\star$ and convexity of $F$ we have
		\begin{equation}
			F(x^\star) \le F(z) \le (1-s)F(x^\star) + s F(y) \implies F(x^\star) \le F(y).
		\end{equation}
		This concludes the proof of the first point.

		Assume now that $x_1,x_2$ are minima for $F$. This clearly implies that $F(x_1)=F(x_2) =: m$, and thus, by convexity of $F$, for any $t\in [0,1]$ we have
		\begin{equation}
			m \le F(t x_1 + (1-t)x_2) \le t F(x_1) +(1-t) F(x_2) = m \implies F(t x_1 + (1-t)x_2).
		\end{equation}
		This implies that $t x_1 + (1-t)x_2$ is a minimum for any $t\in [0,1]$, thus proving the second point.
		
		The same argument as above in the case of a strictly convex function yields to
		\begin{equation}
			m \le F(tx_1+(1-t)x_2) < m \qquad \text{ if } x_1\neq x_2.
		\end{equation}
		This implies immediately that the minimum is unique.
	
		Assume, finally, that $F$ is strongly convex. Since it is strictly convex, we just need to prove the existence of a minimum. 
		By Proposition~\ref{prop:convex-cont} we have that $F$ is continuos, and thus it suffices to prove its coercivity: $F(x)\to +\infty$ if $\|x\|\to +\infty$. 
		We provide a proof of this fact in the case where $F$ is differentiable (the general case can be obtained similarly using Proposition~\ref{prop:subdifferential}, proven later on). In this case, by Proposition~\ref{prop:convexity-diff} we have that 
		\begin{equation}
			F(y) \ge F(0)+\langle \nabla F(0),y\rangle +\frac\gamma2\|y\|_2^2 \qquad \forall y\in \bbR^d.
		\end{equation}
		Since $\langle \nabla F(0),y\rangle \le \|y\|_2$, the quadratic term on the right-hand side of the above equation, implies that the limit as $\|y\|_2\to +\infty$ is $+\infty$.
	\end{proof}

	\begin{remark}
		Strict convexity is not enough to ensure the existence of a minimum. Consider, for example, $F(x) = e^x$.
	\end{remark}

\section{Convex conjugate and sub-differential}

\dfn{}{
	The convex conjugate (of Fenchel dual)  of an extended function $F:\bbR^d\to \bbR\cup\{+\infty\}$ is the function $F^*:\bbR^d\to \bbR\cup\{+\infty\}$ defined by
	\begin{equation}
		F^*(y) = \sup_{x\in\bbR^d}\left[\langle x,y\rangle-F(x)\right].
	\end{equation}
}

	Recall the following.
	
	\dfn{}{A function $F:\bbR^d\to \bbR$ is \emph{lower semicontinuous} (l.s.c.) if 
	\begin{equation}
		\liminf_{x\to x_0} F(x) \ge F(x_0), \qquad \forall x_0\in\bbR^d.
	\end{equation}
	Equivalently, $F$ is l.s.c.~if its epigraph is closed.
	}
	
	\ex{}{
		\begin{itemize}
			\item Every continuous function is lower semicontinuous.
			\item For any set $\Omega\subset \bbR^d$, the $0-\infty$ characteristic function 
			\begin{equation}
				\label{eq:0-infty-char}
				\chi_K=\begin{cases}
					0 & \text{if } x\in\Omega,\\
					+\infty & \text{otherwise},
				\end{cases}
			\end{equation}
			is lower semicontinuous, but not continuous.
		\end{itemize}
	}


\prop[convex-conj-prop]{}{
	Let $F:\bbR^d\to \bbR\cup\{+\infty\}$. Then,
	\begin{enumerate}
		\item The convex conjugate $F^*$ is a lower semicontinuous convex function.
		\item We have the Fenchel (or Young, or Fenchel-Young) inequality
		\begin{equation}
			\langle x,y\rangle \le F(x)+F^*(y)
		\end{equation}
	\end{enumerate}
}

\begin{proof}
	For any $y_1,y_2\in\bbR^d$ and $t\in[0,1]$ we have
	\begin{equation}
		\langle x, ty_1+(1-t)y_2 \rangle-F(x) = 
		t \left(\langle x, y_1\rangle-F(x)\right) + (1-t) \left(\langle x, y_2\rangle-F(x)\right).
	\end{equation}
	Taking the supremum for $x\in\bbR^d$ of the above, and recalling that $\sup(g(x)+h(x))\le \sup g(x)+\sup h(x)$ proves convexity of $F^*$. 

	Lower semicontinuity of $F^*$ follows since it is the supremum for $x\in\bbR^d$ of $g_x(y):=\langle x,y\rangle -F(x)$, which is affine and in particular lower semicontinuous. Indeed, the supremum of a family of l.s.c.~functions is l.s.c..

	The second point (Fenchel inequality) is a direct consequence of the definition of $F^*$.
\end{proof}




\ex{}{
	\begin{itemize}
		\item Let $F(x)=\frac12 \|x\|_2^2$. Then, $F^*(y)=\frac12\|y\|_2^2=F(y)$. This is the only function with this property.
		\item Let $F=\chi_K$ be the $0-\infty$ characteristic function of a convex set $K\subset\bbR^d$ defined in \eqref{eq:0-infty-char}. Then, 
		\begin{equation}
			F^*(y)=\sup_{x\in K} \langle x,y\rangle.
		\end{equation}
	\end{itemize}
}

\dfn{}{
	The \emph{subdifferential} of a convex extended function $F:\bbR^d\to \bbR\cup\{+\infty\}$ at $x\in\bbR^d$ is the set 
	\begin{equation}
		\partial F(x) = \left\{ v\in\bbR^d \mid F(y) \ge F(x)+\langle v,y-x \rangle, \qquad \forall y\in\bbR^d \right\}.
	\end{equation}
	A vector $v\in \partial F(x)$ is called a \emph{subgradient} for $F$ at $x$.
}

\begin{figure}
	\centering
	\includegraphics[width=.4\textwidth]{images/abs-subgrad.png}
	\caption{Visualization of the subgradients of $F(x)=|x|$ at $x=0$. Image from \href{https://tlienart.github.io/posts/2018/09/23-convex-optimisation-1/}{this website}.}
	\label{fig:subgrad-abs}
\end{figure}

\ex{}{
	Consider $F(x)=|x|$. Then,
	\begin{equation}
		\partial F(x) =
		\begin{cases}
			\{ \operatorname{sgn}(x) \} & \text{if }x\neq 0,\\
			[-1,1] & \text{if } x=0.
		\end{cases}
	\end{equation}
	Here, $\operatorname{sgn}(x) = x/|x|$ is the sign function.
	See Figure~\ref{fig:subgrad-abs}.
}

\thm{}{
	Let $F:\bbR^d\to \bbR\cup\{+\infty\}$ be a convex function. Then, $x\in\bbR^d$ is a minimum for $F$ if and only if $0\in \partial F(x)$
}

\begin{proof}
	The fact that $x$ is a minimum means that $F(x)\le F(y)$ for any $y\in\bbR^d$, which is the definition of $0\in\partial F(x)$.
\end{proof}

We have the following.

\prop[subdifferential]{}{
	Let $F:\bbR^d\to \bbR\cup \{+\infty\}$ be a convex function. Then,
	\begin{itemize}
		\item For any $x\in \bbR^d$ the subdifferential $\partial F(x)$ is non-empty.
		\item It holds that 
		\begin{equation}
			\partial F(x) = \left\{ v\in \bbR^d\mid F^*(v) + F(x) = \langle x,v\rangle  \right\}.
		\end{equation}
		\item If $F$ is differentiable at $x\in\bbR^d$, then $\partial F(x)=\{\nabla F(x)\}$.
	\end{itemize}
}

\begin{proof}
	The first part of the theorem is a consequence of the Supporting Hyperplane Theorem (see Corollary~\ref{th:supporting-hyperplane}) and Proposition~\ref{prop:epigraph}. Indeed, the latter implies that the epigraph $\operatorname{epi}(F)$ is convex and hence, by the former, any of its boundary point admits a supporting hyperplane. Using the fact that $\partial\operatorname{epi}(F)=\{(x,F(x))\mid x\in\bbR^d\}$ allows to conclude.

	To prove the second statement, observe that $v\in\partial F(x)$ is equivalent to
	\begin{equation}
		\langle y,v\rangle - F(y) \le \langle x,v\rangle-F(x), \qquad \forall y\in\bbR^d.
	\end{equation}
	Taking the sup for $y\in\bbR^d$ yields that $F^*(v)\le \langle x,v\rangle -F(x)$. The opposite inequality follows from Fenchel inequality (see Proposition~\ref{prop:convex-conj-prop}).

	Concerning the proof of the last statement, the fact that $\nabla F(x)\in \partial F(x)$ follows from the characterisation of convexity for differentiable functions given in Proposition~\ref{prop:convexity-diff}.
	To prove the opposite implication, let $v\in \partial F(x)$ and observe that by definition of subgradient the directional derivative $\partial_h F(x)$ of $f$ in the direction $h\in\bbR^d$ at $x$ satisfies
	\begin{equation}
		\partial_h F(x) = \lim_{t\to 0} \frac{F(x+th)-F(x)}{t} \ge \langle v, h\rangle.
	\end{equation}
	Since we know that $\partial_hF (x)=\langle \nabla F(x),h\rangle$, we have that 
	\begin{equation}
		\langle \nabla F(x) - v,h\rangle \ge 0, \qquad \forall h\in\bbR^d.
	\end{equation}
	But this implies that $\nabla F(x)=v$, concluding the proof.
\end{proof}



Thanks to the previous result, we are in a position to prove the following property of the convex biconjugate.

\thm[fenchel-moreau]{Fenchel-Moreau Theorem}{
	The biconjugate $F^{**}$ is the largest convex lower semicontinuous function satisfying $F^{**}(x)\le F(x)$ for any $x\in \bbR^d$. In particular, $F^{**}=F$ if $F$ is convex and proper.
}

\begin{proof}
	% We start by proving that for any extended function $F$ it holds
	% \begin{equation}
	% 	F^{**}(x)\le F(x), \qquad \forall x\in\bbR^d.
	% \end{equation}	
	% Indeed, w
	We have that $-F^*(y)=\inf_{x\in\bbR^d} \left(F(x)-\langle x,y\rangle\right)$, which implies that for any $y,z\in\bbR^d$ it holds 
	\begin{equation}
		\langle z,y\rangle - F^*(y) \le \langle z-x,y\rangle +F(x), \qquad \forall x\in\bbR^d.
	\end{equation}
	In particular, considering $z=x$ we have 
	\begin{equation}
		F^{**}(x) = \sup_{y\in\bbR^d} \left(\langle x,y\rangle - F^*(y)\right) \le F(x),
	\end{equation}
	proving the first part of the statement.
	
	Since $F^{**}$ is convex and l.s.c.~by Proposition~\ref{prop:convex-conj-prop}, in order to complete the proof it suffices to show that if $F$ is convex, then 
	\begin{equation}
		F^{**}(x)\ge F(x),\qquad \forall x\in\bbR^d.
	\end{equation}
	Let $v\in \partial F(x)$, which exists thanks to Proposition~\ref{prop:subdifferential}. For such a $v$, using the characterisation of the subdifferential in Proposition~\ref{prop:subdifferential}, we have 
	\begin{equation}
		F^*(v)=\langle x,v\rangle - F(x), 
	\end{equation}
	so that $F^{**}(z)\ge \langle v,z-x\rangle+F(x)$ for any $z\in \bbR^d$. Picking $z=x$ allows to conclude.
\end{proof}



\prop{}{
	Let $F:\bbR^d\to \bbR\cup\{+\infty\}$ be a convex function and $x,y\in \bbR^d$. Then, the following are equivalent:
	\begin{enumerate}
		\item[i.] $y\in \partial F(x)$.
		\item[ii.] $F(x)+F^*(y) = \langle  x,y\rangle$.
	\end{enumerate}
	If, additionally, $F$ is l.s.c.~, then the above are also equivalent to 
	\begin{enumerate}
		\item[iii.] $x\in \partial F^*(y)$.
	\end{enumerate}
}

\begin{proof}
	To show that \emph{i} is equivalent to \emph{ii}, we just need to show that $y\in \partial F(x)$ is equivalent to 
	\begin{equation}
		\label{eq:opp-fenchel-ineq}
		F(x)+F^*(y) \le \langle  x,y\rangle.
	\end{equation}
	Indeed, the opposite inequality is always true due to Fenchel's inequality (see Proposition~\ref{prop:convex-conj-prop}).

	Observe that the fact that $y\in\partial F(x)$ means that
	\begin{equation}
		\langle x,y\rangle F(x)
		\ge \langle z,y\rangle F(z), \qquad \forall z\in \bbR^d.
	\end{equation}
	That is, the function $z\mapsto \langle z,y\rangle F(z)$ attains its maximum at $z=x$. But, by definition of $F^*$, this is equivalent to \eqref{eq:opp-fenchel-ineq}, thus proving that \emph{i} is equivalent to \emph{ii}.

	To complete the proof, observe that by Theorem~\ref{th:fenchel-moreau} the lower semicontinuity of $F$ yield that $F^{**}=F$, so that \emph{ii} is equivalent to $F^{**}(x)+F^*(y)=\langle x,y\rangle$. Using the fact that \emph{i}$\iff$\emph{ii} with $F$ replaced by $F^*$ completes the proof.
\end{proof}


\section{Convex optimization problems}

\dfn{}{
An optimization problem is a minimization problem of the form
\begin{equation}
	\tag{OP}
	\label{eq:op}
	\min_{x\in \bbR^d} F_0(x) 
	\qquad\text{subject to}\qquad
	Ax=y
	\qquad \text{and}\qquad 
	F_j(x)\le 0, \qquad j\in\llbracket 1, M\rrbracket.
\end{equation}
Here,
\begin{enumerate}
	\item $F_0:\bbR^d\to \bbR\cup\{+\infty\}$ is the \emph{objective function};
	\item $F_1,\ldots, F_M:\bbR^d\to \bbR\cup\{+\infty\}$ are the \emph{constraing functions};
	\item $A\in \bbR^{m\times n}$ and $y\in \bbR^m$ provide the \emph{equality constraints};
\end{enumerate}

The optimization problem is \emph{convex} (resp.~\emph{linear}) if $F_0,\ldots, F_M$ are convex (resp.~linear) functions.
}

\dfn{}{
	Consider an optimization problem \eqref{eq:op}. Then,
	\begin{itemize}
		\item The set $\feas\subset\bbR^d$ of points $x\in\bbR^d$ satisfying the constraints is the set of \emph{feasible points}. That is,
		\begin{equation}
			\feas = \left\{ x\in \bbR^d \mid Ax = y, \qquad F_j(x)\le 0 \qquad \forall j\in \llbracket 1,M\rrbracket \right\}.
		\end{equation}
		In particular, $\feas$ is convex if \eqref{eq:op} is convex.
		\item Problem \eqref{eq:op} is \emph{feasible} if it admits at least a feasible point (i.e., $\feas\neq \varnothing$).
		\item The \emph{optimal value} is $p^\star = \min_{x\in\feas} F(x_0)$. 
		\item A \emph{minimizer} is a feasible point $x^\star$ such that $F_0(x^\star)\le F_0(x)$ for all feasible $x\in \feas$. That is, $F_0(x^\star)=p^\star$.
	\end{itemize}
}

Observe that the constrained optimization problem \eqref{eq:op} is equivalent to the uncostrained optimization problem
\begin{equation}
	\min_{x\in \bbR^d} F_0(x) + \chi_{\feas},
\end{equation}
where $\chi_{\feas}$ is the $0-\infty$ characteristic function defined in \eqref{eq:0-infty-char}.

Let us introduce the notation 
\begin{equation}
	\bbR^M=\{\nu\in \bbR^M\mid \nu_j\ge 0\quad \forall j\in \llbracket 1,M\rrbracket\}.
\end{equation}

\dfn{Lagrange and Lagrange dual functions}{
	The \emph{Lagrange function} of the optimization problem \eqref{eq:op} is the function $F:\bbR^d \times \bbR^m \times \bbR^M_+\to \bbR\cup\{+\infty\}$ defined by 
	\begin{equation}
		L(x,\xi,\nu) = F_0(x) + \langle \xi, Ax-y\rangle +\sum_{j=1}^m \nu_j F_j(x).
	\end{equation}

	The \emph{Lagrange dual function} is the function $H:\bbR^m\times \bbR^M_+ \to \bbR\cup\{-\infty\}\cup\{+\infty\}$, defined by 
	\begin{equation}
		H(\xi,\nu) = \inf_{x\in \bbR^d} L(x,\xi,\nu).
	\end{equation}
	}

\prop[weak-duality]{}{
	The dual function is always concave. Moreover, if $x^\star$ is a minimizer of \eqref{eq:op}, we have
	\begin{equation}
		H(\xi,\nu) \le F(x^\star), \qquad \forall \xi\in \bbR^m,\, \nu\in \bbR^M_+.
	\end{equation}
}

\begin{proof}
	Observe that $-H$ is the supremum w.r.t.~$x\in \bbR^d$ of the functions $g_x(\xi,\nu) = -L(x,\xi,\nu)$. The function $g_x$ is affine, and thus convex. Hence, $-H$ is the pointwise supremum of the family $\{g_x\}_{x\in\bbR^d}$ of convex function. It is immediate to check that it is convex, and thus that $H$ is concave.

	On the other hand, for any feasible point $x\in \Phi$, since $\nu_j\ge 0$ for any $j\in \llbracket1,M\rrbracket$, we have 
	\begin{equation}
		\langle \xi, Ax-y\rangle +\sum_{j=1}^m \nu_j F_j(x) \le 0.
	\end{equation}
	Then, $L(x,\xi,\nu)\le F_0(x)\le F_0(x^\star)$ and, as a consequence,
	\begin{equation}
		H(\xi,\nu) \le \inf_{x\in\feas} L(x,\xi,\nu) \le F_0(x^\star).
	\end{equation}
	This completes the proof of the statement.
\end{proof}

The previous result suggests to introduce the following.

\dfn{Primal and dual problem}{
	The \emph{dual problem} to \eqref{eq:op}, which is called the \emph{primal problem}, it the optimization problem
	\begin{equation}
		\tag{DP}
		\label{eq:dp}
		\max_{\xi \in \bbR^m,\, \nu \in \bbR^M} H(\xi,\nu) 
		\qquad \text{subject to} \qquad 
		\nu_j\ge 0 \quad \forall j\in \llbracket1,M\rrbracket.
	\end{equation}

	\begin{itemize}
		\item A pair $(\xi,\nu)\in \bbR^m\times \bbR^M_+$ is called \emph{dual feasible}.
		\item The \emph{dual optimal value} is the solution $d^\star$ of \eqref{eq:dp}.
		\item A \emph{dual optimal} or \emph{optimal Lagrange multiplier} is a feasible maximizer $(\xi^\star\nu^\star)\in \bbR^m\times \bbR^M_+$.
		\item A \emph{primal-dual optimal} is a triple $(x^\star,\xi^\star,\nu^\star)$ where $x^\star$ is a minimizer for \eqref{eq:op} and $(\xi^\star,\nu^\star)$ is a dual optimal.
	\end{itemize}
}

\dfn{Duality}{
	The primal-dual problems always satisfy \emph{weak duality}, that is $d^\star \le p^\star$ where $d^\star$ is the dual optimal value and $p^\star$ is the primal optimal value.
	
	We say that the problems enjoy \emph{strong duality} if it holds
	\begin{equation}
		p^\star = d^\star.
	\end{equation}
}

The above shows the interest of the dual problem: when strong duality holds, in order to solve the minimization problem \eqref{eq:op} it suffices to solve the dual problem \eqref{eq:dp}.

The following is the most used criterion for strong duality. 

\thm[slater]{Slater's constraint quantification}{
	Assume that $F_0,\ldots, F_M$ are convex functions with domain $\operatorname{dom}(\bbR^d)$ and that $F_0(x)\ge -c$ for some $c\ge 0$. Then, strong duality holds if there exists $x\in \feas\subset \bbR^d$ such that $F_j(x)<0$ for any $j\in \llbracket 1,M\rrbracket$.
}

For a proof of the above result, we refer to \cite[Section 5.3.2]{boydConvex2023}.

\subsection*{Geometric interpretation}

Let us follow \cite[Section~5.3]{boydConvex2023} and present a geometric interpretation of the previous discussion.
% 
% Introduce the domain of the optimisation problem, which is the set $\mcD = \bigcap_{i=0}^M \operatorname{dom}(F_i)\subset \bbR^d$, and define
% \begin{equation}
% 	\mcG = \{ (F_1(x), \ldots, F_M(x),Ax-y, F_0(x))\in \bbR^M\times\bbR^m\times \bbR \mid x\in \mcD  \}.
% \end{equation}
% This is the set of values taken by the constraints and the objective function. The optimal value is then 
% \begin{equation}
% 	p^\star = \inf \left\{ t \mid (u,v,t) \in \mcG \text{ and } u\in \bbR^M_+ \right\}.
% \end{equation}
% But then, observing that $L(x,\xi,\nu) = (\nu,\xi,1)^\top (F_1(x),\ldots, F_M(x),Ax-y,F_0(x))$, we have that
% \begin{equation}
% 	H(\xi,\nu) = \inf \left\{ (\nu,\xi,1)^\top (u,v,t)\mid (u,v,t) \in \mcG \right\}.
% \end{equation}
% In particular, if the infimum is finite the inequality 
% \begin{equation}
% 	(\nu,\xi,1)^\top (u,v,t) \ge H(\xi,\nu),
% \end{equation}
% defines a supporting hyperplane for the set $\mcG$.
% 
Assume that there are no equality constraints and a single inequality constraint, and define 
\begin{equation}
	\mcG = \{ (F_1(x), F_0(x)) \mid x\in \bbR^d\}.
\end{equation}
By construction, the problem is feasible if and only if $\mcG$ intersects the left-half plane.
Furthermore, we have
\begin{equation}
	p^\star  = \min\{ t \mid (u,t)\in \mcG,\, u\le 0\}.
\end{equation}
Since $L(x,\nu) = (\nu,1)^\top (F_1(x),F_0(x))$, we also have 
\begin{equation}
	H(\nu) = \inf \{  (\nu,1)^\top (u,t) \mid (u,t)\in \mcG \}.
\end{equation}
Hence, if this infimum is finite, the inequality $(\nu,1)^\top (u,t)\ge H(\nu)$ defines a supporting hyperplane for $\mcG$. 

If the problem is convex, then $\mcG$ is convex and under Slater's condition its interior intersects the left-hand plane. This insures that strong duality holds.

\begin{figure}
	\centering
	\begin{minipage}{.48\textwidth}
		\centering
		\includegraphics[width=.8\textwidth]{images/geometric-primal-dual-1.png}	
		\caption{Geometric interpretation. The value of the dual function $H(\nu)$ identifies a supporting hyperplane for the set $\mcG$.}
		\label{fig:geom1}
	\end{minipage}
	\hfill
	\begin{minipage}{.48\textwidth}
		\centering
		\includegraphics[width=.8\textwidth]{images/geometric-primal-dual-2.png}	
		\caption{Geometric interpretation. Solving the dual problem yields the blue hyperplane. In this case $p^\star > d^\star$ and strong duality does not hold.}
		\label{fig:geom2}
	\end{minipage}
	\begin{minipage}{.48\textwidth}
		\centering
		\includegraphics[width=.8\textwidth]{images/geometric-primal-dual-3.png}	
		\caption{Geometric interpretation of Slater's condition. When the set $\mathcal G$ is convex and has interior that intersects the left-hand plane, the best supporting hyperplane yields the optimal value $p^\star$.}
		\label{fig:geom3}
	\end{minipage}
\end{figure}

	% \dfn{Definition Topic}{Definition Statement}
	% \thm{Theorem Name}{Theorem Statement}
	% \cor[cori]{Corollary Name}{Corollary Statement}
	% \lem{Lemma Name}{Lemma Statement}
	% \clm{Claim Name}{Claim Statement}
	% \ex{Example Name}{Example explained}
% \opn{Open Question Name}{Question Statement}
% \qs{Question Name}{Question Statement}
% \nt{Special Note}
% \wc{Wrong Concept topic}{Explanation}
% \pf{Proof}{Proof}
	
% \chapter{Second Chapter}
% \section{Section 1}

\printbibliography
\end{document}